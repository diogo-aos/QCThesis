\select@language {english}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces First and second features of the Iris dataset. Fig. \ref {fig:intro raw} shows the scatter plot of the raw input data, i.e. how the algorithms "see" the data. Fig. \ref {fig:intro natural} shows the desired labels for each point, where each color and symbol are coded to a class.\relax }}{5}{figure.caption.8}
\contentsline {figure}{\numberline {2.2}{\ignorespaces The output labels of the K-Means algorithm with the number of clusters (input parameter) set to 3. The different plots show the centroids (squares) evolution on each iteration. Between iteration 3 and the converged state 2 more iterations were executed.\relax }}{9}{figure.caption.9}
\contentsline {figure}{\numberline {2.3}{\ignorespaces The above figures show an example of a graph (left) and its corresponding Minimum Spanning Tree (right). The circles are vertices and the edges are the lines linking the vertices.\relax }}{11}{figure.caption.10}
\contentsline {figure}{\numberline {2.4}{\ignorespaces The above plots show the dendrogram and a possible clustering taken from a Single-Link run over the Iris dataset. Fig. \ref {fig:sl clustering} was obtained by performing a cut on a level that would yield a partition of 3 clusters.\relax }}{11}{figure.caption.11}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Thread hierarchy \cite {Nvidia2014}.\relax }}{19}{figure.caption.15}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Distribution of thread blocks is automatically scaled with the increase of the number of multiprocessors \cite {Nvidia2014}.\relax }}{19}{figure.caption.15}
\contentsline {figure}{\numberline {3.3}{\ignorespaces Memory model used by CUDA \cite {Nvidia2014}.\relax }}{20}{figure.caption.16}
\contentsline {figure}{\numberline {3.4}{\ignorespaces Sample execution flow of a CUDA application \cite {Nvidia2014}.\relax }}{20}{figure.caption.16}
\contentsline {figure}{\numberline {3.5}{\ignorespaces Flow execution of the GPU parallel K-Means algorithm.\relax }}{22}{figure.caption.17}
\contentsline {figure}{\numberline {3.6}{\ignorespaces Correspondence between a sparse matrix and its CSR counterpart.\relax }}{24}{figure.caption.20}
\contentsline {figure}{\numberline {3.7}{\ignorespaces Flow execution of Sousa2015.\relax }}{26}{figure.caption.22}
\contentsline {figure}{\numberline {3.8}{\ignorespaces Example of the exprefix sum operation.\relax }}{27}{figure.caption.24}
\contentsline {figure}{\numberline {3.9}{\ignorespaces Representation of the reduce phase of Blelloch's algorithm \cite {Harris2007}. \emph {d} is the level of the tree and the input array can be observed at $d=0$.\relax }}{28}{figure.caption.25}
\contentsline {figure}{\numberline {3.10}{\ignorespaces Representation of the down-sweep phase of Blelloch's algorithm \cite {Harris2007}. \emph {d} is the level of the tree.\relax }}{28}{figure.caption.26}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Diagram of proposed solution in each phase of EAC.\relax }}{35}{figure.caption.35}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Inserting a cluster of the first partition in the co-association matrix.\relax }}{41}{figure.caption.36}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Inserting a cluster from a partition in the co-association matrix. The arrows indicate to where the indices are moved. The numbers indicate the order of the operation.\relax }}{42}{figure.caption.37}
\contentsline {figure}{\numberline {4.4}{\ignorespaces The left figure shows the number of associations per pattern in a complete matrix; the right figure shows the number of associations per pattern in a condensed matrix.\relax }}{44}{figure.caption.39}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Diagram of the connected components labeling algorithm used.\relax }}{47}{figure.caption.44}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Speed-up of the labeling phase for datasets of 2 dimensions and varying cardinality and number of clusters. The dotted black line represents a speed-up of one.\relax }}{58}{figure.caption.53}
\contentsline {figure}{\numberline {5.2}{\ignorespaces Speed-up of the labeling phase for datasets of 200 dimensions and varying cardinality and number of clusters. The dotted black line represents a speed-up of one.\relax }}{59}{figure.caption.54}
\contentsline {figure}{\numberline {5.3}{\ignorespaces Execution times for computing the condensed co-association matrix using different matrix strategies.\relax }}{61}{figure.caption.58}
\contentsline {figure}{\numberline {5.4}{\ignorespaces Evolution of $K_{min}$ with cardinality for different rules.\relax }}{66}{figure.caption.63}
\contentsline {figure}{\numberline {5.5}{\ignorespaces Execution time for the production of the clustering ensemble.\relax }}{67}{figure.caption.64}
\contentsline {figure}{\numberline {5.6}{\ignorespaces Execution time for building the co-association matrix from ensemble with different rules.\relax }}{67}{figure.caption.65}
\contentsline {figure}{\numberline {5.7}{\ignorespaces Execution time for building the co-association matrix with different matrix formats.\relax }}{68}{figure.caption.66}
\contentsline {figure}{\numberline {5.8}{\ignorespaces Comparison between the execution times of the three methods of SL. SLINK runs over fully allocated condensed matrix while SL-MST and SL-MST-Disk run over the condensed and complete sparse matrices.\relax }}{68}{figure.caption.67}
\contentsline {figure}{\numberline {5.9}{\ignorespaces Comparison between the execution times of SLINK to different rules.\relax }}{69}{figure.caption.68}
\contentsline {figure}{\numberline {5.10}{\ignorespaces Comparison between the execution times of SL-MST for different rules.\relax }}{69}{figure.caption.69}
\contentsline {figure}{\numberline {5.11}{\ignorespaces Execution times for all phases combined, using SL-MST in the recovery phase.\relax }}{70}{figure.caption.70}
\contentsline {figure}{\numberline {5.12}{\ignorespaces Execution times for all phases combined, using SL-MST-Disk in the recovery phase.\relax }}{70}{figure.caption.71}
\contentsline {figure}{\numberline {5.13}{\ignorespaces Density of associations relative to the full co-association matrix, which hold $n^2$ associations.\relax }}{71}{figure.caption.72}
\contentsline {figure}{\numberline {5.14}{\ignorespaces Evolution of the total number of associations divided by the number of patterns according to the different rules.\relax }}{72}{figure.caption.73}
\contentsline {figure}{\numberline {5.15}{\ignorespaces Maximum number of associations of any pattern divided by the number of patterns in the biggest cluster of the ensemble.\relax }}{72}{figure.caption.74}
\contentsline {figure}{\numberline {5.16}{\ignorespaces Allocated number of associations relative to the full $n^2$ matrix.\relax }}{73}{figure.caption.75}
\contentsline {figure}{\numberline {5.17}{\ignorespaces Memory used relative to the full $n^2$ matrix.\relax }}{73}{figure.caption.76}
\contentsline {figure}{\numberline {5.18}{\ignorespaces Accuracy of the final clusterings as measured with the Consistency Index.\relax }}{74}{figure.caption.77}
\addvspace {10\p@ }
