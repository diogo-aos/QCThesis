Automatically generated by Mendeley Desktop 1.15.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Silva2016,
author = {Silva, Diogo and Fred, Ana and Aidos, Helena},
journal = {Proccedings of the 5th International Conference on Pattern Recognition Applications and Methods},
number = {Accepted},
title = {{Efficient Evidence Accumulation Clustering for Large Datasets}},
year = {2016}
}
@article{Horn2004,
abstract = {We describe results of a novel algorithm for grammar induction from a large corpus. The ADIOS (Automatic DIstillation of Structure) algorithm searches for significant patterns, chosen according to context dependent statistical criteria, and builds a hierarchy of such patterns according to a set of rules leading to structured generalization. The corpus is thus generalized into a context free grammar (CFG), composed of patterns, equivalence classes and words of the initial lexicon. We have evaluated our method both on corpora generated by CFG and on natural language ones. The performance of ADIOS is judged by searching for both good recall (acceptance of correct novel sentences) and good precision (production of correct novel sentences). The results are very encouraging.},
author = {Horn, David and Solan, Zach and Ruppin, Eytan and Edelman, Shimon},
file = {:home/chiroptera/Dropbox/mendeley/Horn et al. - 2004 - Unsupervised language acquisition syntax from plain corpus.pdf:pdf},
journal = {{\ldots} on Human Language},
title = {{Unsupervised language acquisition: syntax from plain corpus}},
url = {http://horn.tau.ac.il/{~}horn/publications/newcastle.pdf},
year = {2004}
}
@article{drineas2006fast,
author = {Drineas, Petros and Kannan, Ravi and Mahoney, Michael W},
file = {:home/chiroptera/Dropbox/mendeley/Drineas, Kannan, Mahoney - 2006 - Fast Monte Carlo algorithms for matrices III Computing a compressed approximate matrix decomposition.pdf:pdf},
journal = {SIAM Journal on Computing},
number = {1},
pages = {184--206},
publisher = {SIAM},
title = {{Fast Monte Carlo algorithms for matrices III: Computing a compressed approximate matrix decomposition}},
volume = {36},
year = {2006}
}
@article{Walter2008,
abstract = {Hierarchical representations of large data sets, such as binary cluster trees, are a crucial component in many scalable algorithms used in various fields. Two major approaches for building these trees are agglomerative, or bottom-up, clustering and divisive, or top-down, clustering. The agglomerative approach offers some real advantages such as more flexible clustering and often produces higher quality trees, but has been little used in graphics because it is frequently assumed to be prohibitively expensive (O(N<sup>2</sup>) or worse). In this paper we show that agglomerative clustering can be done efficiently even for very large data sets. We introduce a novel locally-ordered algorithm that is faster than traditional heap-based agglomerative clustering and show that the complexity of the tree build time is much closer to linear than quadratic. We also evaluate the quality of the agglomerative clustering trees compared to the best known divisive clustering strategies in two sample applications: bounding volume hierarchies for ray tracing and light trees in the Lightcuts rendering algorithm. Tree quality is highly application, data set, and dissimilarity function specific. In our experiments the agglomerative-built tree quality is consistently higher by margins ranging from slight to significant, with up to 35{\%} reduction in tree query times.},
author = {Walter, Bruce and Bala, Kavita and Kulkarni, Milind and Pingali, Keshav},
doi = {10.1109/RT.2008.4634626},
file = {:home/chiroptera/Dropbox/mendeley/Walter et al. - 2008 - Fast agglomerative clustering for rendering.pdf:pdf},
isbn = {9781424427413},
journal = {RT'08 - IEEE/EG Symposium on Interactive Ray Tracing 2008, Proceedings},
keywords = {Agglomerative clustering,Bottom-up tree construction,Bounding volume hierarchy,Dendogram,Lightcuts rendering},
pages = {81--86},
title = {{Fast agglomerative clustering for rendering}},
year = {2008}
}
@article{Klipfel1943,
author = {Klipfel, Joel},
file = {:home/chiroptera/Dropbox/mendeley/Klipfel - 1943 - A brief introduction to hilbert space and quantum logic.pdf:pdf},
pages = {1--31},
title = {{A brief introduction to hilbert space and quantum logic}},
year = {1943}
}
@techreport{AltetiAbad2007,
abstract = {In a world that is continuously providing enormous amounts of data (be it from satellite teledetection, industrial/biological sensors or historical data for ﬁnancial use, among many other sources), the need to quickly search through it and discover its interesting trends is becoming more and more important. Indexing is the process that allows to extract the interesting bits of information out of these oceans of data in a very short time, thus making possible the analysis of huge amounts of data. PyTables is a hierarchically-organized database that is focused on dealing with such large data scenarios. In PyTables 1.x series, indexing was implemented using a simple approach, so-called PSI, that worked ﬁne for medium sized tables, but that was not very efﬁcient for dealing with large ones. With the advent of PyTables 2.0 Pro, we are introducing OPSI, a completely new implementation of the indexing engine that signiﬁcantly improves look-up times for all sizes of tables, and most specially for very large ones (> 100 millions of rows). In addition, the new implementation provides an innovative feature that lets the user create indexes having a certain level of quality, allowing her to select the one that is best suited to her particular needs. This article describes the implementation of this new indexing system, while showing experimental ﬁgures about its performance.},
author = {{Altet i Abad}, Francesc and Balaguer, Ivan Vilata},
file = {:home/chiroptera/Dropbox/mendeley/Altet i Abad, Balaguer - 2007 - OPSI The indexing system of PyTables 2 Professional Edition.pdf:pdf},
pages = {1--27},
title = {{OPSI : The indexing system of PyTables 2 Professional Edition}},
url = {http://www.pytables.org/docs/OPSI-indexes.pdf},
year = {2007}
}
@article{Owens2006,
author = {Owens, John D and Luebke, David and Govindraju, Naga and Harris, Mark and Kruger, Jens and Lefohn, Aaron E and Purcell, Timothy J},
file = {:home/chiroptera/Dropbox/mendeley/Owens et al. - 2006 - A Survey of General Purpose Computation on Graphics Hardware.pdf:pdf},
journal = {Computer Graphics Forum},
number = {August},
pages = {21--51},
title = {{A Survey of General Purpose Computation on Graphics Hardware}},
url = {http://www.cs.virginia.edu/papers/ASurveyofGeneralPurposeComputationonGraphicsHardware.pdf},
year = {2006}
}
@article{Xiao2010,
abstract = {The number of clusters has to be known in advance for the conventional k-means clustering algorithm and moreover the clustering result is sensitive to the selection of the initial cluster centroids. This sensitivity may make the algorithm converge to the local optima. This paper proposes a quantum-inspired genetic algorithm for k-means clustering (KMQGA). In KMQGA, a Q-bit based representation is employed for exploration and exploitation in discrete 0-1 hyperspace using rotation operation of quantum gate as well as the typical genetic algorithm operations (selection, crossover and mutation) of Q-bits. Different from the typical quantum-inspired genetic algorithms (QGA), the length of a Q-bit in KMQGA is variable during evolution. Without knowing the exact number of clusters beforehand, KMQGA can obtain the optimal number of clusters as well as providing the optimal cluster centroids. Both the simulated datasets and the real datasets are used to validate KMQGA, respectively. The experimental results show that KMQGA is promising and effective. ?? 2009 Elsevier Ltd. All rights reserved.},
annote = {From Duplicate 2 (A quantum-inspired genetic algorithm for k-means clustering - Xiao, Jing; Yan, YuPing; Zhang, Jun; Tang, Yong)

This algorithm has the interest property of choosing the number of clusters. It does this in the crossover operation, in which the number of new clusters is chosen somewhat randonmly.},
author = {Xiao, Jing and Yan, YuPing and Zhang, Jun and Tang, Yong},
doi = {10.1016/j.eswa.2009.12.017},
file = {:home/chiroptera/Dropbox/mendeley/Xiao et al. - 2010 - A quantum-inspired genetic algorithm for k-means clustering.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Genetic algorithms,Quantum-inspired genetic algorithms,k-means,k-means clustering,qubit},
mendeley-tags = {k-means,qubit},
pages = {4966--4973},
title = {{A quantum-inspired genetic algorithm for k-means clustering}},
url = {http://ac.els-cdn.com/S095741740901063X/1-s2.0-S095741740901063X-main.pdf?{\_}tid=f303a76c-ac71-11e4-be73-00000aacb35e{\&}acdnat=1423056793{\_}66291f279193fa69b86c93aecea405b0},
volume = {37},
year = {2010}
}
@article{Fred2009c,
author = {Fred, Ana},
file = {:home/chiroptera/Dropbox/mendeley/Fred - 2009 - Tutorial Pt.1 Basic Concepts of data clustering.pdf:pdf},
number = {April},
pages = {1--17},
title = {{Tutorial Pt.1: Basic Concepts of data clustering}},
year = {2009}
}
@article{Stovall2014,
author = {Stovall, Thomas and Kockara, Sinan and Avci, Recep},
doi = {10.1109/TPDS.2014.2374607},
file = {:home/chiroptera/Dropbox/mendeley/Stovall, Kockara, Avci - 2014 - GPUSCAN GPU-based Parallel Structural Clustering Algorithm for Networks.pdf:pdf},
issn = {1045-9219},
journal = {IEEE Transactions on Parallel and Distributed Systems},
keywords = {clustering,gpu},
mendeley-tags = {clustering,gpu},
number = {c},
pages = {1--1},
title = {{GPUSCAN: GPU-based Parallel Structural Clustering Algorithm for Networks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6967853},
volume = {9219},
year = {2014}
}
@article{Chen2014,
abstract = {In this paper, we review the background and state-of-the-art of big data. We first introduce the general background of big data and review related technologies, such as could computing, Internet of Things, data centers, and Hadoop.We then focus on the four phases of the value chain of big data, i.e., data generation, data acquisition, data storage, and data analysis. For each phase, we introduce the general background, discuss the technical challenges, and review the latest advances. We finally examine the several representative applications of big data, including enterprise management, Internet of Things, online social networks, medial applications, collective intelligence, and smart grid. These discussions aimto provide a comprehensive overview and big-picture to readers of this exciting area. This survey is concluded with a discussion of open problems and future directions.},
author = {Chen, Min and Mao, Shiwen and Liu, Yunhao},
doi = {10.1007/s11036-013-0489-0},
file = {:home/chiroptera/Dropbox/mendeley/Chen, Mao, Liu - 2014 - Big data A survey.pdf:pdf},
isbn = {1383-469X},
issn = {1383469X},
journal = {Mobile Networks and Applications},
keywords = {Big data,Big data analysis,Cloud computing,Data center,Hadoop,Internet of things,Smart grid},
number = {2},
pages = {171--209},
title = {{Big data: A survey}},
volume = {19},
year = {2014}
}
@article{Elteir2011,
abstract = {MapReduce is a programming model from Google that facilitates parallel processing on a cluster of thousands of commodity computers. The success of MapReduce in cluster environments has motivated several studies of implementing MapReduce on a graphics processing unit (GPU), but generally focusing on the NVIDIA GPU. Our investigation reveals that the design and mapping of the MapReduce framework needs to be revisited for AMD GPUs due to their notable architectural differences from NVIDIA GPUs. For instance, current state-of-the-art MapReduce implementations employ atomic operations to coordinate the execution of different threads. However, atomic operations can implicitly cause inefficient memory access, and in turn, severely impact performance. In this paper, we propose Streamer, an OpenCL MapReduce framework optimized for AMD GPUs. With efficient atomic-free algorithms for output handling and intermediate result shuffling, Stream MR is superior to atomic-based MapReduce designs and can outperform existing atomic-free MapReduce implementations by nearly five-fold on an AMD Radeon HD 5870.},
author = {Elteir, Marwa and Lin, Heshan and Feng, Wu Chun and Scogland, Tom},
doi = {10.1109/ICPADS.2011.131},
file = {:home/chiroptera/Dropbox/mendeley/Elteir et al. - 2011 - StreamMR An optimized MapReduce framework for AMD GPUs.pdf:pdf},
isbn = {9780769545769},
issn = {15219097},
journal = {Proceedings of the International Conference on Parallel and Distributed Systems - ICPADS},
keywords = {AMD GPU,Atomics,GPGPU,MapCG,MapReduce,Mars,OpenCL,Parallel computing},
pages = {364--371},
title = {{StreamMR: An optimized MapReduce framework for AMD GPUs}},
year = {2011}
}
@article{Kijsipongse2012,
abstract = {K-Means is the clustering algorithm which is widely used in many areas such as information retrieval, computer vision and pattern recognition. With the recent advance in General Purpose Graphics Processing Unit (GPGPU), we can use a modern GPU which is capable to do computation up to Tflops to calculate K-Means clustering on average problems. However, due to the exponential growth of data, the K-Means clustering on a single GPU will not be adequate for large datasets in the near future. In this paper, we present the design and implementation of an efficient large-scale parallel K-Means on GPU clusters. We utilize the massive parallelism in GPUs to speed up the most time consuming part of K-Means clustering in each node. We employ the dynamic load balancing to distribute workload equally on different GPUs installed in the clusters so as to improve the performance of the parallel K-Means at the inter-node level. We also take advantage from software distributed shared memory to simplify the communication and collaboration among nodes. The result of the evaluation shows the performance improvement of the parallel K-Means by maintaining load balance on GPU clusters.},
author = {Kijsipongse, Ekasit and U-Ruekolan, Suriya},
doi = {10.1109/JCSSE.2012.6261977},
file = {:home/chiroptera/Dropbox/mendeley/Kijsipongse, U-Ruekolan - 2012 - Dynamic load balancing on GPU clusters for large-scale K-Means clustering.pdf:pdf},
isbn = {9781467319218},
journal = {JCSSE 2012 - 9th International Joint Conference on Computer Science and Software Engineering},
keywords = {gpu,k-means},
mendeley-tags = {gpu,k-means},
pages = {346--350},
title = {{Dynamic load balancing on GPU clusters for large-scale K-Means clustering}},
year = {2012}
}
@article{Herrero-Lopez2011,
abstract = {The uninterrupted growth of information repositories has progressively lead data-intensive applications, such as MapReduce-based systems, to the mainstream. The MapReduce paradigm has frequently proven to be a simple yet flexible and scalable technique to distribute algorithms across thousands of nodes and petabytes of information. Under these circumstances, classic data mining algorithms have been adapted to this model, in order to run in production environments. Unfortunately, the high latency nature of this architecture has relegated the applicability of these algorithms to batch-processing scenarios. In spite of this shortcoming, the emergence of massively threaded shared-memory multiprocessors, such as Graphics Processing Units (GPU), on the commodity computing market has enabled these algorithms to be executed orders of magnitude faster, while keeping the same MapReduce based model. In this paper, we propose the integration of massively threaded shared-memory multiprocessors into MapReduce-based clusters creating a unified heterogeneous architecture that enables executing Map and Reduce operators on thousands of threads across multiple GPU devices and nodes, while maintaining the built-in reliability of the baseline system. For this purpose, we created a programming model that facilitates the collaboration of multiple CPU cores and multiple GPU devices towards the resolution of a data intensive problem. In order to prove the potential of this hybrid system, we take a popular NP-Hard supervised learning algorithm, the Support Vector Machine (SVM) and show that a 36x {\&}{\#}x2013; 192x speedup can be achieved on large datasets without changing the model or leaving the commodity hardware paradigm.},
annote = {Top notch paper.

Deals mostly with bringing GPU to clusters. Combines the two categories described below.

Presents two views of MapReduce: cluster category and multiprocessor. In cluster category jobs are distributed over a network, where each node is responsible for some jobs and communication is done over TCP/IP or MPI. On the multiprocessor category the jobs are distributed over an architecture of cores with shared memory: cpu or gpu.

It gives examples for each of these, as well as advantages and disadvantages.},
author = {Herrero-Lopez, Sergio},
doi = {10.1109/ICSMC.2011.6083839},
file = {:home/chiroptera/Dropbox/mendeley/Herrero-Lopez - 2011 - Accelerating SVMs by integrating GPUs into MapReduce clusters.pdf:pdf},
isbn = {9781457706523},
issn = {1062922X},
journal = {Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics},
keywords = {Multiprocessing,Parallel Algorithms,Pattern Classification,cluster,gpu,machine learning,mapreduce,svm,topnotch},
mendeley-tags = {cluster,gpu,machine learning,mapreduce,svm,topnotch},
pages = {1298--1305},
title = {{Accelerating SVMs by integrating GPUs into MapReduce clusters}},
year = {2011}
}
@article{Stuart2011,
abstract = {We present GPMR, our stand-alone MapReduce library that leverages the power of GPU clusters for large-scale computing. To better utilize the GPU, we modify MapReduce by combining large amounts of map and reduce items into chunks and using partial reductions and accumulation. We use persistent map and reduce tasks and stress aspects of GPMR with a set of standard MapReduce benchmarks. We run these benchmarks on a GPU cluster and achieve desirable speedup and efficiency for all benchmarks. We compare our implementation to the current-best GPU-MapReduce library (runs only on a solo GPU) and a highly-optimized multi-core MapReduce to show the power of GPMR. We demonstrate how typical MapReduce tasks are easily modified to fit into GPMR and leverage a GPU cluster. We highlight how total and relative amounts of communication affect GPMR. We conclude with an exposition on the types of MapReduce tasks well-suited to GPMR, and why some tasks need more modifications than others to work well with GPMR.},
author = {Stuart, Jeff a. and Owens, John D.},
doi = {10.1109/IPDPS.2011.102},
file = {:home/chiroptera/Dropbox/mendeley/Stuart, Owens - 2011 - Multi-GPU MapReduce on GPU clusters.pdf:pdf},
isbn = {9780769543857},
issn = {1530-2075},
journal = {Proceedings - 25th IEEE International Parallel and Distributed Processing Symposium, IPDPS 2011},
keywords = {GPU,MapReduce,cluster},
mendeley-tags = {GPU,MapReduce,cluster},
pages = {1068--1079},
title = {{Multi-GPU MapReduce on GPU clusters}},
year = {2011}
}
@inproceedings{grover1996fast,
author = {Grover, Lov K},
booktitle = {Proceedings of the twenty-eighth annual ACM symposium on Theory of computing},
file = {:home/chiroptera/Dropbox/mendeley/Grover - 1996 - A fast quantum mechanical algorithm for database search.pdf:pdf},
organization = {ACM},
pages = {212--219},
title = {{A fast quantum mechanical algorithm for database search}},
year = {1996}
}
@article{Satish2009a,
abstract = {We describe the design of high-performance parallel radix sort and merge sort routines for manycore GPUs, taking advantage of the full programmability offered by CUDA. Our radix sort is the fastest GPU sort and our merge sort is the fastest comparison-based sort reported in the literature. Our radix sort is up to 4 times faster than the graphics-based GPUSort and greater than 2 times faster than other CUDA-based radix sorts. It is also 23{\%} faster, on average, than even a very carefully optimized multicore CPU sorting routine. To achieve this performance, we carefully design our algorithms to expose substantial fine-grained parallelism and decompose the computation into independent tasks that perform minimal global communication. We exploit the high-speed onchip shared memory provided by NVIDIA's GPU architecture and efficient data-parallel primitives, particularly parallel scan. While targeted at GPUs, these algorithms should also be well-suited for other manycore processors.},
author = {Satish, Nadathur and Harris, Mark and Garland, Michael},
doi = {10.1109/IPDPS.2009.5161005},
file = {:home/chiroptera/Dropbox/mendeley/Satish, Harris, Garland - 2009 - Designing efficient sorting algorithms for manycore gpus.pdf:pdf},
isbn = {9781424437504},
issn = {1530-2075},
journal = {IPDPS 2009 - Proceedings of the 2009 IEEE International Parallel and Distributed Processing Symposium},
number = {May},
pages = {1--10},
title = {{Designing efficient sorting algorithms for manycore gpus}},
url = {http://www.nvidia.com/docs/io/67073/nvr-2008-001.pdf},
year = {2009}
}
@inbook{Blelloch1990,
author = {Blelloch, Guy E},
booktitle = {Synthesis of parallel algorithms},
chapter = {1},
file = {:home/chiroptera/Dropbox/mendeley//Blelloch - 1993 - Prefix Sums and Their Applications.pdf:pdf},
pages = {35--60},
title = {{Prefix Sums and Their Applications}},
url = {http://www.cs.cmu.edu/{\{}{~}{\}}blelloch/papers/Ble93.pdf},
year = {1993}
}
@article{Nvidia,
author = {Nvidia},
file = {:home/chiroptera/Dropbox/mendeley/Nvidia - Unknown - NVIDIA’s Next Generation CUDA Compute Architecture Kepler GK110210.pdf:pdf},
title = {{NVIDIA’s Next Generation CUDA Compute Architecture: Kepler GK110/210}}
}
@inproceedings{li2007quantum,
author = {Li, Zhi-Hua and Wang, Shi-Tong and Wuxi, Jiangsu},
booktitle = {Wavelet Analysis and Pattern Recognition, 2007. ICWAPR'07. International Conference on},
file = {:home/chiroptera/Dropbox/mendeley/Li, Wang, Wuxi - 2007 - Quantum Theory The unified framework for FCM and QC algorithm.pdf:pdf},
isbn = {1424410665},
keywords = {2-,algorithm,function,implemented by quantum clustering,interpretation,qc,quantum clustering,quantum potential,quantum theory,wave},
organization = {IEEE},
pages = {1045--1048},
title = {{Quantum Theory: The unified framework for FCM and QC algorithm}},
volume = {3},
year = {2007}
}
@article{Jiang2009,
abstract = {Labeling the connected components in the feature space is an important step in grid based clustering algorithms in data mining. Although connected components labeling algorithms have been highly improved in image processing domain, there is little progress in grid based clustering in data mining domain. Two problems exist in transplanting these algorithms from image processing to data mining. One is how to process multi-dimensional dataset. The other is how to reduce the cost of auxiliary space. This paper describes an optimal two-scan Connected Components Labeling algorithm based that in image processing domain. It does not need auxiliary space, and easy to be extended to multi-dimension data set.},
author = {Jiang, Tao and Qiu, Ming and Chen, Jie and Cao, Xue},
doi = {10.1109/DBTA.2009.144},
file = {:home/chiroptera/Dropbox/mendeley/Jiang et al. - 2009 - LILA A connected components labeling algorithm in grid-based clustering.pdf:pdf},
isbn = {9780769536040},
journal = {Proceedings - 2009 1st International Workshop on Database Technology and Applications, DBTA 2009},
keywords = {Connected components labeling,Grid-based clustering,Multi-dimensional dataset},
pages = {213--216},
title = {{LILA: A connected components labeling algorithm in grid-based clustering}},
year = {2009}
}
@article{Chen2005,
abstract = { Feature models have been widely adopted in software reuse to organize the requirements of a set of similar applications in a software domain/product line. However, in most feature-oriented methods, the construction of feature models heavily depends on the domain analysts' personal understanding, and the work of constructing feature models from the original requirements of sample applications is often tedious and ineffective. This paper proposes a semiautomatic approach to constructing feature models based on requirements clustering, which automates the activities of feature identification, organization and variability modeling to a great extent. The underlying idea of this approach is to analyze the relationships between individual requirements and cluster tight-related requirements into features. With the automatic support of this approach, good quality feature models can be constructed in a more effective way. A case study is also provided to show the feasibility of this approach.},
author = {Chen, Kun Chen Kun and Zhang, Wei Zhang Wei and Zhao, Haiyan Zhao Haiyan and Mei, Hong Mei Hong},
doi = {10.1109/RE.2005.9},
file = {:home/chiroptera/Dropbox/mendeley/Chen et al. - 2005 - An approach to constructing feature models based on requirements clustering.pdf:pdf},
isbn = {0-7695-2425-7},
issn = {1090705X},
journal = {13th IEEE International Conference on Requirements Engineering (RE'05)},
title = {{An approach to constructing feature models based on requirements clustering}},
year = {2005}
}
@article{Spector1999,
author = {Spector, Lee and Barnum, Howard and Bernstein, Herbert J. and Swamy, Nikhil},
file = {:home/chiroptera/Dropbox/mendeley/Spector et al. - 1999 - Quantum Computing Applications of Genetic Programming.pdf:pdf},
journal = {Advances in Genetic Programming},
pages = {135--160},
title = {{Quantum Computing Applications of Genetic Programming}},
volume = {3},
year = {1999}
}
@article{Zechner2009b,
author = {Zechner, Mario and Granitzer, Michael},
doi = {issn: 1942-261x},
file = {:home/chiroptera/Dropbox/mendeley/Zechner, Granitzer - 2009 - K-Means on the Graphics Processor Design And Experimental Analysis.pdf:pdf},
journal = {International Journal on Advances in System and Measurements},
keywords = {-parallelization,gpgpu,k-means},
number = {2},
pages = {224--235},
title = {{K-Means on the Graphics Processor: Design And Experimental Analysis}},
url = {http://www.iariajournals.org/systems{\_}and{\_}measurements/sysmea{\_}v2{\_}n23{\_}2009{\_}paged.pdf},
volume = {2},
year = {2009}
}
@article{Zhang2008,
abstract = {The principle advantage and shortcoming of quantum clustering algorithm (QC) is analyzed. Based on its shortcomings, an improved algorithm - exponent distance-based quantum clustering algorithm (EQDC) is produced. It improved the iterative procedure of QC algorithm and used exponent distance formula to measure the distance between data points and the cluster centers. Experimental results demonstrate that the cluster accuracy of EDQC outperforms that of QC, and the exponent distance formula used in the clustering process performs better than the Euclidean distance in data preprocessing. What's more, the IRIS dataset can come to a satisfied result without preprocessing.},
author = {Zhang, Yao and Wang, Peng and Chen, Gao Yun and Chen, Dong Dong and Ding, Rui and Zhang, Yan},
doi = {10.1109/KAMW.2008.4810518},
file = {:home/chiroptera/Dropbox/mendeley/Zhang et al. - 2008 - Quantum clustering algorithm based on exponent measuring distance.pdf:pdf},
isbn = {9781424435296},
journal = {2008 IEEE International Symposium on Knowledge Acquisition and Modeling Workshop Proceedings, KAM 2008},
keywords = {Clustering accuracy,Data preprocessing,Exponent distance-based quantum clustering algorit,Measuring formula,Quantum clustering algorithm,Quantum potential},
number = {1},
pages = {436--439},
title = {{Quantum clustering algorithm based on exponent measuring distance}},
year = {2008}
}
@inproceedings{kmeansoriginal,
annote = {This is the original paper refering to K-Means.},
author = {MacQueen, J B},
booktitle = {Proceedings of 5-th Berkeley Symposium on Mathematical Statistics and Probability, Berkeley, University of California Press},
file = {:home/chiroptera/Dropbox/mendeley/MacQueen - 1967 - Some Methods for classification and Analysis of Multivariate Observations.pdf:pdf},
pages = {281--297},
title = {{Some Methods for classification and Analysis of Multivariate Observations}},
volume = {1},
year = {1967}
}
@article{Hillis1986,
abstract = {This section describes how the parallel prefix algorithm can be used to recognize a regular language. Therefore, each symbol of the string is associated with a set of state transitions in the deterministic finite automaton, that corresponds to the regular language. Then, these sets are pairwisely combined, thus yielding for every symbol, the state the automaton would have been in after reading that symbol.},
author = {Hillis, W. Daniel and Steele, Guy L.},
doi = {10.1145/7902.7903},
file = {:home/chiroptera/Dropbox/mendeley/Hillis, Steele - 1986 - Data parallel algorithms.pdf:pdf},
isbn = {0070730202},
issn = {00010782},
journal = {Communications of the ACM},
number = {12},
pages = {1170--1183},
title = {{Data parallel algorithms}},
volume = {29},
year = {1986}
}
@inproceedings{topchy2004mixture,
author = {Topchy, Alexander and Jain, Anil K and Punch, William},
booktitle = {Society for Industrial and Applied Mathematics. Proceedings of the SIAM International Conference on Data Mining},
file = {:home/chiroptera/Dropbox/mendeley/Topchy, Jain, Punch - 2004 - A mixture model for clustering ensembles.pdf:pdf},
organization = {Society for Industrial and Applied Mathematics},
pages = {379},
title = {{A mixture model for clustering ensembles}},
year = {2004}
}
@article{Wittek2013,
abstract = {Clustering methods in machine learning may benefit from borrowing metaphors from physics. Dynamic quantum clustering associates a Gaussian wave packet with the multidimensional data points and regards them as eigenfunctions of the Schr??dinger equation. The clustering structure emerges by letting the system evolve and the visual nature of the algorithm has been shown to be useful in a range of applications. Furthermore, the method only uses matrix operations, which readily lend themselves to parallelization. In this paper, we develop an implementation on graphics hardware and investigate how this approach can accelerate the computations. We achieve a speedup of up to two magnitudes over a multicore CPU implementation, which proves that quantum-like methods and acceleration by graphics processing units have a great relevance to machine learning. ?? 2012 Elsevier Inc.},
author = {Wittek, Peter},
doi = {10.1016/j.jcp.2012.08.048},
file = {:home/chiroptera/Dropbox/mendeley/Wittek - 2013 - High-performance dynamic quantum clustering on graphics processors(2).pdf:pdf},
issn = {00219991},
journal = {Journal of Computational Physics},
keywords = {Clustering,GPU computing,Quantum-like learning,Time-dependent schr??dinger equation},
pages = {262--271},
publisher = {Elsevier},
title = {{High-performance dynamic quantum clustering on graphics processors}},
url = {http://dx.doi.org/10.1016/j.jcp.2012.08.048},
volume = {233},
year = {2013}
}
@article{Arthur2007,
abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, ran- domized seeding technique, we obtain an algorithm that is $\Theta$(log k)-competitive with the optimal clustering. Prelim- inary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
author = {Arthur, D. and Arthur, D. and Vassilvitskii, S. and Vassilvitskii, S.},
doi = {10.1145/1283383.1283494},
file = {:home/chiroptera/Dropbox/mendeley/Arthur et al. - 2007 - k-means The advantages of careful seeding.pdf:pdf},
isbn = {978-0-898716-24-5},
journal = {Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms},
pages = {1027--1035},
title = {{k-means++: The advantages of careful seeding}},
url = {http://portal.acm.org/citation.cfm?id=1283494},
volume = {8},
year = {2007}
}
@article{Qian2010,
abstract = {We described a combined multiple clustering approach to automatically identify chronic lymphocytic leukemia neoplastic population by flow cytometry immunophenotyping. Flow cytometry data from various specimens were preprocessed by data cross-linking and subset selection before undergoing subspace and consensus clustering. This approach was implemented as a Server-side application, with results comparable to those performed by manual gating on commercial software.},
author = {Qian, You Wen and Cukierski, William and Osman, Mona and Goodell, Lauri},
doi = {10.1109/ICBBT.2010.5478955},
file = {:home/chiroptera/Dropbox/mendeley/Qian et al. - 2010 - Combined multiple clusterings on flow cytometry data to automatically identify chronic lymphocytic leukemia.pdf:pdf},
isbn = {9781424467761},
journal = {ICBBT 2010 - 2010 International Conference on Bioinformatics and Biomedical Technology},
keywords = {Chronic lymphocytic leukemia,Clustering,Flow cytometry},
pages = {305--309},
title = {{Combined multiple clusterings on flow cytometry data to automatically identify chronic lymphocytic leukemia}},
year = {2010}
}
@article{davies1979cluster,
author = {Davies, David L and Bouldin, Donald W},
file = {:home/chiroptera/Dropbox/mendeley/Davies, Bouldin - 1979 - A cluster separation measure.pdf:pdf},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
number = {2},
pages = {224--227},
publisher = {IEEE},
title = {{A cluster separation measure}},
year = {1979}
}
@article{Che2008,
abstract = {Graphics processors (GPUs) provide a vast number of simple, data-parallel, deeply multithreaded cores and high memory bandwidths. GPU architectures are becoming increasingly programmable, offering the potential for dramatic speedups for a variety of general-purpose applications compared to contemporary general-purpose processors (CPUs). This paper uses NVIDIA's C-like CUDA language and an engineering sample of their recently introduced GTX 260 GPU to explore the effectiveness of GPUs for a variety of application types, and describes some specific coding idioms that improve their performance on the GPU. GPU performance is compared to both single-core and multicore CPU performance, with multicore CPU implementations written using OpenMP. The paper also discusses advantages and inefficiencies of the CUDA programming model and some desirable features that might allow for greater ease of use and also more readily support a larger body of applications. © 2008 Elsevier Inc. All rights reserved.},
author = {Che, Shuai and Boyer, Michael and Meng, Jiayuan and Tarjan, David and Sheaffer, Jeremy W. and Skadron, Kevin},
doi = {10.1016/j.jpdc.2008.05.014},
file = {:home/chiroptera/Dropbox/mendeley/Che et al. - 2008 - A performance study of general-purpose applications on graphics processors using CUDA.pdf:pdf},
isbn = {0743-7315},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
keywords = {CUDA,GPGPU,GPU,Graphics processors,Heterogeneous computing organizations,Manycore,Multicore,OpenMP,Parallel programming},
number = {10},
pages = {1370--1380},
title = {{A performance study of general-purpose applications on graphics processors using CUDA}},
volume = {68},
year = {2008}
}
@article{Nadungodage2013,
abstract = {Recommendation systems are a popular marketing strategy for online service providers. These systems predict a customer's future preferences from the past behaviors of that customer and the other customers. Most of the popular online stores process millions of transactions per day; therefore, providing quick and quality recommendations using the large amount of data collected from past transactions can be challenging. Parallel processing power of GPUs can be used to accelerate the recommendation process. However, the amount of memory available on a GPU card is limited; thus, a number of passes may be required to completely process a large-scale dataset. This paper proposes two parallel, item-based recommendation algorithms implemented using the CUDA platform. Considering the high sparsity of the user-item data, we utilize two compression techniques to reduce the required number of passes and increase the speedup. The experimental results on synthetic and real-world datasets show that our algorithms outperform the respective CPU implementations and also the na{\~{A}}¯ve GPU implementation which does not use compression. {\^{A}}© 2013 IEEE.},
author = {Nadungodage, Chandima Hewa and Xia, Yuni and Lee, John Jaehwan and Lee, Myungcheol and Park, Choon Seo},
doi = {10.1109/BigData.2013.6691571},
file = {:home/chiroptera/Dropbox/mendeley/Nadungodage et al. - 2013 - GPU accelerated item-based collaborative filtering for big-data applications.pdf:pdf},
isbn = {9781479912926},
journal = {Proceedings - 2013 IEEE International Conference on Big Data, Big Data 2013},
keywords = {CUDA,big-data,collaborative filtering,recommendation systems GPU},
pages = {175--180},
title = {{GPU accelerated item-based collaborative filtering for big-data applications}},
year = {2013}
}
@incollection{fred2002evidence,
author = {Fred, Ana and Jain, Anil K},
booktitle = {Structural, syntactic, and statistical pattern recognition},
file = {:home/chiroptera/Dropbox/mendeley/Fred, Jain - 2002 - Evidence accumulation clustering based on the k-means algorithm.pdf:pdf},
pages = {442--451},
publisher = {Springer},
title = {{Evidence accumulation clustering based on the k-means algorithm}},
year = {2002}
}
@article{Yu2010a,
abstract = {KDD Cup 2010 is an educational data mining competition. Participants are asked to learn a model from students’ past behavior and then predict their future performance. At National Taiwan University, we organized a course for KDD Cup 2010. Most student sub-teams expanded features by various binarization and discretization techniques. The resulting sparse feature sets were trained by logistic regression (using LIBLINEAR). One sub- team considered condensed features using simple statistical techniques and applied random forest (through Weka) for training. Initial development was conducted on an internal split of training data for training and validation. We identified some useful feature combinations to improve the performance. For the final submission, we combined results of student sub- teams by regularized linear regression. Our team is the first prize winner of both tracks (all teams and student teams) of KDD Cup 2010.},
author = {Yu, Hf and Lo, Hy and Hsieh, Hp},
file = {:home/chiroptera/Dropbox/mendeley/Yu, Lo, Hsieh - 2010 - Feature engineering and classifier ensemble for KDD cup 2010.pdf:pdf},
journal = {{\ldots} of the KDD Cup 2010 {\ldots}},
keywords = {classification,classifier ensemble,educational data mining,feature engineering,linear,logistic regression,random forest},
pages = {1--12},
title = {{Feature engineering and classifier ensemble for KDD cup 2010}},
url = {https://bitbucket.org/jzbontar/ref/src/6368ec74a72c/test/data/ref/documents/Yu et al - 2010 - Feature engineering and classifier ensemble for KDD cup 2010 - 2.pdf},
year = {2010}
}
@article{Ji2011,
abstract = {Modern General Purpose Graphics Processing Units (GPGPUs) provide high degrees of parallelism in computation and memory access, making them suitable for data parallel applications such as those using the elastic MapReduce model. Yet designing a MapReduce framework for GPUs faces significant challenges brought by their multi-level memory hierarchy. Due to the absence of atomic operations in the earlier generations of GPUs, existing GPU MapReduce frameworks have problems in handling input/output data with varied or unpredictable sizes. Also, existing frameworks utilize mostly a single level of memory, $\backslash$emph{\{}i.e.{\}}, the relatively spacious yet slow global memory. In this work, we attempt to explore the potential benefit of enabling a GPU MapReduce framework to use multiple levels of the GPU memory hierarchy. We propose a novel GPU data staging scheme for MapReduce workloads, tailored toward the GPU memory hierarchy. Centering around the efficient utilization of the fast but very small shared memory, we designed and implemented a GPU MapReduce framework, whose key techniques include (1) shared memory staging area management, (2) thread-role partitioning, and (3) intra-block thread synchronization. We carried out evaluation with five popular MapReduce workloads and studied their performance under different GPU memory usage choices. Our results reveal that exploiting GPU shared memory is highly promising for the Map phase (with an average 2.85x speedup over using global memory only), while in the Reduce phase the benefit of using shared memory is much less pronounced, due to the high input-to-output ratio. In addition, when compared to Mars, an existing GPU MapReduce framework, our system is shown to bring a significant speedup in Map/Reduce phases.},
author = {Ji, Feng and Ma, Xiaosong},
doi = {10.1109/IPDPS.2011.80},
file = {:home/chiroptera/Dropbox/mendeley/Ji, Ma - 2011 - Using Shared Memory to Accelerate MapReduce on Graphics Processing Units.pdf:pdf},
isbn = {978-0-7695-4385-7},
issn = {1530-2075},
journal = {2011 IEEE International Parallel {\&} Distributed Processing Symposium},
keywords = {MapReduce,gpu},
mendeley-tags = {MapReduce,gpu},
pages = {805--816},
title = {{Using Shared Memory to Accelerate MapReduce on Graphics Processing Units}},
year = {2011}
}
@article{Perez2007,
abstract = {Python offers basic facilities for interactive work and a comprehensive library on top of which more sophisticated systems can be built. The IPython project provides on enhanced interactive environment that includes, among other features, support for data visualization and facilities for distributed and parallel computation},
author = {P{\'{e}}rez, Fernando and Granger, Brian E.},
doi = {10.1109/MCSE.2007.53},
file = {:home/chiroptera/Dropbox/mendeley/P{\'{e}}rez, Granger - 2007 - IPython A system for interactive scientific computing.pdf:pdf},
isbn = {3518437208},
issn = {15219615},
journal = {Computing in Science and Engineering},
number = {3},
pages = {21--29},
title = {{IPython: A system for interactive scientific computing}},
volume = {9},
year = {2007}
}
@article{Manju2014,
abstract = {This paper makes an exhaustive survey of various applications of Quantum inspired computational intelligence (QCI) techniques proposed till date. Definition, categorization and motivation for QCI techniques are stated clearly. Major Drawbacks and challenges are discussed. The significance of this work is that it presents an overview on applications of QCI in solving various problems in engineering, which will be very much useful for researchers on Quantum computing in exploring this upcoming and young discipline.[PUBLICATION ABSTRACT]},
author = {Manju, a. and Nigam, M. J.},
doi = {10.1007/s10462-012-9330-6},
file = {:home/chiroptera/Dropbox/mendeley/Manju, Nigam - 2014 - Applications of quantum inspired computational intelligence A survey.pdf:pdf},
issn = {02692821},
journal = {Artificial Intelligence Review},
keywords = {Computational intelligence,Quantum computing,Quantum mechanics},
pages = {79--156},
title = {{Applications of quantum inspired computational intelligence: A survey}},
volume = {42},
year = {2014}
}
@article{Wang2011,
author = {Wang, Wei and Huang, Yongzhong and Guo, Shaozhong},
file = {:home/chiroptera/Dropbox/mendeley//Wang, Huang, Guo - 2011 - Design and Implementation of GPU-Based Prim ' s Algorithm.pdf:pdf},
journal = {International Journal of Modern Education and Computer Science},
number = {July},
pages = {55--62},
title = {{Design and Implementation of GPU-Based Prim ' s Algorithm}},
volume = {4},
year = {2011}
}
@article{Lopes2011,
abstract = {Graphics Processing Units (GPUs) placed at our disposal an unprecedented computational-power, largely surpassing the performance of cutting-edge CPUs (Central Processing Units). The high-parallelism inherent to the GPU makes this device especially well-suited to address Machine Learning (ML) problems with prohibitively computational intensive tasks. Nevertheless, few ML algorithms have been implemented on the GPU and most are not openly shared, posing difficulties for researchers and engineers aiming to develop GPU-based systems. To mitigate this problem, we propose the creation of an open source GPU Machine Learning Library (GPUMLib) that aims to provide the building blocks for the development of efficient GPU ML software. Experimental results on benchmark datasets show that the algorithms already implemented yield significant time savings over the CPU counterparts.},
author = {Lopes, Noel and Ribeiro, Bernardete},
file = {:home/chiroptera/Dropbox/mendeley/Lopes, Ribeiro - 2011 - GPUMLib An efficient open-source GPU machine learning library.pdf:pdf},
journal = {{\ldots} Journal of Computer Information Systems and {\ldots}},
keywords = {gpu,gpu computing,machine learning,machine learning algorithms},
mendeley-tags = {gpu,machine learning},
pages = {355--362},
title = {{GPUMLib: An efficient open-source GPU machine learning library}},
url = {http://www.ece.neu.edu/groups/nucar/NUCARTALKS/GPUMLib.pdf},
volume = {3},
year = {2011}
}
@article{Horn2001,
abstract = {We discuss novel clustering methods that are based on mapping data points to a Hilbert space by means of a Gaussian kernel. The first method, support vector clustering (SVC), searches for the smallest sphere enclosing data images in Hilbert space. The second, quantum clustering (QC), searches for the minima of a potential function defined in such a Hilbert space. In SVC, the minimal sphere, when mapped back to data space, separates into several components, each enclosing a separate cluster of points. A soft margin constant helps in coping with outliers and overlapping clusters. In QC, minima of the potential define cluster centers, and equipotential surfaces are used to construct the clusters. In both methods, the width of the Gaussian kernel controls the scale at which the data are probed for cluster formations. We demonstrate the performance of the algorithms on several data sets.},
author = {Horn, David},
doi = {10.1016/S0378-4371(01)00442-3},
file = {:home/chiroptera/Dropbox/mendeley/Horn - 2001 - Clustering via Hilbert space.pdf:pdf},
issn = {03784371},
journal = {Physica A: Statistical Mechanics and its Applications},
keywords = {Clustering,Hilbert space,Kernel methods,Scale-space clustering,Schr{\"{o}}dinger equation,Support vector clustering},
month = {dec},
number = {1-4},
pages = {70--79},
title = {{Clustering via Hilbert space}},
url = {http://www.sciencedirect.com/science/article/pii/S0378437101004423},
volume = {302},
year = {2001}
}
@article{hunter2007matplotlib,
author = {Hunter, John D},
journal = {Computing in science and engineering},
number = {3},
pages = {90--95},
title = {{Matplotlib: A 2D graphics environment}},
volume = {9},
year = {2007}
}
@article{Mokhtari2014,
annote = {This paper proposes a programming model for the GPU to deal with data that exceeds the memoy of the device. It optimizes CPU-GPU communication.

It also has a chapter reviewing GPU which can be useful for state of the art.},
author = {Mokhtari, Reza and Stumm, Michael},
doi = {10.1109/IPDPS.2014.89},
file = {:home/chiroptera/Dropbox/mendeley/Mokhtari, Stumm - 2014 - BigKernel -- High Performance CPU-GPU Communication Pipelining for Big Data-Style Applications.pdf:pdf},
isbn = {978-1-4799-3800-1},
issn = {23321237},
journal = {Proceedings of the 2014 IEEE 28th International Parallel and Distributed Processing Symposium},
keywords = {CPU,GPU,communication,cpu-gpu,gpu,memory management,optimization,programming model,stream processing},
mendeley-tags = {cpu-gpu,gpu,memory management,programming model},
pages = {819--828},
title = {{BigKernel -- High Performance CPU-GPU Communication Pipelining for Big Data-Style Applications}},
url = {http://dx.doi.org/10.1109/IPDPS.2014.89},
year = {2014}
}
@article{Lourenco2010,
abstract = {This work focuses on the scalability of the Evidence Accumulation Clustering (EAC) method. We first address the space complexity of the co-association matrix. The sparseness of the matrix is related to the construction of the clustering ensemble. Using a split and merge strategy combined with a sparse matrix representation, we empirically show that a linear space complexity is achievable in this framework, leading to the scalability of EAC method to clustering large data-sets.},
annote = {To scale EAC, they use only the upper triangular of the co-association with a sparse representation.

The focus of the paper is studying how the choice of {\{}Kmin,Kmax{\}} affects the sparsity of the co-association matrix and, by consequence, the scalability.},
author = {Louren{\c{c}}o, Andr{\'{e}} and Fred, Ana L N and Jain, Anil K.},
doi = {10.1109/ICPR.2010.197},
file = {:home/chiroptera/Dropbox/mendeley/Louren{\c{c}}o, Fred, Jain - 2010 - On the scalability of evidence accumulation clustering.pdf:pdf},
isbn = {9780769541099},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
keywords = {Cluster analysis,Cluster fusion,Combining clustering partitions,Evidence accumulation,Large data-sets},
pages = {782--785},
pmid = {5596045},
title = {{On the scalability of evidence accumulation clustering}},
volume = {0},
year = {2010}
}
@article{Varshavsky2007,
author = {Varshavsky, Roy and Horn, David and Linial, Michal},
file = {:home/chiroptera/Dropbox/mendeley/Varshavsky, Horn, Linial - 2007 - Clustering Algorithms Optimizer A Framework for Large Datasets.pdf:pdf},
isbn = {3540720308},
issn = {03029743},
pages = {85--96},
title = {{Clustering Algorithms Optimizer : A Framework for Large Datasets}},
year = {2007}
}
@online{hdf5,
annote = {http://www.hdfgroup.org/HDF5/},
author = {{The HDF Group}},
title = {{Hierarchical Data Format, version 5}}
}
@book{starck2007astronomical,
author = {Starck, J-L and Murtagh, Fionn},
file = {:home/chiroptera/Dropbox/mendeley/Starck, Murtagh - 2007 - Astronomical image and data analysis.pdf:pdf},
publisher = {Springer Science {\&} Business Media},
title = {{Astronomical image and data analysis}},
year = {2007}
}
@article{Demar2012,
abstract = {Exascale science translates to big data. In the case of the Large Hadron Collider (LHC), the data is not only immense, it is also globally distributed. Fermilab is host to the LHC Compact Muon Solenoid (CMS) experiment's US Tier-1 Center, the largest of the LHC Tier-1s. The Laboratory must deal with both scaling and wide-area distribution challenges in processing its CMS data. Fortunately, evolving technologies in the form of 100Gigabit ethernet, multi-core architectures, and GPU processing provide tools to help meet these challenges. Current Fermilab R{\&}amp;D efforts in these areas include optimization of network I/O handling in multi-core systems, modification of middleware to improve application performance in 100GE network environments, and network path reconfiguration and analysis for effective use of high bandwidth networks. This poster will describe the ongoing network-related R{\&}amp;D activities at Fermilab as a mosaic of efforts that combine to facilitate big data processing and movement. © 2012 IEEE.},
author = {Demar, Phillip J. and Dykstra, David and Garzoglio, Gabriele and Mhashilkar, Parag and Rajendran, Anupam and Wu, Wenji},
doi = {10.1109/SC.Companion.2012.215},
file = {:home/chiroptera/Dropbox/mendeley/Demar et al. - 2012 - Big data networking at fermilab.pdf:pdf},
isbn = {9780769549569},
journal = {Proceedings - 2012 SC Companion: High Performance Computing, Networking Storage and Analysis, SCC 2012},
keywords = {100GE network,GPU processing,Large Hadron Collider experiments,big data,big data processing,cluster,gpu,multi-core systems,scaling,wide-area distribution},
mendeley-tags = {big data,cluster,gpu},
number = {1007115},
pages = {1400},
title = {{Big data networking at fermilab}},
year = {2012}
}
@article{Malakar2013,
author = {Malakar, Ranajoy and Vydyanathan, Naga},
doi = {10.1109/ParCompTech.2013.6621392},
file = {:home/chiroptera/Dropbox/mendeley/Malakar, Vydyanathan - 2013 - A CUDA-enabled hadoop cluster for fast distributed image processing.pdf:pdf},
isbn = {9781479915910},
journal = {2013 National Conference on Parallel Computing Technologies, PARCOMPTECH 2013},
keywords = {CUDA,GPGPU,Hadoop,Map-reduce},
title = {{A CUDA-enabled hadoop cluster for fast distributed image processing}},
year = {2013}
}
@article{Kang2011,
author = {Kang, U and Tsourakakis, Charalampos E and Faloutsos, Christos},
file = {:home/chiroptera/Dropbox/mendeley/Kang, Tsourakakis, Faloutsos - 2011 - PEGASUS Mining Peta-Scale Graphs.pdf:pdf},
journal = {Knowledge and Information Systems},
keywords = {generalized iterative matrix-vector,gim-v,graph mining,hadoop,multiplication,pegasus},
number = {2},
pages = {303--325},
title = {{PEGASUS : Mining Peta-Scale Graphs}},
volume = {27},
year = {2011}
}
@article{Sotoca2005,
author = {Sotoca, J M and Sanchez, J S and Mollineda, R a},
file = {:home/chiroptera/Dropbox/mendeley/Sotoca, Sanchez, Mollineda - 2005 - A review of data complexity measures and their applicability to pattern classification problems.pdf:pdf},
isbn = {8497324498},
journal = {Actas del III Taller Nacional de Mineria de Datos y Aprendizaje. TAMIDA},
pages = {77--83},
title = {{A review of data complexity measures and their applicability to pattern classification problems}},
year = {2005}
}
@article{Wiebe2014,
abstract = {We present several quantum algorithms for performing nearest-neighbor learning. At the core of our algorithms are fast and coherent quantum methods for computing distance metrics such as the inner product and Euclidean distance. We prove upper bounds on the number of queries to the input data required to compute these metrics. In the worst case, our quantum algorithms lead to polynomial reductions in query complexity relative to the corresponding classical algorithm. In certain cases, we show exponential or even super-exponential reductions over the classical analog. We study the performance of our quantum nearest-neighbor algorithms on several real-world binary classification tasks and find that the classification accuracy is competitive with classical methods.},
archivePrefix = {arXiv},
arxivId = {1401.2142},
author = {Wiebe, Nathan and Kapoor, Ashish and Svore, Krysta},
eprint = {1401.2142},
file = {:home/chiroptera/Dropbox/mendeley/Wiebe, Kapoor, Svore - 2014 - Quantum Algorithms for Nearest-Neighbor Methods for Supervised and Unsupervised Learning.pdf:pdf},
journal = {arXiv preprint arXiv:1401.2142},
pages = {31},
title = {{Quantum Algorithms for Nearest-Neighbor Methods for Supervised and Unsupervised Learning}},
url = {http://arxiv.org/abs/1401.2142},
year = {2014}
}
@article{jarvis1973clustering,
author = {Jarvis, Raymond A and Patrick, Edward A},
file = {:home/chiroptera/Dropbox/mendeley/Jarvis, Patrick - 1973 - Clustering Using a Similarity Measure Based on Shared Near Neighbors.pdf:pdf},
journal = {Computers, IEEE Transactions on},
number = {11},
pages = {1025--1034},
publisher = {IEEE},
title = {{Clustering using a similarity measure based on shared near neighbors}},
volume = {100},
year = {1973}
}
@article{Su2012,
abstract = {GPU (Graphics Processing Unit) has a great impact on computing field. To enhance the performance of computing systems, researchers and developers use the parallel computing architecture of GPU. On the other hand, to reduce the development time of new products, two programming models are included in GPU, which are OpenCL (Open Computing Language) and CUDA (Compute Unified Device Architecture). The benefit of involving the two programming models in GPU is that researchers and developers don't have to understand OpenGL, DirectX or other program design, but can use GPU through simple programming language. OpenCL is an open standard API, which has the advantage of cross-platform. CUDA is a parallel computer architecture developed by NVIDIA, which includes Runtime API and Driver API. Compared with OpenCL, CUDA is with better performance. In this paper, we used plenty of similar kernels to compare the computing performance of C, OpenCL and CUDA, the two kinds of API's on NVIDIA Quadro 4000 GPU. The experimental result showed that, the executive time of CUDA Driver API was 94.9{\%}{\~{}}99.0{\%} faster than that of C, while and the executive time of CUDA Driver API was 3.8{\%}{\~{}}5.4{\%} faster than that of OpenCL. Accordingly, the cross-platform characteristic of OpenCL did not affect the performance of GPU.},
annote = {Comparison between OpenCL and CUDA.},
author = {Su, Ching Lung and Chen, Po Yu and Lan, Chun Chieh and Huang, Long Sheng and Wu, Kuo Hsuan},
doi = {10.1109/APCCAS.2012.6419068},
file = {:home/chiroptera/Dropbox/mendeley/Su et al. - 2012 - Overview and comparison of OpenCL and CUDA technology for GPGPU.pdf:pdf},
isbn = {9781457717291},
journal = {IEEE Asia-Pacific Conference on Circuits and Systems, Proceedings, APCCAS},
keywords = {comparison,cuda,opencl},
mendeley-tags = {comparison,cuda,opencl},
pages = {448--451},
title = {{Overview and comparison of OpenCL and CUDA technology for GPGPU}},
year = {2012}
}
@article{fisher1936use,
author = {Fisher, Ronald A},
file = {:home/chiroptera/Dropbox/mendeley/Fisher - 1936 - The use of multiple measurements in taxonomic problems.pdf:pdf},
journal = {Annals of eugenics},
number = {2},
pages = {179--188},
publisher = {Wiley Online Library},
title = {{The use of multiple measurements in taxonomic problems}},
volume = {7},
year = {1936}
}
@inproceedings{hung2013quantum,
author = {Hung, Chih-Cheng and Casper, Ellis and Kuo, Bor-chen and Liu, Wenping and Jung, Edward and Yang, Ming and Yu, Xiaoyi and Jung, Edward and Yang, Ming},
booktitle = {Geoscience and Remote Sensing Symposium (IGARSS), 2013 IEEE International},
file = {:home/chiroptera/Dropbox/mendeley//Hung et al. - 2013 - A Quantum-Modeled Artificial Bee Colony clustering algorithm for remotely sensed multi-band image segmentation.pdf:pdf},
isbn = {9781479911141},
organization = {IEEE},
pages = {2501--2504},
title = {{A Quantum-Modeled Artificial Bee Colony clustering algorithm for remotely sensed multi-band image segmentation}},
year = {2013}
}
@article{Rajaraman2011a,
abstract = {At the highest level of description, this book is about data mining. However, it focuses on data mining of very large amounts of data, that is, data so large it does not fit in main memory. Because of the emphasis on size, many of our examples are about the Web or data derived from the Web. Further, the book takes an algorithmic point of view: data mining is about applying algorithms to data, rather than using data to train a machine-learning engine of some sort.},
author = {Rajaraman, Anand and Ullman, Jeffrey D},
doi = {10.1017/CBO9781139058452},
file = {:home/chiroptera/Dropbox/mendeley/Rajaraman, Ullman - 2011 - Mining of Massive Datasets(2).pdf:pdf},
isbn = {9781139058452},
issn = {01420615},
journal = {Lecture Notes for Stanford CS345A Web Mining},
pages = {328},
title = {{Mining of Massive Datasets}},
url = {http://ebooks.cambridge.org/ref/id/CBO9781139058452},
volume = {67},
year = {2011}
}
@article{CUDAProgrammingGuideV1,
abstract = {Graphics processing units (GPUs) can provide excellent speedups on some, but not all, general-purpose workloads. Using a set of computational GPU kernels as examples, the authors show how to adapt kernels to utilize the architectural features of a GeForce 8800 GPU and what finally limits the achievable performance.},
author = {{NVIDIA Corporation}},
doi = {10.1109/MCSE.2009.48},
file = {:home/chiroptera/Dropbox/mendeley/NVIDIA Corporation - 2007 - NVIDIA CUDA Compute Unified Device Architecture Programming Guide.pdf:pdf},
issn = {15219615},
title = {{NVIDIA CUDA Compute Unified Device Architecture: Programming Guide}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4814979},
year = {2007}
}
@article{Weinstein2009,
abstract = {A given set of data points in some feature space may be associated with a Schr{\"{o}}dinger equation whose potential is determined by the data. This is known to lead to good clustering solutions. Here we extend this approach into a full-fledged dynamical scheme using a time-dependent Schr{\"{o}}dinger equation. Moreover, we approximate this Hamiltonian formalism by a truncated calculation within a set of Gaussian wave functions (coherent states) centered around the original points. This allows for analytic evaluation of the time evolution of all such states opening up the possibility of exploration of relationships among data points through observation of varying dynamical distances among points and convergence of points into clusters. This formalism may be further supplemented by preprocessing such as dimensional reduction through singular-value decomposition or feature filtering.},
annote = {From Duplicate 2 (Dynamic quantum clustering: a method for visual exploration of structures in data - Weinstein, Marvin; Horn, David)

This articles expands the work of Horn {\&}amp; Gottlieb with the concept of quantum tunneling for temporal evolution of the particles (datapoints) into the nearby potential minima. The choice of a good m (eq. 6) (which is dependant on sigma) will assure lots of minima in V (revealing more structure) while at the same time allowing good tunneling (connection between particles).},
archivePrefix = {arXiv},
arxivId = {arXiv:0908.2644v1},
author = {Weinstein, Marvin and Horn, David},
doi = {10.1103/PhysRevE.80.066117},
eprint = {arXiv:0908.2644v1},
file = {:home/chiroptera/Dropbox/mendeley//Weinstein, Horn - 2009 - Dynamic quantum clustering a method for visual exploration of structures in data.pdf:pdf},
issn = {1539-3755},
journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
keywords = {clustering,quantum},
mendeley-tags = {clustering,quantum},
number = {6},
pages = {1--15},
title = {{Dynamic quantum clustering: a method for visual exploration of structures in data}},
url = {http://link.aps.org/doi/10.1103/PhysRevE.80.066117},
volume = {80},
year = {2009}
}
@article{sneath1962numerical,
author = {Sneath, Peter H A and Sokal, Robert R},
journal = {Nature},
keywords = {Sneath1962},
mendeley-tags = {Sneath1962},
number = {4818},
pages = {855--860},
publisher = {Nature Publishing Group},
title = {{Numerical taxonomy}},
volume = {193},
year = {1962}
}
@article{Steinbach2000,
author = {Steinbach, Michael and Karypis, George and Kumar, Vipin and Others},
doi = {10.1.1.125.9225},
file = {:home/chiroptera/Dropbox/mendeley/Steinbach et al. - 2000 - A comparison of document clustering techniques.pdf:pdf},
journal = {KDD workshop on text mining},
keywords = {document clustering},
number = {1},
pages = {525--526},
title = {{A comparison of document clustering techniques}},
volume = {400},
year = {2000}
}
@article{Zhang2010,
abstract = {Document clustering plays an important role in data mining systems. Recently, a flocking-based document clustering algorithm has been proposed to solve the problem through simulation resembling the flocking behavior of birds in nature. This method is superior to other clustering algorithms, including k-means, in the sense that the outcome is not sensitive to the initial state. One limitation of this approach is that the algorithmic complexity is inherently quadratic in the number of documents. As a result, execution time becomes a bottleneck with large number of documents. In this paper, we assess the benefits of exploiting the computational power of Beowulf-like clusters equipped with contemporary Graphics Processing Units (GPUs) as a means to significantly reduce the runtime of flocking-based document clustering. Our framework scales up to over one million documents processed simultaneously in a sixteen-node moderate GPU cluster. Results are also compared to a four-node cluster with higher-end GPUs. On these clusters, we observe 30X-50X speedups, which demonstrate the potential of GPU clusters to efficiently solve massive data mining problems. Such speedups combined with the scalability potential and accelerator-based parallelization are unique in the domain of document-based data mining, to the best of our knowledge.},
author = {Zhang, Yongpeng and Mueller, Frank and Cui, Xiaohui and Potok, Thomas},
doi = {10.1109/IPDPS.2010.5470429},
file = {:home/chiroptera/Dropbox/mendeley/Zhang et al. - Unknown - Large-Scale Multi-Dimensional Document Clustering on GPU Clusters.pdf:pdf},
isbn = {978-1-4244-6442-5},
issn = {1530-2075},
journal = {2010 IEEE International Symposium on Parallel {\&} Distributed Processing (IPDPS)},
pages = {1--10},
title = {{Large-scale multi-dimensional document clustering on GPU clusters}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5470429},
year = {2010}
}
@article{Burges1998,
abstract = {The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Burges, CJC Christopher J C},
doi = {10.1023/A:1009715923555},
editor = {Fayyad, Usama},
eprint = {1111.6189v1},
institution = {Bell Laboratories, Lucent Technologies},
isbn = {0818672404},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {pattern recognition,statistical learning theory,support vector machines,vc dimension},
number = {2},
pages = {121--167},
pmid = {5207842081938259593},
publisher = {Springer},
series = {NetGames '06},
title = {{A Tutorial on Support Vector Machines for Pattern Recognition}},
url = {http://www.springerlink.com/index/Q87856173126771Q.pdf$\backslash$nhttp://link.springer.com/article/10.1023/A:1009715923555},
volume = {2},
year = {1998}
}
@article{yuk13oro,
abstract = {Diagnostics for gastro-esophageal reflux disease (GERD) are suboptimal because of limited sensitivity. We performed in vitro and in vivo studies to systematically assess the performance characteristics of an oropharyngeal pH probe. In vitro studies compared the oropharyngeal probe with a standard pH catheter in liquid and aerosolized solutions, pH 1-7. The accuracy of measurements, deviation from target pH, and time to equilibrium pH were determined. Simultaneous distal esophageal pH measurements were obtained in 11 patients with GERD. Oropharyngeal and distal esophageal reflux parameters were measured for controls (n = 20), patients with GERD (n = 17), and patients with chronic laryngitis (n = 10). In the liquid phase, at pH 4-5, the oropharyngeal probe had less deviation from the target value than the standard catheter; deviation in the vapor phase was similar (0.4 pH units). Median (interquartile) time to reach equilibrium pH was significantly (P < 0.001) faster with the oropharyngeal than the standard probe. In comparing simultaneous distal esophageal pH characteristics, 96{\%} of recordings with the new and standard probes were in agreement to within ± 1.0 pH unit; 71{\%} of recordings were in agreement within ± 0.5 pH units. Patients with chronic laryngitis had significantly higher levels of oropharyngeal acid exposure at pH <4, <5, and <6, in the upright position than patients with GERD or controls (P < .001). Oropharyngeal pH monitoring appears to be more sensitive than traditional pH monitoring in evaluation of patients with extraesophageal reflux. It is a promising tool in evaluation of this difficult group of patients. {\{}$\backslash$copyright{\}} 2013 Blackwell Publishing Ltd.},
annote = {Paper of data set esopH.rda taken from 

http://biostat.mc.vanderbilt.edu/wiki/Main/DataSets?CGISESSID=10713f6d891653ddcbb7ddbdd9cffb79},
author = {Yuksel, E S and Slaughter, J C and Mukhtar, N and Ochieng, M and Sun, G and Goutte, M and Muddana, S and {Gaelyn Garrett}, C and Vaezi, M F},
issn = {1365-2982},
journal = {Neurogastroenterology and motility : the official journal of the European Gastrointestinal Motility Society},
keywords = {measurement-instruments,smooth-bland-altman-plots,teaching},
number = {5},
pmid = {23495894},
title = {{An oropharyngeal pH monitoring device to evaluate patients with chronic laryngitis.}},
url = {http://view.ncbi.nlm.nih.gov/pubmed/23495894 http://biostat.mc.vanderbilt.edu/wiki/Main/DataSets?CGISESSID=10713f6d891653ddcbb7ddbdd9cffb79},
volume = {25},
year = {2013}
}
@article{Siddiqui2013,
author = {Siddiqui, Mohammad Khubeb and Naahid, Shams},
doi = {10.14257/ijdta.2013.6.5.03},
file = {:home/chiroptera/Dropbox/mendeley/Siddiqui, Naahid - 2013 - Analysis of KDD CUP 99 Dataset using Clustering based Data Mining.pdf:pdf},
issn = {20054270},
journal = {International Journal of Database Theory and Application},
keywords = {clustering,intrusion detection,k-means,kdd 99 dataset},
number = {5},
pages = {23--34},
title = {{Analysis of KDD CUP 99 Dataset using Clustering based Data Mining}},
url = {http://www.sersc.org/journals/IJDTA/vol6{\_}no5/3.pdf},
volume = {6},
year = {2013}
}
@article{Rosenbaum2011,
abstract = {We consider a generalization of the standard oracle model in which the oracle acts on the target with a permutation which is selected according to internal random coins. We show new exponential quantum speedups which may be obtained over classical algorithms in this oracle model. Even stronger, we describe several problems which are impossible to solve classically but can be solved by a quantum algorithm using a single query; we show that such infinity-vs-one separations between classical and quantum query complexities can be constructed from any separation between classical and quantum query complexities (in the unbounded-error regime). We also give conditions to determine when oracle problems---either in the standard model, or in any of the generalizations we consider---cannot be solved with success probability better than random guessing would achieve. In the oracle model with internal randomness where the goal is to gain any nonzero advantage over guessing, we prove (roughly speaking) that (k) quantum queries are equivalent in power to (2k) classical queries, thus extending results of Meyer and Pommersheim, and Montanaro, Nishimura and Raymond.},
archivePrefix = {arXiv},
arxivId = {1111.1462v1},
author = {Rosenbaum, David and Harrow, Aram W.},
eprint = {1111.1462v1},
file = {:home/chiroptera/Dropbox/mendeley/Rosenbaum, Harrow - 2011 - Uselessness for an Oracle Model with Internal Randomness.pdf:pdf},
issn = {15337146},
journal = {arXiv:1111.1462},
keywords = {oracles,quantum},
mendeley-tags = {oracles,quantum},
pages = {1--23},
title = {{Uselessness for an Oracle Model with Internal Randomness}},
year = {2011}
}
@article{Nvidia2014,
author = {Nvidia},
file = {:home/chiroptera/Dropbox/mendeley/Nvidia - 2015 - CUDA C Programming Guide.pdf:pdf},
journal = {Programming Guides},
number = {August},
title = {{CUDA C Programming Guide}},
year = {2015}
}
@article{Woong-KeeLoh2014,
author = {Woong-KeeLoh and Kim, Young-Kuk},
doi = {10.1109/BDCloud.2014.130},
file = {:home/chiroptera/Dropbox/mendeley/Woong-KeeLoh, Kim - 2014 - A GPU-accelerated Density-Based Clustering Algorithm.pdf:pdf},
isbn = {978-1-4799-6719-3},
journal = {2014 IEEE Fourth International Conference on Big Data and Cloud Computing},
keywords = {12,3,and there have been,bohm et al,clustering,cuda,density-based clustering,divide-and-conquer,gpu,many approaches to improve,parallel algorithm,performance,proposed an algorithm,their},
mendeley-tags = {clustering,gpu},
pages = {775--776},
title = {{A GPU-accelerated Density-Based Clustering Algorithm}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7034874},
year = {2014}
}
@article{Guide2013,
author = {Guide, Pytables User},
file = {:home/chiroptera/Dropbox/mendeley/Guide - 2013 - Hierarchical datasets in Python PyTables User G.pdf:pdf},
title = {{Hierarchical datasets in Python: PyTables User G}},
year = {2013}
}
@article{Letham2013,
abstract = {It is easy to find expert knowledge on the Internet on almost any topic, but obtaining a complete overview of a given topic is not always easy: information can be scattered across many sources and must be aggregated to be useful. We introduce a method for intelligently growing a list of relevant items, starting from a small seed of examples. Our algorithm takes advantage of the wisdom of the crowd, in the sense that there are many experts who post lists of things on the Internet. We use a collection of simple machine learning components to find these experts and aggregate their lists to produce a single complete and meaningful list. We use experiments with gold standards and open-ended experiments without gold standards to show that our method significantly outperforms the state of the art. Our method uses the ranking algorithm Bayesian Sets even when its underlying independence assumption is violated, and we provide a theoretical generalization bound to motivate its use.},
author = {Letham, Benjamin and Rudin, Cynthia and Heller, Katherine a.},
doi = {10.1007/s10618-013-0329-7},
file = {:home/chiroptera/Dropbox/mendeley/Letham, Rudin, Heller - 2013 - Growing a list.pdf:pdf},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {Collective intelligence,Internet data mining,Ranking,Set completion},
number = {3},
pages = {372--395},
title = {{Growing a list}},
volume = {27},
year = {2013}
}
@article{boruuvka1926jistem,
author = {Borůvka, Otakar},
journal = {Pr{\'{a}}ce Moravsk{\'{e}} Prirodov{\v{e}}deck{\'{e}} Spole{\v{c}}nosti},
pages = {37--58},
title = {{O jist{\'{e}}m probl{\'{e}}mu minim{\'{a}}ln{\'{\i}}m}},
volume = {3},
year = {1926}
}
@article{Nvidia2010,
author = {Nvidia},
file = {:home/chiroptera/Dropbox/mendeley/Nvidia - 2010 - Introduction to CUDA C.pdf:pdf},
journal = {Siggraph Asia 2010},
title = {{Introduction to CUDA C}},
year = {2010}
}
@article{Ertoz2003,
abstract = {Finding clusters in data, especially high dimensional data, is challenging when the clusters are of widely di ering shapes, sizes, and densities, and when the data contains noise and outliers. We present a novel clustering technique that addresses these issues. Our algorithm rst nds the nearest neighbors of each data point and then rede nes the similarity between pairs of points in terms of how many nearest neighbors the two points share. Using this de nition of similarity, our algorithm identi es core points and then builds clusters around the core points. The use of a shared nearest neighbor de nition of similarity alleviates problems with varying densities and high dimensionality, while the use of core points handles problems with shape and size. While our algorithm can nd the dense" clusters that other clustering algorithms nd, it also nds clusters that these approaches overlook, i.e., clusters of low or medium density which represent relatively uniform regions surrounded" by non-uniform or higher density areas. We experimentally show that our algorithm performs better than traditional methods (e.g., K-means, DBSCAN, CURE) on a variety of data sets: KDD Cup `99 network intrusion data, NASA Earth science time series data, and two-dimensional point sets. The run-time complexity of our technique is O(n2) if the similarity matrix has to be constructed. However, we discuss a number of optimizations that allow the algorithm to handle large data sets efficiently.},
author = {Ert{\"{o}}z, Levent and Steinbach, Michael and Kumar, Vipin},
doi = {doi:10.1137/1.9781611972733.5},
file = {:home/chiroptera/Dropbox/mendeley/Ert{\"{o}}z, Steinbach, Kumar - 2003 - Finding Clusters of Different Sizes, Shapes, and Densities in Noisy, High Dimensional Data.pdf:pdf},
isbn = {978-0-89871-545-3},
journal = {Proceedings of Second SIAM International Conference on Data Mining},
keywords = {cluster analysis,intrusion,neighborhood of a point,network,shared nearest neighbor,spatial data,time series},
pages = {47--59},
title = {{Finding Clusters of Different Sizes, Shapes, and Densities in Noisy, High Dimensional Data}},
url = {http://books.google.com/books?hl=en{\&}amp;lr={\&}amp;id=AyJ9xrrwDnIC{\&}amp;oi=fnd{\&}amp;pg=PA47{\&}amp;dq=Finding+Clusters+of+Different+Sizes+,+Shapes+,+and+Densities+in+Noisy+,+High+Dimensional+Data{\&}amp;ots=K{\_}GAiiJXYX{\&}amp;sig=1ELYJf9l1avdwaQ1F27Uw8RN384},
year = {2003}
}
@article{Ghorpade2012,
abstract = {The future of computation is the Graphical Processing Unit, i.e. the GPU. The promise that the graphics cards have shown in the field of image processing and accelerated rendering of 3D scenes, and the computational capability that these GPUs possess, they are developing into great parallel computing units. It is quite simple to program a graphics processor to perform general parallel tasks. But after understanding the various architectural aspects of the graphics processor, it can be used to perform other taxing tasks as well. In this paper, we will show how CUDA can fully utilize the tremendous power of these GPUs. CUDA is NVIDIA’s parallel computing architecture. It enables dramatic increases in computing performance, by harnessing the power of the GPU. This paper talks about CUDA and its architecture. It takes us through a comparison of CUDA C/C++ with other parallel programming languages like OpenCL and DirectCompute. The paper also lists out the common myths about CUDA and how the future seems to be promising for CUDA.},
annote = {Brief survey of GPGPU (more a introduction to the concept than a complete review of what is).

Some examples of GPGPU.},
archivePrefix = {arXiv},
arxivId = {1202.4347},
author = {Ghorpade, Jayshree},
doi = {10.5121/acij.2012.3109},
eprint = {1202.4347},
file = {:home/chiroptera/Dropbox/mendeley/Ghorpade - 2012 - GPGPU Processing in CUDA Architecture.pdf:pdf},
issn = {2229726X},
journal = {Advanced Computing: An International Journal},
keywords = {gpgpu,gpu},
mendeley-tags = {gpgpu,gpu},
number = {1},
pages = {105--120},
title = {{GPGPU Processing in CUDA Architecture}},
volume = {3},
year = {2012}
}
@article{Wang2013,
author = {Wang, Huaixiao and Liu, Jianyong and Zhi, Jun and Fu, Chengqun},
doi = {10.1155/2013/730749},
file = {:home/chiroptera/Dropbox/mendeley/Wang et al. - 2013 - The Improvement of Quantum Genetic Algorithm and Its Application on Function Optimization.pdf:pdf},
issn = {1024-123X},
journal = {Mathematical Problems in Engineering},
number = {1},
pages = {1--10},
title = {{The Improvement of Quantum Genetic Algorithm and Its Application on Function Optimization}},
url = {http://www.hindawi.com/journals/mpe/2013/730749/},
volume = {2013},
year = {2013}
}
@article{Brito1997,
abstract = {For multivariate data sets, we study the relationship between the connectivity of a mutual k-nearest-neighbor graph, and the presence of clustering structure and outliers in the data. A test for detection of clustering structure and outliers is proposed and its performance is evaluated in simulated data.},
author = {Brito, M.R. and Ch{\'{a}}vez, E.L. and a.J. Quiroz and Yukich, J.E.},
doi = {10.1016/S0167-7152(96)00213-1},
file = {:home/chiroptera/Dropbox/mendeley/Brito et al. - 1997 - Connectivity of the mutual k-nearest-neighbor graph in clustering and outlier detection.pdf:pdf},
issn = {01677152},
journal = {Statistics {\&} Probability Letters},
keywords = {clustering,mutual nearest neighbors,outlier detection},
number = {1},
pages = {33--42},
title = {{Connectivity of the mutual k-nearest-neighbor graph in clustering and outlier detection}},
volume = {35},
year = {1997}
}
@inproceedings{ArulShalom2011,
abstract = {We explore the capabilities of today's high-end Graphics processing units (GPU) on desktops to efficiently perform hierarchical agglomerative clustering (HAC) through partitioning of data. Traditional HAC has high time and memory complexities leading to low clustering efficiencies. We reduce time and memory bottlenecks of the traditional HAC algorithm by exploring the performance capabilities of the GPU, significantly accelerating the computations without compromising the accuracy of clusters. We implement the traditional HAC and the Partially Overlapping Partitioning (PoP) on GPU using Compute Unified Device Architecture (CUDA) and compare the computational performance with CPU using micro array data. The result shows that the PoP HAC and traditional HAC are up to 442 times and 66 times faster on the GPU respectively than the time taken by CPU. The PoP-enabled HAC on GPU requires only a fraction of the memory required by traditional HAC both on the CPU and GPU.},
author = {Shalom, S and Dash, Manoranjan},
booktitle = {Parallel and Distributed Computing, Applications and Technologies (PDCAT), 2011 12th International Conference on},
doi = {10.1109/PDCAT.2011.38},
file = {:home/chiroptera/Dropbox/mendeley/Shalom, Dash - 2011 - Efficient hierarchical agglomerative clustering algorithms on GPU using data partitioning.pdf:pdf},
isbn = {9780769545646},
keywords = {Computational speed-ups,Efficient partitioning,GPGPU,GPU clustering,GPU computing,GPU for acceleration,Hierarchical agglomerative clustering},
pages = {134--139},
title = {{Efficient hierarchical agglomerative clustering algorithms on GPU using data partitioning}},
year = {2011}
}
@article{McKinney2010,
abstract = {In this paper we are concerned with the practical issues of working with data sets common to finance, statistics, and other related fields. pandas is a new library which aims to facilitate working with these data sets and to provide a set of fundamental building blocks for implementing statistical models. We will discuss specific design issues encountered in the course of developing pandas with relevant examples and some comparisons with the R language. We conclude by discussing possible future directions for statistical computing and data analysis using Python.},
author = {McKinney, Wes},
file = {:home/chiroptera/Dropbox/mendeley/McKinney - 2010 - Data Structures for Statistical Computing in Python.pdf:pdf},
isbn = {0440877763224},
journal = {Proceedings of the 9th Python in Science Conference},
number = {Scipy},
pages = {51--56},
title = {{Data Structures for Statistical Computing in Python}},
url = {http://conference.scipy.org/proceedings/scipy2010/mckinney.html},
volume = {1697900},
year = {2010}
}
@article{Bennett,
author = {Bennett, Charles},
file = {:home/chiroptera/Dropbox/mendeley/Bennett - Unknown - Is quantum search practical.pdf:pdf},
pages = {22--30},
title = {{Is quantum search practical?}}
}
@article{Raymond2013,
author = {Raymond, The and Sackler, Beverly},
file = {:home/chiroptera/Dropbox/mendeley/Raymond, Sackler - 2013 - Quantum Clustering of Large Data Sets.pdf:pdf},
title = {{Quantum Clustering of Large Data Sets}},
year = {2013}
}
@inproceedings{hung2013quantumcmeans,
author = {Hung, Chih-Cheng and Casper, Ellis and Kuo, Bor-Chen and Liu, Wenping and Yu, Xiaoyi and Jung, Edward and Yang, Ming},
booktitle = {Geoscience and Remote Sensing Symposium (IGARSS), 2013 IEEE International},
file = {:home/chiroptera/Dropbox/mendeley//Hung et al. - 2013 - A Quantum-Modeled Fuzzy C-Means clustering algorithm for remotely sensed multi-band image segmentation.pdf:pdf},
organization = {IEEE},
pages = {2501--2504},
title = {{A Quantum-Modeled Fuzzy C-Means clustering algorithm for remotely sensed multi-band image segmentation}},
year = {2013}
}
@article{NathanBell2008,
author = {{Nathan Bell}, Michael Garland},
file = {:home/chiroptera/Dropbox/mendeley/Nathan Bell - 2008 - Efficient sparse matrix-vector multiplication on CUDA.pdf:pdf},
pages = {1--32},
title = {{Efficient sparse matrix-vector multiplication on CUDA}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.191.360},
year = {2008}
}
@article{Lee2012,
abstract = {A prominent parallel data processing tool MapReduce is gaining significant momentum from both industry and academia as the volume of data to analyze grows rapidly. While MapReduce is used in many areas where massive data analysis is required, there are still debates on its performance, efficiency per node, and simple abstraction. This survey intends to assist the database and open source communities in understanding various technical aspects of the MapReduce framework. In this survey, we characterize the MapReduce framework and discuss its inherent pros and cons. We then introduce its optimization strategies reported in the recent literature. We also discuss the open issues and challenges raised on parallel data analysis with MapReduce.},
author = {Lee, Kyong-Ha and Lee, Yoon-Joon and Choi, Hyunsik and Chung, Yon Dohn and Moon, Bongki},
doi = {10.1145/2094114.2094118},
file = {:home/chiroptera/Dropbox/mendeley/Lee et al. - 2012 - Parallel data processing with MapReduce.pdf:pdf},
isbn = {0163-5808},
issn = {01635808},
journal = {ACM SIGMOD Record},
keywords = {MapReduce,clusters,distributed computing,hadoop,ing,mapreduce,parallel data process-},
mendeley-tags = {MapReduce},
number = {4},
pages = {11},
title = {{Parallel data processing with MapReduce}},
volume = {40},
year = {2012}
}
@article{Benatchba2006,
abstract = {On one hand, image segmentation is a low-level processing task which consists in partitioning an image into homogeneous regions. It can be seen as being a combinatorial optimization problem. In fact, considering the huge amount of information that an image carries, it is impossible to find the best segmentation. On the other hand, quantum genetic algorithms are characterized by their high diversity, and by a good balance between global and local search. In this paper, we present a quantum genetic algorithm for image segmentation},
author = {Benatchba, K. and Koudil, M. and Boukir, Y. and Benkhelat, N.},
doi = {10.1109/IECON.2006.347758},
file = {:home/chiroptera/Dropbox/mendeley/Benatchba et al. - 2006 - Image segmentation using quantum genetic algorithms.pdf:pdf},
isbn = {1-4244-0390-1},
issn = {1553-572X},
journal = {IECON 2006 - 32nd Annual Conference on IEEE Industrial Electronics},
keywords = {Image segmentation,Optimization problem,quantum genetic algorithms},
pages = {3556--3563},
title = {{Image segmentation using quantum genetic algorithms}},
year = {2006}
}
@article{Govindaraju2006,
abstract = {We present a novel external sorting algorithm using graphics processors (GPUs) on large databases composed of billions of records and wide keys. Our algorithm uses the data parallelism within a GPU along with task parallelism by scheduling some of the memory-intensive and compute-intensive threads on the GPU. Our new sorting architecture provides multiple memory interfaces on the same PC -- a fast and dedicated memory interface on the GPU along with the main memory interface for CPU computations. As a result, we achieve higher memory bandwidth as compared to CPU-based algorithms running on commodity PCs. Our approach takes into account the limited communication bandwidth between the CPU and the GPU, and reduces the data communication between the two processors. Our algorithm also improves the performance of disk transfers and achieves close to peak I/O performance. We have tested the performance of our algorithm on the SortBenchmark and applied it to large databases composed of a few hundred Gigabytes of data. Our results on a 3 GHz Pentium IV PC with {\$}300 NVIDIA 7800 GT GPU indicate a significant performance improvement over optimized CPU-based algorithms on high-end PCs with 3.6 GHz Dual Xeon processors. Our implementation is able to outperform the current high-end PennySort benchmark and results in a higher performance to price ratio. Overall, our results indicate that using a GPU as a co-processor can significantly improve the performance of sorting algorithms on large databases.},
author = {Govindaraju, Naga and Gray, Jim and Kumar, Ritesh and Manocha, Dinesh},
doi = {10.1145/1142473.1142511},
file = {:home/chiroptera/Dropbox/mendeley/Govindaraju et al. - 2006 - GPUTeraSort high performance graphics co-processor sorting for large database management.pdf:pdf},
isbn = {1-59593-434-0},
issn = {07308078},
journal = {Sigmod},
number = {October},
pages = {325 -- 336},
title = {{GPUTeraSort: high performance graphics co-processor sorting for large database management}},
url = {http://portal.acm.org/citation.cfm?id=1142473.1142511},
year = {2006}
}
@inproceedings{He2008,
abstract = {We design and implement Mars, aMapReduce framework, on graphics processors (GPUs). MapReduce is a distributed programming framework originally proposed by Google for the ease of development of web search applications on a large number of commodity CPUs. Compared with CPUs, GPUs have an order of magnitude higher computation power and memory bandwidth, but are harder to program since their architectures are designed as a special-purpose co-processor and their programming interfaces are typically for graphics applications. As the first attempt to harness GPU’s power for MapReduce, we developed Mars on an NVIDIA G80 GPU, which contains over one hundred processors, and evaluated it in comparison with Phoenix, the state-of-the-art MapReduce framework on multi-core CPUs. Mars hides the programming complexity of the GPU behind the simple and familiar MapReduce interface. It is up to 16 times faster than its CPU-based counterpart for six common web applications on a quad-core machine. Categories},
author = {He, Bingsheng and Fang, Weibin and Luo, Qiong and Govindaraju, Naga K and Wang, Tuyong},
booktitle = {Proceedings of the 17th international conference on Parallel architectures and compilation techniques},
doi = {10.1145/1454115.1454152},
file = {:home/chiroptera/Dropbox/mendeley/He et al. - 2008 - Mars a MapReduce framework on graphics processors.pdf:pdf},
isbn = {9781605582825},
issn = {03009440},
keywords = {Data parallelism,GPGPU,Graphics Processor,MapReduce,Multi-core processors,Web Analysis},
pages = {260--269},
title = {{Mars: a MapReduce framework on graphics processors}},
url = {http://dl.acm.org/citation.cfm?id=1454152},
year = {2008}
}
@inproceedings{shuai2006quantum,
abstract = {This paper presents a new generalized quantum particle model for data self-organizing clustering. The stochas- tic motion and collision of quantum particles give rise to a stochastic process of quantum entanglement of particles. The stationary probability distribution over the configuration space of entangled particles results in the optimally clustering solution of the given data set. The quantum particle model has advantages in terms of the insensitivity to noise, the quality robustness to clustered data, the learning ability, and the suitability for high-dimensional multi-shape large-scale data sets. In comparison with the classical version of particle model and the cellular automata, the quantum particle mode has much faster speed and higher quality for clustering. The simulation and comparison show the effectiveness and good performance of the proposed quantum particle approach to data clustering.},
author = {Shuai, Dianxun and Zhang, Bin and Dong, Yumin},
booktitle = {Systems, Man and Cybernetics, 2006. SMC'06. IEEE International Conference on},
file = {:home/chiroptera/Dropbox/mendeley/Shuai, Zhang, Dong - 2006 - Quantum Particles Model for Data Clustering in Enterprise Computing.pdf:pdf},
isbn = {1424401003},
number = {2},
organization = {IEEE},
pages = {4602--4607},
title = {{Quantum Particles Model for Data Clustering in Enterprise Computing}},
volume = {6},
year = {2006}
}
@inproceedings{guha1998cure,
author = {Guha, Sudipto and Rastogi, Rajeev and Shim, Kyuseok},
booktitle = {ACM SIGMOD Record},
file = {:home/chiroptera/Dropbox/mendeley/Guha, Rastogi, Shim - 1998 - CURE an efficient clustering algorithm for large databases.pdf:pdf},
number = {2},
organization = {ACM},
pages = {73--84},
title = {{CURE: an efficient clustering algorithm for large databases}},
volume = {27},
year = {1998}
}
@article{McColl2013,
abstract = {Social networks, communication networks, busi- ness intelligence databases, and large scientific data sources now contain hundreds of millions elements with billions of relationships. The relationships in these massive datasets are changing at ever-faster rates. Through representing these datasets as dynamic and semantic graphs of vertices and edges, it is possible to characterize the structure of the relationships and to quickly respond to queries about how the elements in the set are connected. Statically computing analytics on snapshots of these dynamic graphs is frequently not fast enough to provide current and accurate information as the graph changes. This has led to the development of dynamic graph algorithms that can maintain analytic information without resorting to full static recomputation. In this work we present a novel parallel algorithm for tracking the connected components of a dynamic graph. Our approach has a low memory requirement of O(V ) and is appropriate for all graph densities. On a graph with 512 million edges, we show that our new dynamic algorithm is up to 128X faster than well-known static algorithms and that our algorithm achieves a 14X parallel speedup on a x86 64-core shared-memory system. To the best of the authors’ knowledge, this is the first parallel implementation of dynamic connected components that does not eventually require static recomputation.},
author = {McColl, Robert and Green, Oded and Bader, David a.},
doi = {10.1109/HiPC.2013.6799108},
file = {:home/chiroptera/Dropbox/mendeley/McColl, Green, Bader - 2013 - A new parallel algorithm for connected components in dynamic graphs.pdf:pdf},
isbn = {978-1-4799-0730-4},
journal = {20th Annual International Conference on High Performance Computing, HiPC 2013},
pages = {246--255},
title = {{A new parallel algorithm for connected components in dynamic graphs}},
year = {2013}
}
@article{Harish2007,
abstract = {Large graphs involving millions of vertices are common in many practical applications and are challenging to process. Practical-time implementations using high-end computers are reported but are accessible only to a few. Graphics Processing Units (GPUs) of today have ... $\backslash$n},
author = {Harish, Pawan and Narayanan, Pj},
doi = {10.1007/978-3-540-77220-0{\_}21},
file = {:home/chiroptera/Dropbox/mendeley/Harish, Narayanan - 2007 - Accelerating large graph algorithms on the GPU using CUDA.pdf:pdf},
isbn = {9783540772194},
issn = {978-3-540-77219-4},
journal = {High performance computing–HiPC 2007},
number = {Chapter 21},
pages = {197--208},
title = {{Accelerating large graph algorithms on the GPU using CUDA}},
url = {http://www.springerlink.com/index/10.1007/978-3-540-77220-0{\_}21$\backslash$npapers2://publication/doi/10.1007/978-3-540-77220-0{\_}21$\backslash$nhttp://link.springer.com/chapter/10.1007/978-3-540-77220-0{\_}21},
volume = {4873},
year = {2007}
}
@inproceedings{DiBuccio2011,
abstract = {Dynamic Quantum Clustering is a recent clustering technique which makes use of Parzen window estimator to construct a potential function whose minima are related to the clusters to be found. The dynamic of the system is computed by means of the Schr{\"{o}}dinger differential equation. In this paper, we apply this technique in the context of Information Retrieval to explore its performance in terms of the quality of clusters and the efficiency of the computation. In particular, we want to analyze the clusters produced by using datasets of relevant and non-relevant documents given a topic. © 2011 Springer-Verlag.},
author = {{Di Buccio}, Emanuele and {Di Nunzio}, Giorgio Maria},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {360--363},
title = {{Distilling relevant documents by means of dynamic quantum clustering}},
volume = {6931 LNCS},
year = {2011}
}
@inproceedings{amdahl1967validity,
author = {Amdahl, Gene M},
booktitle = {Proceedings of the April 18-20, 1967, spring joint computer conference},
file = {:home/chiroptera/Dropbox/mendeley/Amdahl - 1967 - Validity of the single processor approach to achieving large scale computing capabilities.pdf:pdf},
organization = {ACM},
pages = {483--485},
title = {{Validity of the single processor approach to achieving large scale computing capabilities}},
year = {1967}
}
@article{Steinbach2004,
abstract = {Cluster analysis divides data into groups (clusters) for the purposes of summarization or improved understanding. For example, cluster analysis has been used to group related documents for browsing, to find genes and proteins that have similar functionality, or as a means of data compression. While clustering has a long history and a large number of clustering techniques have been developed in statistics, pattern recognition, data mining, and other fields, significant challenges still remain. In this chapter we provide a short introduction to cluster analysis, and then focus on the challenge of clustering high dimensional data. We present a brief overview of several recent techniques, including a more detailed description of recent work of our own which uses a concept-based clustering approach.},
author = {Steinbach, Michael and Ertoz, Levent and Kumar, Vipin},
doi = {10.1007/978-3-662-08968-2{\_}16},
file = {:home/chiroptera/Dropbox/mendeley/Steinbach, Ertoz, Kumar - 2004 - The Challenges of Clustering High Dimensional Data.pdf:pdf},
isbn = {978-3-642-07739-5},
journal = {New Directions in Statistical Physics},
keywords = {cluster analysis,curse of dimensionality,high dimensional data},
pages = {273--309},
title = {{The Challenges of Clustering High Dimensional Data}},
year = {2004}
}
@inproceedings{ana2003robust,
author = {Ana, L N F and Jain, Anil K},
booktitle = {Computer Vision and Pattern Recognition, 2003. Proceedings. 2003 IEEE Computer Society Conference on},
file = {:home/chiroptera/Dropbox/mendeley/Ana, Jain - 2003 - Robust data clustering.pdf:pdf},
organization = {IEEE},
pages = {II----128},
title = {{Robust data clustering}},
volume = {2},
year = {2003}
}
@inproceedings{zhang1996birch,
author = {Zhang, Tian and Ramakrishnan, Raghu and Livny, Miron},
booktitle = {ACM SIGMOD Record},
file = {:home/chiroptera/Dropbox/mendeley/Zhang, Ramakrishnan, Livny - 1996 - BIRCH an efficient data clustering method for very large databases.pdf:pdf},
number = {2},
organization = {ACM},
pages = {103--114},
title = {{BIRCH: an efficient data clustering method for very large databases}},
volume = {25},
year = {1996}
}
@article{Liu2010,
abstract = {By reviewing the original INIQGA algorithm, an improved algorithm (IINIQGA) is put forward by revising the lookup table. In addition, By introducing the variable angle-distance rotation method into the update Q(t) procedure, a novel quantum-inspired evolutionary algorithm, QEA-VAR, was proposed. Compared with previous algorithms, our update Q(t) procedure is more simple and feasible. Finally, the corresponding experiments on the 0-1 knapsack problem were carried out, and the results show that our improvement is efficient, and comparing with IINIQGA, QEA, and CGA, QEA-VAR has a faster convergence and better profits than other algorithms.},
author = {Liu, Wenjie and Chen, Hanwu and Yan, Qiaoqiao and Liu, Zhihao and Xu, Juan and Zheng, Yu},
doi = {10.1109/CEC.2010.5586281},
file = {:home/chiroptera/Dropbox/mendeley/Liu et al. - 2010 - A novel quantum-inspired evolutionary algorithm based on variable angle-distance rotation.pdf:pdf},
isbn = {9781424469109},
journal = {2010 IEEE World Congress on Computational Intelligence, WCCI 2010 - 2010 IEEE Congress on Evolutionary Computation, CEC 2010},
keywords = {0/1 knapsack problem,quantum-inspired evolutionary algorithm,qubit,variable angle-distance,variable angle-distance rotation},
mendeley-tags = {qubit,variable angle-distance},
title = {{A novel quantum-inspired evolutionary algorithm based on variable angle-distance rotation}},
year = {2010}
}
@book{Pedersen2008,
abstract = {Matrix identities, relations and approximations. A desktop reference for quick overview of mathematics of matrices.},
author = {Pedersen, Michael Syskind and Baxter, Bill and Templeton, Brian and Rish{\o}j, Christian and Theobald, Douglas L and Hoegh-rasmussen, Esben and Casteel, Glynne and Gao, Jun Bin and Dedecius, Kamil and Strim, Korbinian and Christiansen, Lars and Hansen, Lars Kai and Wilkinson, Leland and He, Liguo and Bar, Miguel and Winther, Ole and Sakov, Pavel and Hattinger, Stephan and Petersen, Kaare Breandt and Rish{\o} j, Christian},
booktitle = {Matrix},
doi = {10.1111/j.1365-294X.2006.03161.x},
editor = {Bloom, B R},
institution = {Technical University of Denmark},
issn = {09621083},
keywords = {acknowledgements,bill baxter,brian templeton,christian,christian rish{\o}j,contributions,derivative,derivative inverse matrix,determinant,differentiate a matrix,matrix algebra,matrix identities,matrix relations,suggestions,thank following,we would like},
number = {1},
pages = {1--71},
pmid = {17284204},
publisher = {Citeseer},
title = {{The Matrix Cookbook}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.139.3165{\&}amp;rep=rep1{\&}amp;type=pdf},
volume = {M},
year = {2008}
}
@book{Aggarwal2014,
author = {Aggarwal, Charu C and Reddy, Chandan K},
file = {:home/chiroptera/Dropbox/mendeley/Aggarwal, Reddy - 2013 - Data clustering algorithms and applications.pdf:pdf},
isbn = {9781466558229},
publisher = {CRC Press},
title = {{Data clustering: algorithms and applications}},
year = {2013}
}
@misc{Weinstein,
author = {Weinstein, Marvin},
title = {{DQClib: A Maple package implementing Dynamic Quantum Clustering (DQC)}},
url = {http://www.slac.stanford.edu/{~}niv/index{\_}files/DQCOverview1.html}
}
@article{DiMarco2013a,
abstract = {Discover and quantify the performance gains of dynamic parallelism for clustering algorithms on GPUs. Dynamic parallelism effectively eliminates the superfluous back and forth communication between the GPU and CPU through nested kernel computations. The change in performance is measured using two well-known clustering algorithms that exhibit data dependencies: the K-means clustering and the hierarchical clustering. K-means has a sequential data dependence wherein iterations occur in a linear fashion, while the hierarchical clustering has a tree-like dependence that produces split tasks. Analyzing the performance of these data-dependent algorithms gives us a better understanding of the benefits or potential drawbacks of CUDA 5's new dynamic parallelism feature.},
author = {DiMarco, Jeffrey and Taufer, Michela},
doi = {10.1117/12.2018069},
file = {:home/chiroptera/Dropbox/mendeley/DiMarco, Taufer - 2013 - Performance impact of dynamic parallelism on different clustering algorithms.pdf:pdf;:home/chiroptera/Dropbox/mendeley/DiMarco, Taufer - 2013 - Performance impact of dynamic parallelism on different clustering algorithms(2).pdf:pdf},
isbn = {9780819495433},
issn = {0277786X},
journal = {Spie},
keywords = {0,cuda 5,divisive hierarchical clustering,k-means},
pages = {87520E},
title = {{Performance impact of dynamic parallelism on different clustering algorithms}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2018069},
year = {2013}
}
@incollection{fred2002evidence,
author = {Fred, Ana and Jain, Anil K},
booktitle = {Structural, syntactic, and statistical pattern recognition},
file = {:home/chiroptera/Dropbox/mendeley/Fred, Jain - 2002 - Evidence accumulation clustering based on the k-means algorithm.pdf:pdf},
pages = {442--451},
publisher = {Springer},
title = {{Evidence accumulation clustering based on the k-means algorithm}},
year = {2002}
}
@article{Evans,
author = {Evans, Michael R},
file = {:home/chiroptera/Dropbox/mendeley/Evans - Unknown - Spatial Big Data Case Studies on Volume , Velocity , and Variety What is Spatial Big Data.pdf:pdf},
pages = {1--16},
title = {{Spatial Big Data : Case Studies on Volume , Velocity , and Variety What is Spatial Big Data ?}}
}
@article{Horn2007,
abstract = {Over the past few years, the powerful computation rates and high memory bandwidth of GPUs have attracted efforts to run raytracing on GPUs. Our work extends Foley et al.s GPU k-d tree research. We port their kd-restart algorithm from multi-pass, using CPU load balancing, to single pass, using current GPUs branching and looping abilities. We introduce three optimizations: a packetized formulation, a technique for restarting partially down the tree instead of at the root, and a small, fixed-size stack that is checked before resorting to restart. Our optimized implementation achieves 15 - 18 million primary rays per second and 16 - 27 million shadow rays per second on our test scenes. Our system also takes advantage of GPUs strengths at rasterization and shading to offer a mode where rasterization replaces eye ray scene intersection, and primary hits and lo- cal shading are produced with standard Direct3D code. For 1024x1024 renderings of our scenes with shadows and Phong shading, we achieve 12-18 frames per second. Finally, we in- vestigate the efficiency of our implementation relative to the computational resources of our GPUs and also compare it against conventional CPUs and the Cell processor, which both have been shown to raytrace well.},
author = {Horn, Daniel Reiter and Sugerman, Jeremy and Houston, Mike and Hanrahan, Pat},
doi = {10.1145/1230100.1230129},
file = {:home/chiroptera/Dropbox/mendeley/Horn et al. - 2007 - Interactive k-d tree GPU raytracing.pdf:pdf},
isbn = {9781595936288},
journal = {Proceedings of the 2007 symposium on Interactive 3D graphics and games I3D 07},
keywords = {data,gpu computing,parallel computing,programmable graphics hardware,stream computing},
pages = {167},
title = {{Interactive k-d tree GPU raytracing}},
url = {http://portal.acm.org/citation.cfm?doid=1230100.1230129},
year = {2007}
}
@book{Moler2008,
abstract = {This chapter is about eigenvalues and singular values of matrices. Computational algorithms and sensitivity to perturbations are both discussed.},
author = {Moler, Cleve},
booktitle = {Numerical Computing with MATLAB, Revised Reprint},
doi = {10.1016/0377-0427(90)90025-U},
file = {:home/chiroptera/Dropbox/mendeley/Moler - 2008 - Eigenvalues and Singular Values.pdf:pdf},
isbn = {978-0-898716-60-3},
issn = {1550-2376},
pages = {39},
pmid = {21230651},
title = {{Eigenvalues and Singular Values}},
url = {http://www.mathworks.nl/moler/chapters.html},
year = {2008}
}
@article{Owens2008,
abstract = {The graphics processing unit (GPU) has become an integral part of today's mainstream computing systems. Over the past six years, there has been a marked increase in the performance and capabilities of GPUs. The modern GPU is not only a powerful graphics engine but also a highly parallel programmable processor featuring peak arithmetic and memory bandwidth that substantially outpaces its CPU counterpart. The GPU's rapid increase in both programmability and capability has spawned a research community that has successfully mapped a broad range of computationally demanding, complex problems to the GPU. This effort in general-purpose computing on the GPU, also known as GPU computing, has positioned the GPU as a compelling alternative to traditional microprocessors in high-performance computer systems of the future. We describe the background, hardware, and programming model for GPU computing, summarize the state of the art in tools and techniques, and present four GPU computing successes in game physics and computational biophysics that deliver order-of-magnitude performance gains over optimized CPU applications.},
annote = {Excellent survey (albeit old) of GPU, GPU architecture, GPU programming flow, GPGPU, examples and trends.},
author = {Owens, Jd and Houston, M},
doi = {10.1109/JPROC.2008.917757},
file = {:home/chiroptera/Dropbox/mendeley/Owens, Houston - 2008 - GPU computing.pdf:pdf},
isbn = {0769527000},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {gpgpu,gpu,gpu-overview,survey,topnotch},
mendeley-tags = {gpgpu,gpu,survey,topnotch},
number = {5},
pages = {879 -- 899},
pmid = {21776805},
title = {{GPU computing}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4490127},
volume = {96},
year = {2008}
}
@article{wang2011bayesian,
author = {Wang, Hongjun and Shan, Hanhuai and Banerjee, Arindam},
file = {:home/chiroptera/Dropbox/mendeley/Steinhaeuser, Chawla, Ganguly - 2010 - Complex Networks as a Unified Framework for Descriptive Analysis and Predictive Modeling in Clima.pdf:pdf},
journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
number = {1},
pages = {54--70},
publisher = {Wiley Online Library},
title = {{Bayesian cluster ensembles}},
volume = {4},
year = {2011}
}
@article{Rodriguez2011,
abstract = {Ultra high density oligonucleotide micro arrays allow several millions of genetic markers in a single experiment to be observed. Current bioinformatics software for gene expression quantile data normalization is unable to process such huge datasets. In parallel with this perception, the huge volume of molecular data produced by current high-throughput technologies in modern molecular biology has increased at a similar pace the challenge in our capacity to process and understand data. On the other hand, the arrival of CUDA has unveiled the extraordinary power of Graphics Processors (GPUs) to accelerate data intensive general purpose computing more and more as times goes by. This work takes these two emerging trends to benefit side by side during the development of a high performance version for a biomedical application of growing popularity: gene expression normalization. A variety of experimental issues are analyzed for this execution, including cost, performance and scalability of the graphics architecture on three different platforms. Our study reveals advantages and drawbacks of using the GPU as target hardware, providing lessons to benefit a broad set of existing genetic applications, either based on those pillars or having similarities with their procedures.},
author = {Rodriguez, Andres and Trelles, Oswaldo and Ujaldon, Manuel},
doi = {10.1109/HPCC.2011.85},
file = {:home/chiroptera/Dropbox/mendeley/Rodriguez, Trelles, Ujaldon - 2011 - Using Graphics Processors for a High Performance Normalization of Gene Expressions.pdf:pdf},
isbn = {978-0-7695-4538-7},
journal = {2011 IEEE International Conference on High Performance Computing and Communications},
keywords = {Bioinformatics,CUDA,Graphics Processing Units (GPUs),High Performance Computing,Normalization of Gene Expressions},
pages = {599--604},
title = {{Using Graphics Processors for a High Performance Normalization of Gene Expressions}},
year = {2011}
}
@article{CalatravaMoreno2013,
abstract = {gpu-overview},
author = {{Calatrava Moreno}, Maria Del Carmen and Auzinger, Thomas},
doi = {10.1109/SOCA.2013.15},
file = {:home/chiroptera/Dropbox/mendeley/Calatrava Moreno, Auzinger - 2013 - General-Purpose Graphics Processing Units in Service-Oriented Architectures.pdf:pdf},
isbn = {978-1-4799-2702-9},
journal = {2013 IEEE 6th International Conference on Service-Oriented Computing and Applications},
pages = {260--267},
title = {{General-Purpose Graphics Processing Units in Service-Oriented Architectures}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6717316},
year = {2013}
}
@article{Ho2002b,
abstract = {Using a number of measures for characterising the complexity of classification problems, we studied the comparative advantages of two methods for constructing decision forests - bootstrapping and random subspaces. We investigated a collection of 392 two-class problems from the UCI depository, and observed that there are strong correlations between the classifier accuracies and measures of length of class boundaries, thickness of the class manifolds, and nonlinearities of decision boundaries. We found characteristics of both difficult and easy cases where combination methods are no better than single classifiers. Also, we observed that the bootstrapping method is better when the training samples are sparse, and the subspace method is better when the classes are compact and the boundaries are smooth. © 2002 Springer-Verlag London Limited.},
author = {Ho, Tin Kam},
doi = {10.1007/s100440200009},
file = {:home/chiroptera/Dropbox/mendeley/Ho - 2002 - A data complexity analysis of comparative advantages of decision forest constructors.pdf:pdf},
issn = {14337541},
journal = {Pattern Analysis and Applications},
keywords = {Bagging,Classifier combination,Data complexity,Decision forest,Decision tree,Random subspace method},
number = {2},
pages = {102--112},
title = {{A data complexity analysis of comparative advantages of decision forest constructors}},
volume = {5},
year = {2002}
}
@article{Shell2008,
abstract = {Despite growing evidence that increased brain-derived neurotrophic factor (BDNF) and hippocampal adult neurogenesis are necessary for the behavioral actions of antidepressants in rodents, the cellular mechanisms involved in these effects are still unknown. Li et al. in this issue of Neuron demonstrate that the presence of TrkB, the high-affinity receptor for BDNF, in hippocampal neural progenitor cells is required for the neurogenic and behavioral actions of antidepressant treatments.},
author = {Shell, M.},
doi = {10.1016/j.neuron.2008.07.028},
issn = {1097-4199},
journal = {Neuron},
keywords = {Adult Stem Cells,Adult Stem Cells: drug effects,Animals,Antidepressive Agents,Antidepressive Agents: pharmacology,Cell Proliferation,Cell Proliferation: drug effects,Mice,Neurons,Neurons: drug effects,Receptor,trkB,trkB: metabolism},
number = {3},
pages = {349--51},
pmid = {18701059},
title = {{How to Use the IEEEtran L TEX Class}},
url = {http://www.cs.northwestern.edu/{~}sle841/papers/Arch{\_}TVCG/misc/tex files/original{\_}tvcg{\_}files/IEEEtran{\_}HOWTO.pdf$\backslash$nhttp://www.ncbi.nlm.nih.gov/pubmed/18701059},
volume = {59},
year = {2008}
}
@article{Dianxun2006,
abstract = {A novel generalized quantum particle model (GQPM) is presented for data self-organizing clustering. Using GQPM we transform the data clustering into a stochastic process of equivalence classes of particles under the quantum entanglement relation. The GQPM approach has much faster clustering speed and higher clustering quality than the nonquantum particle model GPM and GCA we proposed before. GQPM is also characterized by the self-organizing clustering and has advantages in terms of the insensitivity to noise, the quality robustness to clustered data, the learning ability, the suitability for high-dimensional multi-shape large-scale data sets. The simulations and comparisons have shown the effectiveness and good performance of the proposed GQPM approach to data clustering},
author = {Dianxun, Shuai and Zhang, Ping and Huang, Liangjun},
doi = {10.1109/ISIE.2006.296087},
file = {:home/chiroptera/Dropbox/mendeley/Dianxun, Zhang, Huang - 2006 - Self-organizing data clustering A novel quantum particle approach.pdf:pdf},
isbn = {1424404975},
journal = {IEEE International Symposium on Industrial Electronics},
number = {2},
pages = {2960--2965},
title = {{Self-organizing data clustering: A novel quantum particle approach}},
volume = {4},
year = {2006}
}
@article{prim1957shortest,
author = {Prim, Robert Clay},
journal = {Bell system technical journal},
number = {6},
pages = {1389--1401},
publisher = {Wiley Online Library},
title = {{Shortest connection networks and some generalizations}},
volume = {36},
year = {1957}
}
@inproceedings{1331155,
abstract = {This work proposes a new face detection system using quantum-inspired evolutionary algorithm (QEA). The proposed detection system is based on elliptical blobs and principal component analysis (PCA). The elliptical blobs in the directional image are used to find the face candidate regions, and then PCA and QEA are employed to verify faces. Although PCA related algorithms have shown outstanding performance, there still exist some problems such as optimal decision boundary or learning capabilities. By PCA, we can obtain the optimal basis but they may not be the optimal ones for discriminating faces from non-faces. Moreover, a threshold value should be selected properly considering the success rate and false alarm rate. To solve these problems, QEA is employed to find out the optimal decision boundary under the predetermined threshold value which distinguishes between face images and non-face images. The proposed system provides learning capability by reconstructing the training database, which means that system performance can be improved as failure trials occur.},
author = {Jang, Jun-Su and Han, Kuk-Hyun and Kim, Jong-Hwan},
booktitle = {Evolutionary Computation, 2004. CEC2004. Congress on},
doi = {10.1109/CEC.2004.1331155},
file = {:home/chiroptera/Dropbox/mendeley/Jang, Han, Kim - 2004 - Face detection using quantum-inspired evolutionary algorithm.pdf:pdf},
keywords = {Evolutionary computation,Face detection,Image databases,Image reconstruction,Neural networks,Paper technology,Principal component analysis,Research and development,System performance,Visual databases,directional image,elliptical blobs,face detection,face images,face recognition,genetic algorithms,nonface images,optimal decision boundary,principal component analysis,quantum computing,quantum-inspired evolutionary algorithm,training database},
pages = {2100--2106 Vol.2},
title = {{Face detection using quantum-inspired evolutionary algorithm}},
volume = {2},
year = {2004}
}
@article{Aimeur2013,
abstract = {We show how the quantum paradigm can be used to speed up unsupervised learning algorithms. More precisely, we explain how it is possible to accelerate learning algorithms by quantizing some of their subroutines. Quantization refers to the process that partially or totally converts a classical algorithm to its quantum counterpart in order to improve performance. In particular, we give quantized versions of clustering via minimum spanning tree, divisive clustering and k-medians that are faster than their classical analogues. We also describe a distributed version of k-medians that allows the participants to save on the global communication cost of the protocol compared to the classical version. Finally, we design quantum algorithms for the construction of a neighbourhood graph, outlier detection as well as smart initialization of the cluster centres.},
author = {A{\"{\i}}meur, Esma and Brassard, Gilles and Gambs, S{\'{e}}bastien},
doi = {10.1007/s10994-012-5316-5},
file = {:home/chiroptera/Dropbox/mendeley/A{\"{\i}}meur, Brassard, Gambs - 2013 - Quantum speed-up for unsupervised learning.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {Clustering,Grover's algorithm,Quantum information processing,Quantum learning,Unsupervised learning},
number = {February 2012},
pages = {261--287},
title = {{Quantum speed-up for unsupervised learning}},
volume = {90},
year = {2013}
}
@article{lange2004stability,
author = {Lange, Tilman and Roth, Volker and Braun, Mikio L and Buhmann, Joachim M},
file = {:home/chiroptera/Dropbox/mendeley/Lange et al. - 2004 - Stability-based validation of clustering solutions.pdf:pdf},
journal = {Neural computation},
number = {6},
pages = {1299--1323},
publisher = {MIT Press},
title = {{Stability-based validation of clustering solutions}},
volume = {16},
year = {2004}
}
@article{Halkidi2001,
abstract = {Cluster analysis aims at identifying groups of similar objects and, therefore helps to discover distribution of patterns and interesting correlations in large data sets. It has been subject of wide research since it arises in many application domains in engineering, business and social sciences. Especially, in the last years the availability of huge transactional and experimental data sets and the arising requirements for data mining created needs for clustering algorithms that scale and can be applied in diverse domains.},
author = {Halkidi, Maria and Batistakis, Yannis and Vazirgiannis, Michalis},
doi = {10.1023/A:1012801612483},
file = {:home/chiroptera/Dropbox/mendeley/Halkidi, Batistakis, Vazirgiannis - 2001 - On clustering validation techniques.pdf:pdf},
isbn = {0925-9902},
issn = {09259902},
journal = {Journal of Intelligent Information Systems},
keywords = {Cluster validity,Clustering algorithms,Unsupervised learning,Validity indices},
number = {2-3},
pages = {107--145},
title = {{On clustering validation techniques}},
volume = {17},
year = {2001}
}
@article{Fred2006a,
abstract = {Each clustering algorithm induces a similarity between given data points, according to the underlying clustering criteria. Given the large number of available clustering techniques, one is faced with the following questions: (a) Which measure of similarity should be used in a given clustering problem? (b) Should the same similarity measure be used throughout the d-dimensional feature space? In other words, are the underlying clusters in given data of similar shape? Our goal is to learn the pairwise similarity between points in order to facilitate a proper partitioning of the data without the a priori knowledge of k, the number of clusters, and of the shape of these clusters. We explore a clustering ensemble approach combined with cluster stability criteria to selectively learn the similarity from a collection of different clustering algorithms with various parameter configurations},
author = {Fred, Ana L N and Jain, Anil K.},
doi = {10.1109/ICPR.2006.754},
file = {:home/chiroptera/Dropbox/mendeley/Fred, Jain - 2006 - Learning pairwise similarity for data clustering(2).pdf:pdf},
isbn = {0769525210},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
pages = {925--928},
title = {{Learning pairwise similarity for data clustering}},
volume = {1},
year = {2006}
}
@article{Hore2009,
author = {Hore, Prodib and Hall, Lawrence O. and Goldgof, Dmitry B.},
doi = {10.1016/j.patcog.2008.09.027.A},
file = {:home/chiroptera/Dropbox/mendeley/Hore, Hall, Goldgof - 2009 - A Scalable Framework For Cluster Ensembles.pdf:pdf},
journal = {Science},
keywords = {clustering,ensemble,fuzzy-k-means,hard,large data sets,scalability,single pass algorithm},
number = {1},
pages = {676--688},
title = {{A Scalable Framework For Cluster Ensembles}},
volume = {42},
year = {2009}
}
@article{Joshi2003,
abstract = {Clustering large data sets can be time consuming and processor intensive. This project is an implementation of the parallel version of a popular clustering algorithm, the k-means algorithm, to provide faster clustering solutions. This algorithm was tested such that 3,4,5,7 clusters were created on a cluster of Sun workstations. Optimal levels of speedup were not achieved; but the benefits of parallelization were observed. This methodology exploits the inherent data- parallelism in the k-means algorithm and makes use of the message-passing model.},
annote = {Results only on very small datasets, only accuracy results and no timings. Speedup purely theoretical.},
author = {Joshi, Manasi N},
file = {:home/chiroptera/Dropbox/mendeley/Joshi - 2003 - Parallel K - Means Algorithm on Distributed Memory Multiprocessors.pdf:pdf},
journal = {Cities},
pages = {12},
title = {{Parallel K - Means Algorithm on Distributed Memory Multiprocessors}},
year = {2003}
}
@inproceedings{Casper2012KMeans,
abstract = {A Quantum-Modeled K-Means clustering algorithm for multi- band image segmentation is explored and evaluated. Data sets of interest include multi-band RGB imagery, which subsequent to classification is analyzed and assessed for accuracy. Results demonstrate that under specific conditions the algorithm exhibits improved accuracy, when compared to its classical counterpart. Categories},
author = {Casper, Ellis and Hung, Chih-Cheng and Jung, Edward and Yang, Ming},
booktitle = {Proceedings of the 2012 ACM Research in Applied Computation Symposium},
file = {:home/chiroptera/Dropbox/mendeley//Casper et al. - 2012 - A Quantum-Modeled K-Means Clustering Algorithm for Multi-band Image Segmentation.pdf:pdf},
keywords = {K-Means,QK-Means,Quantum computing,image,image segmentation,k-means,qk-means,quantum computing,qubit},
mendeley-tags = {image segmentation,k-means,qk-means,qubit},
number = {3},
pages = {158--163},
title = {{A Quantum-Modeled K-Means Clustering Algorithm for Multi-band Image Segmentation}},
url = {http://delivery.acm.org/10.1145/2410000/2401639/p158-casper.pdf?ip=193.136.132.10{\&}id=2401639{\&}acc=ACTIVE SERVICE{\&}key=2E5699D25B4FE09E.F7A57B2C5B227641.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}CFID=476955365{\&}CFTOKEN=55494231{\&}{\_}{\_}acm{\_}{\_}=1423057410{\_}0d77d9b5028cb3},
volume = {1},
year = {2012}
}
@book{Lanzagorta2008,
author = {Lanzagorta, Marco and Uhlmann, Jeffrey},
booktitle = {Synthesis Lectures on Quantum Computing},
doi = {10.2200/S00159ED1V01Y200810QMC002},
file = {:home/chiroptera/Dropbox/mendeley/Lanzagorta, Uhlmann - 2008 - Quantum Computer Science.pdf:pdf},
isbn = {9780511813870},
issn = {1945-9726},
pages = {1--124},
title = {{Quantum Computer Science}},
volume = {1},
year = {2008}
}
@inproceedings{gao20xx,
abstract = {Document clustering is one of the most important tasks in text mining. In clustering algorithms, high-dimensional vector is usually used to represent a document which causes that the algorithms are often computationally expensive. On the other hand, Graphic Processing Unit (GPU) is increasingly important in parallel computing due to its powerful parallel capacity and high bandwidth. This paper implements a GPU-based Harmony K-means Algorithm (HKA) with NVIDIA's Compute Unified Device Architecture (CUDA), and uses it for document clustering. In our experiment, our GPU-based program can acquire a maximum 20 times speedup in contrast with CPU-based program.},
author = {{Yanjun Jiang} and {Enxing Li} and {Zhanchun Gao}},
booktitle = {Information Science and Control Engineering 2012 (ICISCE 2012), IET International Conference on},
doi = {10.1049/cp.2012.2426},
file = {:home/chiroptera/Dropbox/mendeley/Yanjun Jiang, Enxing Li, Zhanchun Gao - 2012 - A GPU-based harmony k-means algorithm for document clustering.pdf:pdf},
isbn = {978-1-84919-641-3},
keywords = {cluster number is large,document clustering,faster,gpu,harmony search,implementation is 20 times,k-means,parallel computing,results show that cuda,than cpu implement when},
pages = {1--4},
title = {{A GPU-based harmony k-means algorithm for document clustering}},
url = {http://digital-library.theiet.org/content/conferences/10.1049/cp.2012.2426},
year = {2012}
}
@article{li2008novel,
abstract = {The enormous successes have been made by quantum algorithms during the last decade. In this paper, we combine the quantum random walk (QRW) with the problem of data clustering, and develop two clustering algorithms based on the one dimensional QRW. Then, the probability distributions on the positions induced by QRW in these algorithms are investigated, which also indicates the possibility of obtaining better results. Consequently, the experimental results have demonstrated that data points in datasets are clustered reasonably and efficiently, and the clustering algorithms are of fast rates of convergence. Moreover, the comparison with other algorithms also provides an indication of the effectiveness of the proposed approach.},
archivePrefix = {arXiv},
arxivId = {0812.1357},
author = {Li, Qiang and He, Yan and Jiang, Jing-ping},
eprint = {0812.1357},
file = {:home/chiroptera/Dropbox/mendeley/Li, He, Jiang - 2008 - A Novel Clustering Algorithm Based on Quantum Random Walk.pdf:pdf},
journal = {arXiv preprint arXiv:0812.1357},
keywords = {data clustering,quantum compu-,quantum game,tation,unsupervised learning},
pages = {14},
title = {{A Novel Clustering Algorithm Based on Quantum Random Walk}},
url = {http://arxiv.org/abs/0812.1357},
year = {2008}
}
@misc{Olson1995,
abstract = {Hierarchical clustering is a common method used to determine clusters of similar data points in multidimensional spaces. O(n2) algorithms are known for this problem [3,4,11,19]. This paper reviews important results for sequential algorithms and describes previous work on parallel algorithms for hierarchical clustering. Parallel algorithms to perform hierarchical clustering using several distance metrics are then described. Optimal PRAM algorithms using n/log n processors are given for the average link, complete link, centroid, median, and minimum variance metrics. Optimal butterfly and tree algorithms using n/log n processors are given for the centroid, median, and minimum variance metrics. Optimal asymptotic speedups are achieved for the best practical algorithm to perform clustering using the single link metric on a n/log n processor PRAM, butterfly, or tree.},
author = {Olson, Clark F},
booktitle = {Parallel Computing},
file = {:home/chiroptera/Dropbox/mendeley/Olson - 1995 - parellalAlgorithms for Hierarchical clustering.pdf.pdf:pdf},
pages = {1313--1325},
title = {{parellalAlgorithms for Hierarchical clustering.pdf}},
volume = {21},
year = {1995}
}
@article{Geras2011,
author = {Geras, Krzysztof Jerzy},
file = {:home/chiroptera/Dropbox/mendeley/Geras - 2011 - Prediction Markets for Machine Learning.pdf:pdf},
number = {248264},
title = {{Prediction Markets for Machine Learning}},
year = {2011}
}
@article{Zentall2014,
abstract = {Learning by rats was facilitated when response-relevant cues were provided by other rats; learning increased as a fzlnction of number of cues provided. These results suggest that rats can learn by imitation. Learning by rats that observed conspeclifics not esnitting response-relevant cues was retarded compared to learning by rats that did not observe conspecifics. This indicates that a conspecific's presence can also inhibit learning, a result consistent with socia facilitation theory.},
author = {Zentall, Thomas R. and Levine, John M.},
file = {:home/chiroptera/Dropbox/mendeley/Zentall, Levine - 2014 - American Association for the Advancement of Science.pdf:pdf},
journal = {Science},
number = {5278},
pages = {1220--1221},
title = {{American Association for the Advancement of Science}},
volume = {178},
year = {2014}
}
@misc{pytables,
author = {Alted, Francesc and Vilata, Ivan and Others},
title = {{PyTables: Hierarchical Datasets in Python}},
url = {http://www.pytables.org/}
}
@article{Jamsek2009a,
abstract = {The complexity of modern microprocessor design involving billions of transistors at increasingly denser scales creates many challenges particularly in the area of design reliability and predictable yields. Researchers at IBM's Austin Research Lab have increasingly depended on software based simulation of various aspects of the design and manufacturing process to help address these challenges. The computational complexity and sheer scale of these simulations have lead to the exploration of the application of high-performance hybrid computing clusters to accelerate the design process. Currently, the hybrid clusters in use are composed primarily of commodity workstations and servers incorporating commodity NVIDIA-based GPU graphics cards and TESLA GPU computational accelerators. We have also been experimenting with blade clusters composed of both general purpose servers and PowerXcell accelerators leveraging the computational throughput of the Cell processor. In this paper we will detail our experiences with accelerating our workloads on these hybrid cluster platforms. We will discuss our initial approach of combining hybrid runtimes such as CUDA with MPI to address cluster computation. We will also describe a custom cluster hybrid infrastructure we are developing to deal with some of the perceived shortcomings of MPI and other traditional cluster tools when dealing with hybrid computing environments.},
author = {Jamsek, Damir and {Van Hensbergen}, Eric},
doi = {10.1109/CLUSTR.2009.5289126},
file = {:home/chiroptera/Dropbox/mendeley/Jamsek, Van Hensbergen - 2009 - Experiences with hybrid clusters.pdf:pdf},
isbn = {9781424450121},
issn = {15525244},
journal = {Proceedings - IEEE International Conference on Cluster Computing, ICCC},
keywords = {cluster,gpu,mpi},
mendeley-tags = {cluster,gpu,mpi},
pages = {1--4},
title = {{Experiences with hybrid clusters}},
year = {2009}
}
@article{Bacao2005,
abstract = {One of the most widely used clustering techniques used in GISc problems is the k-means algorithm. One of the most important issues in the correct use of k-means is the initialization procedure that ultimately determines which part of the solution space will be searched. In this paper we briefly review different initialization procedures, and propose Kohonens Self- Organizing Maps as the most convenient method, given the proper training parameters. Furthermore, we show that in the final stages of its training procedure the Self-Organizing Map algorithms is rigorously the same as the kmeans algorithm. Thus we propose the use of Self-Organizing Maps as possible substitutes for the more classical k-means clustering algorithms.},
author = {Ba{\c{c}}{\~{a}}o, Fernando and Lobo, Victor and Painho, Marco},
doi = {10.1007/11428862{\_}65},
file = {:home/chiroptera/Dropbox/mendeley/Ba{\c{c}}{\~{a}}o, Lobo, Painho - 2005 - Self-organizing maps as substitutes for k-means clustering.pdf:pdf},
isbn = {0302-9743},
issn = {03029743},
journal = {Computational Science–ICCS 2005},
pages = {476 -- 483},
title = {{Self-organizing maps as substitutes for k-means clustering}},
url = {http://www.springerlink.com/index/rcvxbt4av8bp5byl.pdf},
volume = {3516},
year = {2005}
}
@article{Nvidia2009,
abstract = {The lethal outcome of high-dose pulmonary virus infection is thought to reflect high-level, sustained virus replication and associated lung inflammation prior to development of an adaptive immune response. Herein, we demonstrate that the outcome of lethal/sublethal influenza infection instead correlates with the initial virus replication tempo. Furthermore, the magnitude of early lung antiviral CD8+ T cell responses varies inversely with inoculum dose and is controlled by lymph-node-resident dendritic cells (LNDC) through IL-12p40-regulated FasL-dependent T cell apoptosis. These results suggest that the inoculum dose and replication rate of a pathogen entering the respiratory tract may regulate the strength of the adaptive immune response, and the subsequent outcome of infection and that LNDC may serve as regulators (gatekeepers) in the development of CD8+ T cell responses.},
author = {Nvidia, Whitepaper and Generation, Next and Compute, Cuda},
doi = {10.1016/j.immuni.2005.11.006},
file = {:home/chiroptera/Dropbox/mendeley/Nvidia, Generation, Compute - 2009 - Whitepaper NVIDIA’s Next Generation CUDA Compute Architecture.pdf:pdf},
issn = {10747613},
journal = {ReVision},
number = {6},
pages = {1--22},
pmid = {16356862},
title = {{Whitepaper NVIDIA’s Next Generation CUDA Compute Architecture}},
url = {http://www.nvidia.com/content/PDF/fermi{\_}white{\_}papers/NVIDIA{\_}Fermi{\_}Compute{\_}Architecture{\_}Whitepaper.pdf},
volume = {23},
year = {2009}
}
@article{Fred2001,
abstract = {Given an arbitrary data set, to which no particular paramet- rical, statistical or geometrical structure can be assumed, different clus- tering algorithms will in general produce different data partitions. In fact, several partitions can also be obtained by using a single clustering algo- rithm due to dependencies on initialization or the selection of the value of some design parameter. This paper addresses the problem of finding consistent clusters in data partitions, proposing the analysis of the most common associations performed in a majority voting scheme. Combina- tion of clustering results are performed by transforming data partitions into a co-association sample matrix, which maps coherent associations. This matrix is then used to extract the underlying consistent clusters. The proposed methodology is evaluated in the context of k-means clus- tering, a new clustering algorithm - voting-k-means, being presented. Examples, using both simulated and real data, show how this major- ity voting combination scheme simultaneously handles the problems of selecting the number of clusters, and dependency on initialization. Fur- thermore, resulting clusters are not constrained to be hyper-spherically shaped.},
annote = {Consistency Index.},
author = {Fred, Ana},
file = {:home/chiroptera/Dropbox/mendeley/Fred - 2001 - Finding consistent clusters in data partitions.pdf:pdf},
isbn = {3540422846},
journal = {Multiple classifier systems},
pages = {309--318},
title = {{Finding consistent clusters in data partitions}},
url = {http://link.springer.com/chapter/10.1007/3-540-48219-9{\_}31},
year = {2001}
}
@article{Blekas2007a,
abstract = {Given a data set, a dynamical procedure is applied to the data points in order to shrink and separate, possibly overlapping clusters. Namely, Newton's equations of motion are employed to concentrate the data points around their cluster centers, using an attractive potential, constructed specially for this purpose. During this process, important information is gathered concerning the spread of each cluster. In succession this information is used to create an objective function that maps each cluster to a local maximum. Global optimization is then used to retrieve the positions of the maxima that correspond to the locations of the cluster centers. Further refinement is achieved by applying the EM-algorithm to a Gaussian mixture model whose construction and initialization is based on the acquired information. To assess the effectiveness of our method, we have conducted experiments on a plethora of benchmark data sets. In addition we have compared its performance against four clustering techniques that are well established in the literature. [All rights reserved Elsevier]},
author = {Blekas, K. and Lagaris, I.E.},
doi = {10.1016/j.patcog.2006.07.012},
file = {:home/chiroptera/Dropbox/mendeley/Blekas, Lagaris - 2007 - Newtonian clustering An approach based on molecular dynamics and global optimization(2).pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {clustering,global optimization,molecular dynamics,order statistics},
pages = {1734--1744},
title = {{Newtonian clustering: An approach based on molecular dynamics and global optimization}},
volume = {40},
year = {2007}
}
@article{Murtagh2012,
abstract = {We survey agglomerative hierarchical clustering algorithms and discuss efficient implementations that are available in R and other software environments. We look at hierarchical self-organizing maps, and mixture models. We review grid-based clustering, focusing on hierarchical density-based approaches. Finally, we describe a recently developed very efficient (linear time) hierarchical clustering algorithm, which can also be viewed as a hierarchical grid-based algorithm. © 2011 Wiley Periodicals, Inc.},
archivePrefix = {arXiv},
arxivId = {1105.0121},
author = {Murtagh, Fionn and Contreras, Pedro},
doi = {10.1002/widm.53},
eprint = {1105.0121},
file = {:home/chiroptera/Dropbox/mendeley/Murtagh, Contreras - 2012 - Algorithms for hierarchical clustering An overview.pdf:pdf},
isbn = {19424787},
issn = {19424787},
journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
number = {1},
pages = {86--97},
title = {{Algorithms for hierarchical clustering: An overview}},
volume = {2},
year = {2012}
}
@article{Marinelli2009,
author = {Marinelli, E. E.},
file = {:home/chiroptera/Dropbox/mendeley/Marinelli - 2009 - Hyrax Cloud computing on mobile devices using MapReduce.pdf:pdf},
number = {September},
title = {{Hyrax: Cloud computing on mobile devices using MapReduce}},
volume = {0389},
year = {2009}
}
@article{Gower1969,
author = {Gower, J. C. and Ross, G. J. S.},
file = {:home/chiroptera/Dropbox/mendeley/Gower, Ross - 1969 - Minimum Spanning Trees and Single Linkage Cluster Analysis.pdf:pdf},
journal = {Journal of the Royal Statistical Society},
number = {1},
pages = {54--64},
title = {{Minimum Spanning Trees and Single Linkage Cluster Analysis}},
volume = {18},
year = {1969}
}
@article{Chang2009,
abstract = {Graphics processing units (GPUs) are powerful computational devices tailored towards the needs of the 3-D gaming industry for high-performance, real-time graphics engines. Nvidia Corporation released a new generation of GPUs designed for general-purpose computing in 2006, and it released a GPU programming language called CUDA in 2007. The DNA microarray technology is a high throughput tool for assaying mRNA abundance in cell samples. In data analysis, scientists often apply hierarchical clustering of the genes, where a fundamental operation is to calculate all pairwise distances. If there are n genes, it takes O(n{\^{}}2) time. In this work, GPUs and the CUDA language are used to calculate pairwise distances. For Manhattan distance, GPU/CUDA achieves a 40 to 90 times speed-up compared to the central processing unit implementation; for Pearson correlation coefficient, the speed-up is 28 to 38 times.},
author = {Chang, Dar J. and Desoky, Ahmed H. and Ouyang, Ming and Rouchka, Eric C.},
doi = {10.1109/SNPD.2009.34},
file = {:home/chiroptera/Dropbox/mendeley/Chang et al. - 2009 - Compute pairwise Manhattan distance and Pearson correlation coefficient of data points with GPU.pdf:pdf},
isbn = {9780769536422},
issn = {0769536425},
journal = {10th ACIS Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing, SNPD 2009, In conjunction with IWEA 2009 and WEACR 2009},
keywords = {Hierarchical clustering,Parallel and distributed computation,Similarity and dissimilarity metrics},
pages = {501--506},
title = {{Compute pairwise Manhattan distance and Pearson correlation coefficient of data points with GPU}},
year = {2009}
}
@article{Chen2014a,
author = {Chen, Shangyi and Li, Wei and Li, Min and Zhang, Xiaofei and Min, Yue},
doi = {10.1109/CCBD.2014.25},
file = {:home/chiroptera/Dropbox/mendeley/Chen et al. - 2014 - Latest Progress and Infrastructure Innovations of Big Data Technology.pdf:pdf},
isbn = {978-1-4799-6621-9},
journal = {2014 International Conference on Cloud Computing and Big Data},
keywords = {big data,incremental,mapreduce},
mendeley-tags = {big data},
pages = {8--15},
title = {{Latest Progress and Infrastructure Innovations of Big Data Technology}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7062865},
year = {2014}
}
@article{Feynman1982,
abstract = {A digital computer is generally believed to be an efficient universal computing device; that is, it is believed able to simulate any physical computing device with an increase in computation time of at most a polynomial factor. This may not be true when quantum mechanics is taken into consideration. This paper considers factoring integers and finding discrete logarithms, two problems which are generally thought to be hard on a classical computer and have been used as the basis of several proposed cryptosystems. Efficient randomized algorithms are given for these two problems on a hypothetical quantum computer. These algorithms take a number of steps polynomial in the input size, e.g., the number of digits of the integer to be factored.  AMS subject classifications: 82P10, 11Y05, 68Q10. 1 Introduction  One of the first results in the mathematics of computation, which underlies the subsequent development of much of theoretical computer science, was the distinction between computable and ...},
archivePrefix = {arXiv},
arxivId = {quant-ph/9508027},
author = {Feynman, Richard P.},
doi = {10.1007/BF02650179},
eprint = {9508027},
file = {:home/chiroptera/Dropbox/mendeley/Feynman - 1982 - Simulating physics with computers.pdf:pdf},
isbn = {0020774815729575},
issn = {00207748},
journal = {International Journal of Theoretical Physics},
number = {6-7},
pages = {467--488},
primaryClass = {quant-ph},
title = {{Simulating physics with computers}},
volume = {21},
year = {1982}
}
@inproceedings{Li2010,
abstract = {Based on the concepts and principles of quantum computing, a novel clustering algorithm, called a quantum-inspired immune clonal clustering algorithm based on watershed (QICW), is proposed to deal with the problem of image segmentation. In QICW, antibody is proliferated and divided into a set of subpopulation groups. Antibodies in a subpopulation group are represented by multi-state gene quantum bits. In the antibody's updating, the quantum mutation operator is applied to accelerate convergence. The quantum recombination realizes the information communication between the subpopulation groups so as to avoid premature convergences. In this paper, the segmentation problem is viewed as a combinatorial optimization problem, the original image is partitioned into small blocks by watershed algorithm, and the quantum-inspired immune clonal algorithm is used to search the optimal clustering centre, and make the sequence of maximum affinity function as clustering result, and finally obtain the segmentation result. Experimental results show that the proposed method is effective for texture image and SAR image segmentation, compared with the genetic clustering algorithm based on watershed (W-GAC), and the k-means algorithm based on watershed (W-KM).},
author = {Li, Yangyang and Wu, Nana and Ma, Jingjing and Jiao, Licheng},
booktitle = {IEEE Congress on Evolutionary Computation},
doi = {10.1109/CEC.2010.5586362},
file = {:home/chiroptera/Dropbox/mendeley/Li et al. - 2010 - Quantum-inspired immune clonal clustering algorithm based on watershed.pdf:pdf},
isbn = {978-1-4244-6909-3},
keywords = {Clustering algorithms,Error analysis,Feature extraction,Image segmentation,Partitioning algorithms,Radiative recombination,SAR image segmentation,Wavelet transforms,combinatorial mathematics,combinatorial optimization problem,genetic algorithms,genetic clustering algorithm,image segmentation,image texture,k-means algorithm,maximum affinity function,multistate gene quantum bits,pattern clustering,quantum computing,quantum mutation operator,quantum-inspired immune clonal clustering algorith,synthetic aperture radar,texture image,watershed algorithm},
month = {jul},
pages = {1--7},
publisher = {IEEE},
shorttitle = {Evolutionary Computation (CEC), 2010 IEEE Congress},
title = {{Quantum-inspired immune clonal clustering algorithm based on watershed}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5586362},
year = {2010}
}
@inproceedings{lourenco2014ecg,
author = {Lourenco, Andre and Carreiras, Carlos and Bulo, Samuel Rota and Fred, Ana},
booktitle = {Signal Processing Conference (EUSIPCO), 2014 Proceedings of the 22nd European},
file = {:home/chiroptera/Dropbox/mendeley//Lourenco et al. - 2014 - ECG analysis using consensus clustering.pdf:pdf},
organization = {IEEE},
pages = {511--515},
title = {{ECG analysis using consensus clustering}},
url = {Lourenco2009},
year = {2014}
}
@article{johnson1984extensions,
author = {Johnson, William B and Lindenstrauss, Joram},
file = {:home/chiroptera/Dropbox/mendeley/Johnson, Lindenstrauss - 1984 - Extensions of Lipschitz mappings into a Hilbert space.pdf:pdf},
journal = {Contemporary mathematics},
number = {189-206},
pages = {1},
title = {{Extensions of Lipschitz mappings into a Hilbert space}},
volume = {26},
year = {1984}
}
@article{vitter2001dealing,
author = {Vitter, Jeffrey Scott},
file = {:home/chiroptera/Dropbox/mendeley/Vitter - 2001 - Dealing with Massive Data.pdf:pdf},
journal = {ACM Computing Surveys},
number = {2},
title = {{Dealing with Massive Data}},
volume = {33},
year = {2001}
}
@article{Brassard,
author = {Brassard, Gilles and Centre-ville, Succursale},
file = {:home/chiroptera/Dropbox/mendeley/Brassard, Centre-ville - Unknown - Quantum Clustering Algorithms(2).pdf:pdf;:home/chiroptera/Dropbox/mendeley/Brassard, Centre-ville - Unknown - Quantum Clustering Algorithms.pdf:pdf},
title = {{Quantum Clustering Algorithms}}
}
@article{Fraire2013,
author = {Fraire, Juan a. and Ferreyra, Alejandro and Marques, Carlos},
doi = {10.1109/TLA.2013.6502816},
file = {:home/chiroptera/Dropbox/mendeley/Fraire, Ferreyra, Marques - 2013 - OpenCL Overview, Implementation, and Performance Comparison.pdf:pdf},
issn = {1548-0992},
journal = {IEEE Latin America Transactions},
keywords = {heterogeneous},
number = {1},
pages = {274--280},
title = {{OpenCL Overview, Implementation, and Performance Comparison}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6502816},
volume = {11},
year = {2013}
}
@article{Baldini2014a,
author = {Baldini, Ioana and Fink, Stephen J. and Altman, Erik},
doi = {10.1109/SBAC-PAD.2014.30},
file = {:home/chiroptera/Dropbox/mendeley/Baldini, Fink, Altman - 2014 - Predicting GPU Performance from CPU Runs Using Machine Learning.pdf:pdf},
isbn = {978-1-4799-6905-0},
journal = {2014 IEEE 26th International Symposium on Computer Architecture and High Performance Computing},
pages = {254--261},
title = {{Predicting GPU Performance from CPU Runs Using Machine Learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6970672},
year = {2014}
}
@article{vitter2008algorithms,
author = {Vitter, Jeffrey Scott},
file = {:home/chiroptera/Dropbox/mendeley/Vitter - 2008 - Algorithms and data structures for external memory.pdf:pdf},
journal = {Foundations and Trends{\{}$\backslash$textregistered{\}} in Theoretical Computer Science},
number = {4},
pages = {305--474},
publisher = {Now Publishers Inc.},
title = {{Algorithms and data structures for external memory}},
volume = {2},
year = {2008}
}
@article{Horn2001b,
author = {Horn, David and Gottlieb, Assaf},
doi = {10.1103/PhysRevLett.88.018702},
file = {:home/chiroptera/Dropbox/mendeley//Horn, Gottlieb - 2001 - Algorithm for Data Clustering in Pattern Recognition Problems Based on Quantum Mechanics.ps:ps;:home/chiroptera/Dropbox/mendeley/Horn, Gottlieb - 2001 - Algorithm for Data Clustering in Pattern Recognition Problems Based on Quantum Mechanics.pdf:pdf},
issn = {0031-9007},
journal = {Physical Review Letters},
number = {1},
pages = {1--4},
title = {{Algorithm for Data Clustering in Pattern Recognition Problems Based on Quantum Mechanics}},
url = {http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.88.018702},
volume = {88},
year = {2001}
}
@article{Meila2003,
abstract = {This paper proposes an information theoretic criterion for comparing two partitions, or clusterings, of the same data set. The criterion, called variation of information (VI), measures the amount of information lost and gained in changing from clustering {\$}{\{}\backslashcal C{\}}{\$} to clustering {\$}{\{}\backslashcal C{\}}'{\$} . The criterion makes no assumptions about how the clusterings were generated and applies to both soft and hard clusterings. The basic properties of VI are presented and discussed from the point of view of comparing clusterings. In particular, the VI is positive, symmetric and obeys the triangle inequality. Thus, surprisingly enough, it is a true metric on the space of clusterings. Keywords: Clustering; Comparing partitions; Measures of agreement; Information theory; Mutual information},
author = {Meila, Marina},
doi = {10.1007/978-3-540-45167-9{\_}14},
file = {:home/chiroptera/Dropbox/mendeley/Meila - 2003 - Comparing clusterings by the variation of information.pdf:pdf},
isbn = {978-3-540-40720-1, 978-3-540-45167-9},
issn = {03029743},
journal = {Learning theory and Kernel machines: 16th Annual Conference on Learning Theory and 7th Kernel Workshop, COLT/Kernel 2003, Washington, DC, USA, August 24-27, 2003: proceedings},
keywords = {clustering,comparing partitions,information theory,measures of agreement,mutual information},
pages = {173},
title = {{Comparing clusterings by the variation of information}},
url = {http://books.google.com/books?hl=en{\&}amp;lr={\&}amp;id=hk1dqsM0XF4C{\&}amp;oi=fnd{\&}amp;pg=PA173{\&}amp;dq=Comparing+Clusterings+by+the+Variation+of+Information{\&}amp;ots=7rcmrLpFV1{\&}amp;sig=P-AXGQnlenPfAlSb3fdhphYv6dI},
year = {2003}
}
@inproceedings{Wang2007,
abstract = {A new hybrid fuzzy clustering algorithm that incorporates the fuzzy c-means (FCM) into the quantum-behaved particle swarm optimization (QPSO) algorithm is proposed in this paper (QPSO+FCM). The QPSO has less parameters and higher convergent capability of the global optimizing than particle swarm optimization algorithm (PSO). So the iteration algorithm is replaced by the QPSO based on the gradient descent of FCM, which makes the algorithm have a strong global searching capacity and avoids the local minimum problems of FCM and in a large degree avoids depending on the initialization values. This paper also investigates the ability of FCM algorithm, PSO+FCM algorithm and GA+FCM algorithm with Iris testing data and Wine testing data. The simulation result proves that compared with other algorithms, the new algorithm not only has the favorable convergence but also has been obviously improved the clustering effect.},
author = {Wang, Hao and Yang, Shiqin and Xu, Wenbo and Sun, Jun},
booktitle = {Fourth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2007)},
doi = {10.1109/FSKD.2007.507},
file = {:home/chiroptera/Dropbox/mendeley/Wang et al. - 2007 - Scalability of Hybrid Fuzzy C-Means Algorithm Based on Quantum-Behaved PSO.pdf:pdf},
isbn = {978-0-7695-2874-8},
pages = {261--265},
title = {{Scalability of Hybrid Fuzzy C-Means Algorithm Based on Quantum-Behaved PSO}},
volume = {2},
year = {2007}
}
@article{Banerjee,
author = {Banerjee, Dip Sankar and Sharma, Shashank and Kothapalli, Kishore},
file = {:home/chiroptera/Dropbox/mendeley/Banerjee, Sharma, Kothapalli - Unknown - Work Efficient Parallel Algorithms for Large Graph Exploration.pdf:pdf},
isbn = {9781479907304},
keywords = {gpu,graph},
mendeley-tags = {gpu,graph},
pages = {433--442},
title = {{Work Efficient Parallel Algorithms for Large Graph Exploration}}
}
@article{Bradley2000,
abstract = {We consider practical methods for adding constraints to the K-Means clustering algorithm in order to avoid local solutions with empty clusters or clusters having very few points. We often observe this phenomena when applying K-Means to datasets where the number of dimensions is n 10 and the number of desired clusters is k 20. We propose explicitly adding k constraints to the underlying clustering optimization problem requiring that each cluster have at least a minimum number of points in it. We then investigate the resulting cluster assignment step. Preliminary numerical tests on real datasets indicate the constrained approach is less prone to poor local solutions, producing a better summary of the underlying data.},
author = {Bradley, Ps and Bradley, Ps and Bennett, Kp and Bennett, Kp and Demiriz, a and Demiriz, a},
doi = {10.1016/S0025-7753(14)70064-8},
file = {:home/chiroptera/Dropbox/mendeley/Bradley et al. - 2000 - Constrained k-means clustering.pdf:pdf},
issn = {0025-7753},
journal = {Technical Report},
pages = {9},
pmid = {24913746},
title = {{Constrained k-means clustering}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.33.3257},
year = {2000}
}
@article{Arefin2012,
author = {Arefin, Ahmed Shamsul and Riveros, Carlos and Berretta, Regina and Moscato, Pablo},
doi = {10.1109/ICCSE.2012.6295143},
file = {:home/chiroptera/Dropbox/mendeley/Arefin et al. - 2012 - kNN-MST-Agglomerative A fast and scalable graph-based data clustering approach on GPU.pdf:pdf},
isbn = {9781467302425},
journal = {ICCSE 2012 - Proceedings of 2012 7th International Conference on Computer Science and Education},
keywords = {Computer Application},
number = {Iccse},
pages = {585--590},
title = {{kNN-MST-Agglomerative: A fast and scalable graph-based data clustering approach on GPU}},
year = {2012}
}
@article{Ho2008,
author = {Ho, Tin Kam},
doi = {10.1007/978-3-540-89689-0{\_}102},
file = {:home/chiroptera/Dropbox/mendeley/Ho - 2008 - Data complexity analysis Linkage between context and solution in classification.pdf:pdf},
isbn = {3540896880},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {986--995},
title = {{Data complexity analysis: Linkage between context and solution in classification}},
volume = {5342 LNCS},
year = {2008}
}
@article{Mullner2011,
abstract = {This paper presents algorithms for hierarchical, agglomerative clustering which perform most efficiently in the general-purpose setup that is given in modern standard software. Requirements are: (1) the input data is given by pairwise dissimilarities between data points, but extensions to vector data are also discussed (2) the output is a "stepwise dendrogram", a data structure which is shared by all implementations in current standard software. We present algorithms (old and new) which perform clustering in this setting efficiently, both in an asymptotic worst-case analysis and from a practical point of view. The main contributions of this paper are: (1) We present a new algorithm which is suitable for any distance update scheme and performs significantly better than the existing algorithms. (2) We prove the correctness of two algorithms by Rohlf and Murtagh, which is necessary in each case for different reasons. (3) We give well-founded recommendations for the best current algorithms for the various agglomerative clustering schemes.},
archivePrefix = {arXiv},
arxivId = {1109.2378},
author = {M{\"{u}}llner, Daniel},
eprint = {1109.2378},
file = {:home/chiroptera/Dropbox/mendeley/M{\"{u}}llner - 2011 - Modern hierarchical, agglomerative clustering algorithms.pdf:pdf},
journal = {arXiv preprint arXiv:1109.2378},
keywords = {agglomerative,clustering,hierarchical,linkage,partition},
title = {{Modern hierarchical, agglomerative clustering algorithms}},
url = {http://arxiv.org/abs/1109.2378},
year = {2011}
}
@article{Jung2010,
abstract = {This paper proposes a new connected component labeling algorithm for GPGPU applications based on NVIDIA's CUDA. Various approaches and algorithms for connected component labeling with minimal execution time were designed, but the most of them have been focused on optimizing CPU algorithm. Therefore it is hard to apply these approaches to GPGPU programming models such as NVIDIA's CUDA. Today, GPGPU (General Purpose Graphic Processing Unit) technologies offer dedicated parallel hardware and programming model, and many applications are being moved onto the GPGPU. This algorithm is a multi-pass algorithm to utilize for GPGPU applications, and evaluation results show that maximum speedup is more than double compared with conventional CPU algorithms.},
author = {Jung, In Yong and Jeong, Chang Sung},
doi = {10.1109/ISCIT.2010.5665161},
file = {:home/chiroptera/Dropbox/mendeley/Jung, Jeong - 2010 - Parallel connected-component labeling algorithm for GPGPU applications.pdf:pdf},
isbn = {9781424470105},
journal = {ISCIT 2010 - 2010 10th International Symposium on Communications and Information Technologies},
pages = {1149--1153},
title = {{Parallel connected-component labeling algorithm for GPGPU applications}},
year = {2010}
}
@article{Calders,
author = {Calders, Toon},
file = {:home/chiroptera/Dropbox/mendeley/Calders - Unknown - Data Mining Clustering What is Cluster Analysis.pdf:pdf},
title = {{Data Mining Clustering What is Cluster Analysis ?}}
}
@article{Republic2010,
author = {Republic, Czech and Mareboyana, Manohar},
file = {:home/chiroptera/Dropbox/mendeley/Republic, Mareboyana - 2010 - GPU Accelerated One-pass Algorithm for Computing Minimal Rectangles of Connected Components Lubom{\'{\i}}r Ř{\'{\i}}h.pdf:pdf},
isbn = {9781424494972},
journal = {Components},
pages = {479--484},
title = {{GPU Accelerated One-pass Algorithm for Computing Minimal Rectangles of Connected Components Lubom{\'{\i}}r Ř{\'{\i}}ha Manohar Mareboyana Bowie State University}},
year = {2010}
}
@article{He2012,
author = {He, Lifeng},
file = {:home/chiroptera/Dropbox/mendeley/He - 2012 - A new Algorithm for Labeling Connected-Components and Calculating the Euler Number , Connected-Component Number , and Hole.pdf:pdf},
isbn = {9784990644116},
issn = {10514651},
journal = {International Conference on Pattern Recognition},
keywords = {Features and Image Descriptors,Image and Video Processing,Image and Video Understanding},
number = {Icpr},
pages = {3099--3102},
title = {{A new Algorithm for Labeling Connected-Components and Calculating the Euler Number , Connected-Component Number , and Hole Number  The University of Chicago}},
year = {2012}
}
@article{Strehl2002,
abstract = {This paper introduces the problem of combining multiple partitionings of a set of objects into a single consolidated clustering without accessing the features or algorithms that determined these partitionings. We rst identify several application scenarios for the resultant `knowledge reuse' framework that we call cluster ensembles. The cluster ensemble problem is then formalized as a combinatorial optimization problem in terms of shared mutual information. In addition to a direct maximization approach, we propose three e ective and efficient techniques for obtaining high-quality combiners (consensus functions). The rst combiner induces a similarity measure from the partitionings and then reclusters the objects. The second combiner is based on hypergraph partitioning. The third one collapses groups of clusters into meta-clusters which then compete for each object to determine the combined clustering. Due to the low computational costs of our techniques, it is quite feasible to use a supra-consensus function that evaluates all three approaches against the objective function and picks the best solution for a given situation. We evaluate the effectiveness of cluster ensembles in three qualitatively di erent application scenarios: (i) where the original clusters were formed based on non-identical sets of features, (ii) where the original clustering algorithms worked on non-identical sets of objects, and (iii) where a common data-set is used and the main purpose of combining multiple clusterings is to improve the quality and robustness of the solution. Promising results are obtained in all three situations for synthetic as well as real data-sets.},
author = {Strehl, Alexander and Ghosh, Joydeep},
doi = {10.1162/153244303321897735},
file = {:home/chiroptera/Dropbox/mendeley/Strehl, Ghosh - 2002 - Cluster Ensembles – A Knowledge Reuse Framework for Combining Multiple Partitions.pdf:pdf},
isbn = {0262511290},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {cluster analysis,clustering,clustering ensemble,consensus clustering,consensus functions,ensemble,knowledge reuse,multi-learner,mutual information,partitioning,systems,unsupervised learning},
pages = {583--617},
pmid = {21423337},
title = {{Cluster Ensembles – A Knowledge Reuse Framework for Combining Multiple Partitions}},
volume = {3},
year = {2002}
}
@article{Lloyd2013,
abstract = {Machine-learning tasks frequently involve problems of manipulating and classi- fying large numbers of vectors in high-dimensional spaces. Classical algorithms for solving such problems typically take time polynomial in the number ofvectors and the dimension of the space. Quantum computers are good at manipulating high-dimensional vectors in large tensor product spaces. This paper provides supervisedandunsupervised quantum machine learning algorithms for cluster assignment and cluster finding. Quantum machine learning can take time logarithmic in both the number of vectors and their dimension, an exponential speed-up over classical algorithms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1307.0411v2},
author = {Lloyd, Seth and Mohseni, Masoud and Rebentrost, Patrick},
eprint = {arXiv:1307.0411v2},
file = {:home/chiroptera/Dropbox/mendeley/Lloyd, Mohseni, Rebentrost - 2013 - Quantum algorithms for supervised and unsupervised machine learning.pdf:pdf},
journal = {arXiv preprint arXiv:1307.0411},
keywords = {Machine Learning,quantum computers},
mendeley-tags = {quantum computers},
title = {{Quantum algorithms for supervised and unsupervised machine learning}},
url = {http://arxiv.org/pdf/1307.0411.pdf$\backslash$npapers2://publication/uuid/2BDCBB85-812C-46E4-B506-73B9A41EBBC1},
year = {2013}
}
@article{Chen2012,
abstract = {Accelerators and heterogeneous architectures in general, andGPUs in particular, have recently emerged asmajor players in high perfor- mance computing. For many classes of applications, MapReduce has emerged as the framework for easing parallel programming and improving programmer productivity. There have already been sev- eral efforts on implementingMapReduce on GPUs. In this paper, we propose a new implementation of MapReduce for GPUs, which is very effective in utilizing shared memory, a small programmable cache on modern GPUs. The main idea is to use a reduction-based method to execute aMapReduce applica- tion. The reduction-based method allows us to carry out reductions in shared memory. To support a general and efficient implemen- tation, we support the following features: a memory hierarchy for maintaining the reduction object, a multi-group scheme in shared memory to trade-off space requirements and locking overheads, a general and efficient data structure for the reduction object, and an efficient swapping mechanism. We have evaluated our framework with seven commonly used MapReduce applications and compared it with the sequential im- plementations, MapCG, a recent MapReduce implementation on GPUs, and Ji et al.’s work, a recent MapReduce implementation that utilizes shared memory in a different way. The main observa- tions from our experimental results are as follows. For four of the seven applications that can be considered as reduction-intensive ap- plications, our framework has a speedup of between 5 and 200 over MapCG (for large datasets). Similarly, we achieved a speedup of between 2 and 60 over Ji et al.’s work.},
author = {Chen, Linchuan and Agrawal, Gagan},
doi = {10.1145/2287076.2287109},
file = {:home/chiroptera/Dropbox/mendeley/Chen, Agrawal - 2012 - Optimizing MapReduce for GPUs with Effective Shared Memory Usage.pdf:pdf},
isbn = {9781450308052},
journal = {Proceedings of the 21st international symposium on High-Performance Parallel and Distributed Computing (HPDC'12)},
keywords = {MapReduce,gpu,mapreduce,shared memory},
mendeley-tags = {MapReduce,gpu},
pages = {199--210},
title = {{Optimizing MapReduce for GPUs with Effective Shared Memory Usage}},
url = {http://dl.acm.org/citation.cfm?doid=2287076.2287109$\backslash$nhttp://dl.acm.org/citation.cfm?id=2287109},
year = {2012}
}
@article{Mullner2013,
abstract = {The fastcluster package is a C++ library for hierarchical, agglomerative clustering. It provides a fast implementation of the most efficient, current algorithms when the input is a dissimilarity index. Moreover, it features memory-saving routines for hierarchical clustering of vector data. It improves both asymptotic time complexity (in most cases) and practical performance (in all cases) compared to the existing implementations in standard software: several R packages, MATLAB, Mathematica, Python with SciPy.},
archivePrefix = {arXiv},
arxivId = {1109.2378},
author = {M{\"{u}}llner, Daniel},
eprint = {1109.2378},
file = {:home/chiroptera/Dropbox/mendeley/M{\"{u}}llner - 2013 - fastcluster Fast Hierarchical , Agglomerative.pdf:pdf},
issn = {1548-7660},
journal = {Journal of Statistical Software},
keywords = {age,agglomerative,algorithm,aver-,c,centroid,clustering,complete,hierarchical,linkage,mathematica,matlab,mcquitty,median,python,scipy,single,upgma,upgmc,ward,weighted,wpgma,wpgmc},
number = {9},
pages = {1--18},
title = {{fastcluster : Fast Hierarchical , Agglomerative}},
url = {http://www.jstatsoft.org/v53/i09},
volume = {53},
year = {2013}
}
@article{Guha2003,
abstract = {The data stream model has recently attracted attention for its applicability to numerous types of data, including telephone records, Web documents, and clickstreams. For analysis of such data, the ability to process the data in a single pass, or a small number of passes, while using little memory, is crucial. We describe such a streaming algorithm that effectively clusters large data streams. We also provide empirical evidence of the algorithm's performance on synthetic and real data streams.},
author = {Guha, Sudipto and Meyerson, a},
doi = {10.1109/TKDE.2003.1198387},
file = {:home/chiroptera/Dropbox/mendeley/Guha, Meyerson - 2003 - Clustering data streams Theory and practice.pdf:pdf},
issn = {1041-4347},
journal = {Knowledge and Data {\ldots}},
pages = {1--33},
title = {{Clustering data streams: Theory and practice}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1198387},
year = {2003}
}
@article{mikki2006quantum,
author = {Mikki, Said M and Kishk, Ahmed and Others},
file = {:home/chiroptera/Dropbox/mendeley/Mikki, Kishk, others - 2006 - Quantum particle swarm optimization for electromagnetics.pdf:pdf},
journal = {Antennas and Propagation, IEEE Transactions on},
number = {10},
pages = {2764--2775},
publisher = {IEEE},
title = {{Quantum particle swarm optimization for electromagnetics}},
volume = {54},
year = {2006}
}
@misc{Lichman:2013,
author = {Lichman, M},
institution = {University of California, Irvine, School of Information and Computer Sciences},
title = {{{\{}UCI{\}} Machine Learning Repository}},
url = {http://archive.ics.uci.edu/ml},
year = {2013}
}
@misc{Sneath1992,
author = {Sneath, Peter H. a. and Sokal, Robert R.},
file = {:home/chiroptera/Dropbox/mendeley/Sneath, Sokal - 1973 - Numerical taxonomy. The principles and pratice of numerical classification.pdf:pdf},
pages = {15},
title = {{Numerical taxonomy. The principles and pratice of numerical classification.}},
year = {1973}
}
@misc{Horn2010,
author = {Horn, David and Aviv, Tel and Gottlieb, Assaf and HaSharon, Hod and Axel, Inon and Gan, Ramat},
file = {:home/chiroptera/Dropbox/mendeley/Horn et al. - 2010 - Method and Apparatus for Quantum Clustring.pdf:pdf},
number = {12},
title = {{Method and Apparatus for Quantum Clustring}},
volume = {2},
year = {2010}
}
@article{Winterstein1997,
author = {Winterstein, Felix and Bayliss, Samuel and Constantinides, Geoge a.},
file = {:home/chiroptera/Dropbox/mendeley/Winterstein, Bayliss, Constantinides - 1997 - FPGA-Based K-Means Clustering Using Tree-Based Data Structures.pdf:pdf},
pages = {1450--1455},
title = {{FPGA-Based K-Means Clustering Using Tree-Based Data Structures}},
volume = {101},
year = {1997}
}
@article{Fred2009b,
author = {Fred, Ana},
file = {:home/chiroptera/Dropbox/mendeley/Fred - 2009 - Tutorial Pt. 4 - Clustering Ensemble Methods.pdf:pdf},
journal = {Methods},
number = {April},
pages = {1--27},
title = {{Tutorial Pt. 4 - Clustering Ensemble Methods}},
year = {2009}
}
@misc{Guerra2009,
abstract = {The integration of usable and flexible analysis support in modelling environments is a key success factor in Model-Driven Development. In this paradigm, models are the core asset from which code is automatically generated, and thus ensuring model correctness is a fundamental quality control activity. For this purpose, a common approach is to transform the system models into formal semantic domains for verification. However, if the analysis results are not shown in a proper way to the end-user (e.g. in terms of the original language) they may become useless. In this paper we present a novel DSVL called BaVeL that facilitates the flexible annotation of verification results obtained in semantic domains to different formats, including the context of the original language. BaVeL is used in combination with a consistency framework, providing support for all steps in a verification process: acquisition of additional input data, transformation of the system models into semantic domains, verification, and flexible annotation of analysis results. The approach has been validated analytically by the cognitive dimensions framework, and empirically by its implementation and application to several DSVLs. Here we present a case study of a notation in the area of Digital Libraries, where the analysis is performed by transformations into Petri nets and a process algebra. © 2008 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:cond-mat/0402594v3},
author = {Guerra, Esther and de Lara, Juan and Malizia, Alessio and D{\'{\i}}az, Paloma},
booktitle = {Information and Software Technology},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {0402594v3},
file = {:home/chiroptera/Dropbox/mendeley/Guerra et al. - 2009 - Supporting user-oriented analysis for multi-view domain-specific visual languages.pdf:pdf},
isbn = {0950-5849},
issn = {09505849},
keywords = {Back-annotation,Consistency,Domain-specific visual languages,Formal methods,Model transformation,Modelling environments},
number = {4},
pages = {769--784},
primaryClass = {arXiv:cond-mat},
title = {{Supporting user-oriented analysis for multi-view domain-specific visual languages}},
volume = {51},
year = {2009}
}
@article{Duarte2005,
abstract = {We explore the idea of evidence accumulation (EAC) for combining the results of multiple clusterings. The EAC paradigm combines the information existent in n partitions into a co-association matrix (similarity matrix) based on pairwise associations, where each partition has an identical weight in the combination process. The final data partition is obtained by applying a clustering algorithm over this co-association matrix. In this paper we propose the idea of weighting differently the partitions (WEAC). Each partition contributes differently in a weighted co-association matrix depending on the quality of the partitions, as measured by internal and relative validity indices. Based on experimental results in synthetic and real data sets, the weighting of the partitions (WEAC), generally leads to a better performance than EAC. The evaluation of results is based on a consistency index between the combined partition and the "ideal" data partition taken as ground truth},
author = {Duarte, F.J. and a.L.N. Fred and Lourenco, a. and Rodrigues, M.F.},
doi = {10.1109/EPIA.2005.341287},
file = {:home/chiroptera/Dropbox/mendeley/Duarte et al. - 2005 - Weighting Cluster Ensembles in Evidence Accumulation Clustering.pdf:pdf},
isbn = {0-7803-9366-X},
journal = {2005 Portuguese Conference on Artificial Intelligence},
keywords = {Clustering,Combining Multiple Partitions,Validity Indices,Weighting Cluster Ensembles},
pages = {159--167},
title = {{Weighting Cluster Ensembles in Evidence Accumulation Clustering}},
volume = {00},
year = {2005}
}
@article{El-sherbiny,
abstract = {Quantum computing proved good results and performance when applied to solving optimization problems. This paper proposes a quantum crossover-based quantum genetic algorithm (QXQGA) for solving non-linear programming. Due to the significant role of mutation function on the QXQGA's quality, a number of quantum crossover and quantum mutation operators are presented for improving the capabilities of searching, overcoming premature convergence, and keeping diversity of population. For calibrating the QXQGA, the quantum crossover and mutation operators are evaluated using relative percentage deviation for selecting the best combination. In addition, a set of non-linear problems is used as benchmark functions to illustrate the effectiveness of optimizing the complexities with different dimensions, and the performance of the proposed QXQGA algorithm is compared with the quantum inspired evolutionary algorithm to demonstrate its superiority.},
author = {El-sherbiny, Mahmoud M},
file = {:home/chiroptera/Dropbox/mendeley/El-sherbiny - Unknown - Quantum Crossover Based Quantum Genetic Algorithm for Solving Non-linear Programming Quantum Rotational gate.pdf:pdf},
keywords = {-component,quantum},
title = {{Quantum Crossover Based Quantum Genetic Algorithm for Solving Non-linear Programming Quantum Rotational gate}}
}
@article{VanDerWalt2011,
abstract = {In the Python world, NumPy arrays are the standard representation for numerical data and enable efficient implementation of numerical computations in a high-level language. As this effort shows, NumPy performance can be improved through three techniques: vectorizing calculations, avoiding copying data in memory, and minimizing operation counts.},
archivePrefix = {arXiv},
arxivId = {1102.1523},
author = {{Van Der Walt}, St{\'{e}}fan and Colbert, S. Chris and Varoquaux, Ga{\"{e}}l},
doi = {10.1109/MCSE.2011.37},
eprint = {1102.1523},
file = {:home/chiroptera/Dropbox/mendeley/Van Der Walt, Colbert, Varoquaux - 2011 - The NumPy array A structure for efficient numerical computation.pdf:pdf},
isbn = {1521-9615 VO - 13},
issn = {15219615},
journal = {Computing in Science and Engineering},
keywords = {NumPy,Python,numerical computations,programming libraries,scientific programming},
number = {2},
pages = {22--30},
title = {{The NumPy array: A structure for efficient numerical computation}},
volume = {13},
year = {2011}
}
@article{rostrup2013fast,
author = {Rostrup, Scott and Srivastava, Shweta and Singhal, Kishore},
journal = {International Journal of Computational Science and Engineering},
number = {1},
pages = {21--33},
publisher = {Inderscience Publishers Ltd},
title = {{Fast and memory-efficient minimum spanning tree on the GPU}},
volume = {8},
year = {2013}
}
@article{Satish2009,
author = {Satish, Nadathur and Harris, Mark and Garland, Michael},
file = {:home/chiroptera/Dropbox/mendeley/Satish, Harris, Garland - 2009 - Designing Efficient Soring Algorithms for Manycore GPUs.pdf:pdf},
journal = {Proceedings of 23rd IEEE International Parallel and Distributed Processing Symposium},
pages = {1--10},
title = {{Designing Efficient Soring Algorithms for Manycore GPUs}},
year = {2009}
}
@article{Vacaliuc2011,
abstract = {Design of data structures for high performance computing (HPC) is one of the principal challenges facing researchers looking to utilize heterogeneous computing machinery. Heterogeneous systems derive cost, power, and speed efficiency by being composed of the appropriate hardware for the task. Yet, each type of processor requires a specific organization of the application state in order to achieve peak performance. Discovering this and refactoring the code can be a challenging and time-consuming task for the researcher, as the data structures and the computational model must be co-designed. We present a methodology that uses Python as the environment for which to explore tradeoffs in both the data structure design as well as the code executing on the computation accelerator. Our method enables multi-dimensional arrays to be used effectively in any target environment. We have chosen to focus on OpenMP and CUDA environments, thus exploring the development of optimized kernels for the two most common classes of computing hardware available today: multi-core CPU and GPU. Python's large palette of file and network access routines, its associative indexing syntax and support for common HPC environments makes it relevant for diverse hardware ranging from laptops through computing clusters to the highest performance supercomputers. Our work enables researchers to accelerate the development of their codes on the computing hardware of their choice.},
author = {Vacaliuc, Bogdan and Patlolla, Dilip R. and D'Azevedo, Ed and Davidson, Greg G. and Munro, John K. and Evans, Thomas M. and Joubert, Wayne and Bell, Zane W.},
doi = {10.1109/SAAHPC.2011.26},
file = {:home/chiroptera/Dropbox/mendeley/Vacaliuc et al. - 2011 - Python for development of OpenMP and CUDA kernels for multidimensional data.pdf:pdf},
isbn = {9780769544489},
journal = {Proceedings - 2011 Symposium on Application Accelerators in High-Performance Computing, SAAHPC 2011},
keywords = {cuda,openMP,python},
mendeley-tags = {cuda,openMP,python},
pages = {159--167},
title = {{Python for development of OpenMP and CUDA kernels for multidimensional data}},
year = {2011}
}
@inproceedings{Xin2012,
author = {Xin, Miao and Li, Hao},
booktitle = {Proceedings - 2012 International Joint Conference on Service Sciences, Service Innovation in Emerging Economy: Cross-Disciplinary and Cross-Cultural Perspective, IJCSS 2012},
doi = {10.1109/IJCSS.2012.22},
file = {:home/chiroptera/Dropbox/mendeley/Xin, Li - 2012 - An implementation of GPU accelerated MapReduce Using Hadoop with OpenCL for data- and compute-intensive jobs.pdf:pdf},
isbn = {9780769547312},
keywords = {GPU acceleration,Hadoop,MapReduce,OpenCL},
pages = {6--11},
title = {{An implementation of GPU accelerated MapReduce: Using Hadoop with OpenCL for data- and compute-intensive jobs}},
year = {2012}
}
@article{Dash2002,
abstract = { Processing applications with a large number of dimensions has been a challenge for the KDD community. Feature selection, an effective dimensionality reduction technique, is an essential pre-processing method to remove noisy features. In the literature only a few methods have been proposed for feature selection for clustering, and almost all these methods are 'wrapper' techniques that require a clustering algorithm to evaluate candidate feature subsets. The wrapper approach is largely unsuitable in real-world applications due to its heavy reliance on clustering algorithms that require parameters such as the number of clusters, and the lack of suitable clustering criteria to evaluate clustering in different subspaces. In this paper we propose a 'filter' method that is independent of any clustering algorithm. The proposed method is based on the observation that data with clusters has a very different point-to-point distance histogram to that of data without clusters. By exploiting this we propose an entropy measure that is low if data has distinct clusters and high if it does not. The entropy measure is suitable for selecting the most important subset of features because it is invariant with the number of dimensions, and is affected only by the quality of clustering. Extensive performance evaluation over synthetic, benchmark, and real datasets shows its effectiveness.},
author = {Dash, M. and Choi, K. and Scheuermann, P. and Liu, Huan Liu Huan},
doi = {10.1109/ICDM.2002.1183893},
file = {:home/chiroptera/Dropbox/mendeley/Dash et al. - 2002 - Feature selection for clustering - a filter solution.pdf:pdf},
isbn = {0-7695-1754-4},
issn = {01628828},
journal = {2002 IEEE International Conference on Data Mining, 2002. Proceedings.},
number = {September 2015},
title = {{Feature selection for clustering - a filter solution}},
year = {2002}
}
@article{Blelloch1996,
author = {Blelloch, Guy E. and Maggs, Bruce M.},
doi = {10.1145/234313.234339},
file = {:home/chiroptera/Dropbox/mendeley/Blelloch, Maggs - 1996 - Improved Parallel Algorithms for Finding Connected Components.pdf:pdf},
isbn = {978-1-58488-820-8},
issn = {03600300},
journal = {ACM Computing Surveys},
number = {1},
pages = {51--54},
title = {{Improved Parallel Algorithms for Finding Connected Components}},
volume = {28},
year = {1996}
}
@article{Singh1977,
annote = {Absolute trash paper. Use only to get references for applications of GPGPU.},
author = {Singh, Sarabjeet and Singh, Satvir and Banga, Vijay and Chauhan, Durlabh},
file = {:home/chiroptera/Dropbox/mendeley/Singh et al. - 2013 - CUDA for GPGPU Applications – A Survey.pdf:pdf},
journal = {Proc. National Conference on Contemporary Techniques {\&} Technologies in Electronics Engineering, Murthal, Sonepat, India},
keywords = {GPGPU,examples},
mendeley-tags = {GPGPU,examples},
pages = {1--4},
title = {{CUDA for GPGPU Applications – A Survey}},
year = {2013}
}
@article{Chiosa2011,
author = {Chiosa, Iurie and Kolb, Andreas},
file = {:home/chiroptera/Dropbox/mendeley/Chiosa, Kolb - 2011 - GPU-Based Multilevel Clustering.pdf:pdf},
number = {2},
pages = {132--145},
title = {{GPU-Based Multilevel Clustering}},
volume = {17},
year = {2011}
}
@article{Pakhira2009,
author = {Pakhira, Malay K},
file = {:home/chiroptera/Dropbox/mendeley/Pakhira - 2009 - A Modified k -means Algorithm to Avoid Empty Clusters.pdf:pdf},
journal = {International Journal of Recent Trends in Enineering},
number = {1},
title = {{A Modified k -means Algorithm to Avoid Empty Clusters}},
volume = {1},
year = {2009}
}
@article{Fred2009,
author = {Fred, Ana},
file = {:home/chiroptera/Dropbox/mendeley/Fred - 2009 - Tutorial Pt. 3 Validation of Custering Solutions.pdf:pdf},
number = {April},
pages = {1--15},
title = {{Tutorial Pt. 3: Validation of Custering Solutions}},
year = {2009}
}
@article{Neelima2010,
abstract = {With the growth of Graphics Processor (GPU) programmability and processing power, graphics hardware has become a compelling platform for computationally demanding tasks in a wide variety of application domains. This state of art paper gives the technical motivations that underlie GPU computing and describe the hardware and software developments that have led to the recent interest in this field.},
author = {Neelima, B. and Raghavendra, Prakash S.},
doi = {10.1109/ICIINFS.2010.5578685},
file = {:home/chiroptera/Dropbox/mendeley/Neelima, Raghavendra - 2010 - Recent trends in software and hardware for GPGPU computing A comprehensive survey.pdf:pdf},
isbn = {9781424466535},
journal = {2010 5th International Conference on Industrial and Information Systems, ICIIS 2010},
keywords = {GPU computing,Graphics Processor (GPU)},
pages = {319--324},
title = {{Recent trends in software and hardware for GPGPU computing: A comprehensive survey}},
year = {2010}
}
@article{Kaldewey2011,
author = {Kaldewey, Tim},
file = {:home/chiroptera/Dropbox/mendeley/Kaldewey - 2011 - Large-Scale GPU programming.pdf:pdf},
keywords = {gpu},
mendeley-tags = {gpu},
title = {{Large-Scale GPU programming}},
year = {2011}
}
@article{Aidos2012,
author = {Aidos, Helena and Fred, Ana},
doi = {10.1016/j.patcog.2011.12.009},
file = {:home/chiroptera/Dropbox/mendeley/Aidos, Fred - 2012 - Statistical modeling of dissimilarity increments for d-dimensional data Application in partitional clustering.pdf:pdf},
issn = {0031-3203},
journal = {Pattern Recognition},
keywords = {Dissimilarity increments,Gaussian mixture decomposition,Likelihood-ratio test,Minimum description length,Partitional clustering,dissimilarity increments,likelihood-ratio test,partitional clustering},
number = {9},
pages = {3061--3071},
publisher = {Elsevier},
title = {{Statistical modeling of dissimilarity increments for d-dimensional data Application in partitional clustering}},
url = {http://dx.doi.org/10.1016/j.patcog.2011.12.009},
volume = {45},
year = {2012}
}
@article{Chang2014,
author = {Chang, Wan Yu and Chiu, Chung Cheng},
doi = {10.1109/IS3C.2014.81},
file = {:home/chiroptera/Dropbox/mendeley/Chang, Chiu - 2014 - Directional Connected Components Algorithm Based on Gradient Information.pdf:pdf},
isbn = {978-1-4799-5277-9},
journal = {2014 International Symposium on Computer, Consumer and Control},
keywords = {-directional connected components,components,connected,gradient information,overlapping components},
pages = {280--283},
title = {{Directional Connected Components Algorithm Based on Gradient Information}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6845873},
year = {2014}
}
@article{Pedregosa2011,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1201.0490},
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
eprint = {1201.0490},
file = {:home/chiroptera/Dropbox/mendeley/Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python(2).pdf:pdf;:home/chiroptera/Dropbox/mendeley/Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf:pdf},
journal = {Journal of Machine Learning Research},
number = {Oct},
pages = {2825--2830},
title = {{Scikit-learn: Machine Learning in Python}},
url = {http://arxiv.org/abs/1201.0490 http://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf},
volume = {12},
year = {2011}
}
@article{Pettie2002,
author = {Pettie, Seth and Ramachandran, Vijaya},
doi = {10.1137/S0097539700371065},
file = {:home/chiroptera/Dropbox/mendeley/Pettie, Ramachandran - 2002 - A Randomized Time-Work Optimal Parallel Algorithm for Finding a Minimum Spanning Forest.pdf:pdf},
issn = {0097-5397},
journal = {SIAM Journal on Computing},
number = {6},
pages = {1879--1895},
title = {{A Randomized Time-Work Optimal Parallel Algorithm for Finding a Minimum Spanning Forest}},
volume = {31},
year = {2002}
}
@article{Jain1999,
abstract = {Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval.},
author = {Jain, a. K. and Murty, M. N. and Flynn, P. J.},
doi = {10.1145/331499.331504},
file = {:home/chiroptera/Dropbox/mendeley/Jain, Murty, Flynn - 1999 - Data clustering a review.pdf:pdf},
isbn = {0360-0300},
issn = {03600300},
journal = {ACM Computing Surveys},
number = {3},
pages = {264--323},
pmid = {17707831},
title = {{Data clustering: a review}},
volume = {31},
year = {1999}
}
@misc{numba,
annote = {Version 19.1},
author = {Team, Numba Development},
title = {{Numba}},
url = {http://numba.pydata.org},
year = {2015}
}
@article{Comiss2014,
author = {Comiss, Curso Aluno and Papel, Aluno and Digital, Suporte and Cient, Conselho},
file = {:home/chiroptera/Dropbox/mendeley/Comiss et al. - 2014 - 8. Tramita{\c{c}}{\~{a}}o da disserta{\c{c}}{\~{a}}oprojeto.pdf:pdf},
keywords = {a alunos},
pages = {2013--2014},
title = {{8. Tramita{\c{c}}{\~{a}}o da disserta{\c{c}}{\~{a}}o/projeto}},
year = {2014}
}
@article{Wang2014,
author = {Wang, Wei},
doi = {10.1109/ICSC.2014.65},
file = {:home/chiroptera/Dropbox/mendeley/Wang - 2014 - Big Data, Big Challenges.pdf:pdf},
isbn = {978-1-4799-4003-5},
journal = {2014 IEEE International Conference on Semantic Computing},
pages = {6--6},
title = {{Big Data, Big Challenges}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6881994},
year = {2014}
}
@article{Boley1998,
abstract = {We propose a new algorithm capable of partitioning a set of documents or other samples based on an embedding in a high dimensional Euclidean space (i.e., in which every document is a vector of real numbers). The method is unusual in that it is divisive, as opposed to agglomerative, and operates by repeatedly splitting clusters into smaller clusters. The documents are assembled into a matrix which is very sparse. It is this sparsity that permits the algorithm to be very efficient. The performance of the method is illustrated with a set of text documents obtained from the World Wide Web. Some possible extensions are proposed for further investigation.},
author = {Boley, Daniel},
doi = {10.1023/A:1009740529316},
file = {:home/chiroptera/Dropbox/mendeley/Boley - 1998 - Principal Direction Divisive Partitioning.pdf:pdf},
issn = {1384-5810},
journal = {Data Mining and Knowledge Discovery},
number = {4},
pages = {325--344},
title = {{Principal Direction Divisive Partitioning}},
url = {http://dx.doi.org/10.1023/A:1009740529316$\backslash$nhttp://www.springerlink.com/content/w15313n737603612/},
volume = {2},
year = {1998}
}
@misc{Graham1985,
abstract = {It is standard practice among authors discussing the minimum spanning tree problem to refer to the work of Kruskal(1956) and Prim (1957) as the sources of the problem and its first efficient solutions, despite the citation by both of Boruvka (1926) as a predecessor. In fact, there are several apparently independent sources and algorithmic solutions of the problem. They have appeared in Czechoslovakia, France, and Poland, going back to the beginning of this century. We shall explore and compare these works and their motivations, and relate them to the most recent advances on the minimum spanning tree problem.},
author = {Graham, R.L. and Hell, Pavol},
booktitle = {IEEE Annals of the History of Computing},
doi = {10.1109/MAHC.1985.10011},
file = {:home/chiroptera/Dropbox/mendeley/Graham, Hell - 1985 - On the History of the Minimum Spanning Tree Problem.pdf:pdf},
issn = {1058-6180},
number = {1},
pages = {43--57},
title = {{On the History of the Minimum Spanning Tree Problem}},
volume = {7},
year = {1985}
}
@article{Zaroliagis1997,
author = {Zaroliagis, Christos D.},
doi = {10.1142/S012962649700005X},
file = {:home/chiroptera/Dropbox/mendeley/Zaroliagis - 1997 - Simple and Work-Efficient Parallel Algorithms for the Minimum Spanning Tree Problem.pdf:pdf},
issn = {0129-6264},
journal = {Parallel Processing Letters},
keywords = {applications 1,g with real,given a connected n,inten-,m -edge undirected graph,minimum spanning tree,network optimization with many,parallel random access machine,problem is one of,sively studied problems in,the minimum spanning tree,the most fundamental and,theoretical and practical,vertex},
number = {01},
pages = {25--37},
title = {{Simple and Work-Efficient Parallel Algorithms for the Minimum Spanning Tree Problem}},
volume = {07},
year = {1997}
}
@article{anderson1936species,
author = {Anderson, Edgar},
journal = {Annals of the Missouri Botanical Garden},
pages = {457--509},
publisher = {JSTOR},
title = {{The species problem in Iris}},
year = {1936}
}
@misc{JonesSciPy,
annote = {[Online; accessed 2015-08-24]},
author = {Jones, Eric and Oliphant, Travis and Peterson, Pearu and Others},
title = {{SciPy: Open source scientific tools for Python}},
url = {http://www.scipy.org/}
}
@article{Karger1995,
author = {Karger, David R. and Klein, Philip N. and Tarjan, Robert E.},
doi = {10.1145/201019.201022},
file = {:home/chiroptera/Dropbox/mendeley/Karger, Klein, Tarjan - 1995 - A randomized linear-time algorithm to find minimum spanning trees.pdf:pdf},
issn = {00045411},
journal = {Journal of the ACM},
number = {2},
pages = {321--328},
title = {{A randomized linear-time algorithm to find minimum spanning trees}},
volume = {42},
year = {1995}
}
@article{Arbelaez2013,
author = {Arbelaez, Alejandro and Quesada, Luis},
file = {:home/chiroptera/Dropbox/mendeley/Arbelaez, Quesada - 2013 - Parallelising the k-Medoids Clustering Problem Using Space-Partitioning.pdf:pdf},
journal = {Sixth Annual Symposium on Combinatorial Search},
keywords = {Full Papers,k-medoids,parallel},
mendeley-tags = {k-medoids,parallel},
pages = {20--28},
title = {{Parallelising the k-Medoids Clustering Problem Using Space-Partitioning}},
url = {http://www.aaai.org/ocs/index.php/SOCS/SOCS13/paper/view/7225{\&}lt;/ee{\&}gt;},
year = {2013}
}
@article{Casper2013,
abstract = {The ability to cluster data accurately is essential to applications such as image segmentation. Therefore, techniques that enhance accuracy are of keen interest. One such technique involves applying a quantum mechanical system model, such as that of the quantum bit, to generate probabilistic numerical output to be used as variable input for clustering algorithms. This work demonstrates that applying a quantum bit model to data clustering algorithms can increase clustering accuracy, as a result of simulating superposition as well as possessing both guaranteed and controllable convergence properties. For accuracy assessment purposes, four quantum-modeled clustering algorithms for multi-band image segmentation are explored and evaluated. The clustering algorithms of choice consist of quantum variants of K-Means, Fuzzy C-Means, New Weighted Fuzzy C- Means, and the Artificial Bee Colony. Data sets of interest include multi-band imagery, which subsequent to classification are analyzed and assessed for accuracy. Results demonstrate that these algorithms exhibit improved accuracy, when compared to classical counterparts. Moreover, solutions are enhanced via introduction of the quantum state machine, which provides random initial centroid and variable input values to the various clustering algorithms, and quantum operators, which bring about convergence and maximize local search space exploration. Typically, the algorithms have shown to produce better solutions. Keywords:},
author = {Casper, Ellis and Hung, Chih-cheng},
doi = {10.4156/pica.vol2.issue1.1},
file = {:home/chiroptera/Dropbox/mendeley/Casper, Hung - 2013 - Quantum Modeled Clustering Algorithms for Image Segmentation.pdf:pdf},
journal = {Progress in Intelligent Computing and Applications},
keywords = {artificial bee colony,clustering,clustering algorithms,fuzzy c-means,image segmentation,k-means,quantum computing,quantum mechanics,qubit,weighted fuzzy c-means},
mendeley-tags = {artificial bee colony,clustering,fuzzy c-means,image segmentation,k-means,quantum computing,quantum mechanics,qubit,weighted fuzzy c-means},
pages = {1--21},
title = {{Quantum Modeled Clustering Algorithms for Image Segmentation}},
volume = {2},
year = {2013}
}
@article{Fred2005,
author = {Fred, Ana N L and Jain, Anil K},
file = {:home/chiroptera/Dropbox/mendeley/Fred, Jain - 2005 - Combining multiple clusterings using evidence accumulation.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {K-means,cluster fusion,cluster validity,combining clustering partitions,evidence accumulation,mutual information,robust clustering,single-link},
mendeley-tags = {K-means,cluster fusion,cluster validity,combining clustering partitions,evidence accumulation,mutual information,robust clustering,single-link},
number = {6},
pages = {835--850},
title = {{Combining multiple clusterings using evidence accumulation}},
volume = {27},
year = {2005}
}
@article{Topics2010,
author = {Topics, Advanced and Topic, Machine Learning and Scribe, Dimensionality Reduction and Lecturer, Matt Faulkner and Date, Andreas Krause},
file = {:home/chiroptera/Dropbox/mendeley/Topics et al. - 2010 - Dimensionality reduction.pdf:pdf},
pages = {1--6},
title = {{Dimensionality reduction}},
year = {2010}
}
@article{Arefin2012b,
author = {Arefin, Ahmed Shamsul and Riveros, Carlos and Berretta, Regina and Moscato, Pablo},
doi = {10.1109/ICCSE.2012.6295141},
file = {:home/chiroptera/Dropbox/mendeley/Arefin et al. - 2012 - Computing large-scale distance matrices on GPU.pdf:pdf},
isbn = {9781467302425},
journal = {ICCSE 2012 - Proceedings of 2012 7th International Conference on Computer Science and Education},
keywords = {Computer Application},
number = {Iccse},
pages = {576--580},
title = {{Computing large-scale distance matrices on GPU}},
year = {2012}
}
@article{Barnat2011,
abstract = {The problem of decomposing a directed graph into its strongly connected components is a fundamental graph problem inherently present in many scientific and commercial applications. In this paper we show how some of the existing parallel algorithms can be reformulated in order to be accelerated by NVIDIA CUDA technology. In particular, we design a new CUDA-aware procedure for pivot selection and we adapt selected parallel algorithms for CUDA accelerated computation. We also experimentally demonstrate that with a single GTX 480 GPU card we can easily outperform the optimal serial CPU implementation by an order of magnitude in most cases, 40 times on some sufficiently big instances. This is an interesting result as unlike the serial CPU case, the asymptotic complexity of the parallel algorithms is not optimal.},
author = {Barnat, Jiř{\'{\i}} and Bauch, Petr and Brim, Lubo{\v{s}} and {\v{C}}e{\v{s}}ka, Milan},
doi = {10.1109/IPDPS.2011.59},
file = {:home/chiroptera/Dropbox/mendeley/Barnat et al. - 2011 - Computing strongly connected components in parallel on CUDA.pdf:pdf},
isbn = {9780769543857},
issn = {1530-2075},
journal = {Proceedings - 25th IEEE International Parallel and Distributed Processing Symposium, IPDPS 2011},
pages = {544--555},
title = {{Computing strongly connected components in parallel on CUDA}},
year = {2011}
}
@book{wittek2014quantum,
author = {Wittek, Peter},
file = {:home/chiroptera/Dropbox/mendeley/Wittek - 2014 - Quantum Machine Learning What Quantum Computing Means to Data Mining.pdf:pdf},
publisher = {Academic Press},
title = {{Quantum Machine Learning: What Quantum Computing Means to Data Mining}},
year = {2014}
}
@article{Merrill2012,
abstract = {Breadth-first search (BFS) is a core primitive for graph traversal and a basis for many higher-level graph analysis algorithms. It is also representative of a class of parallel computations whose memory accesses and work distribution are both irregular and data-dependent. Recent work has demonstrated the plausibility of GPU sparse graph traversal, but has tended to focus on asymptotically inefficient algorithms that perform poorly on graphs with non-trivial diameter. We present a BFS parallelization focused on fine-grained task management constructed from efficient prefix sum that achieves an asymptotically optimal O(|V|+|E|) work complexity. Our implementation delivers excellent performance on diverse graphs, achieving traversal rates in excess of 3.3 billion and 8.3 billion traversed edges per second using single and quad-GPU configurations, respectively. This level of performance is several times faster than state-of-the-art implementations both CPU and GPU platforms.},
author = {Merrill, Duane and Garland, Michael and Grimshaw, Andrew},
doi = {10.1145/2370036.2145832},
file = {:home/chiroptera/Dropbox/mendeley/Merrill, Garland, Grimshaw - 2012 - Scalable GPU graph traversal.pdf:pdf},
isbn = {9781450311601},
issn = {03621340},
journal = {ACM SIGPLAN Notices},
keywords = {breadth-first search,gpu,graph algorithms,graph traversal,parallel algorithms,prefix sum,sparse graph},
number = {8},
pages = {117},
title = {{Scalable GPU graph traversal}},
volume = {47},
year = {2012}
}
@article{ng2002clarans,
author = {Ng, Raymond T and Han, Jiawei},
file = {:home/chiroptera/Dropbox/mendeley/Ng, Han - 2002 - Clarans A method for clustering objects for spatial data mining.PDF:PDF},
journal = {Knowledge and Data Engineering, IEEE Transactions on},
number = {5},
pages = {1003--1016},
publisher = {IEEE},
title = {{Clarans: A method for clustering objects for spatial data mining}},
volume = {14},
year = {2002}
}
@article{Han2011,
abstract = {Graphics Processing Units (GPUs) have become a competitive accelerator for applications outside the graphics domain, mainly driven by the improvements inGPUprogrammability. Although the Compute Unified Device Architecture (CUDA) is a simple C-like interface for programming NVIDIA GPUs, porting applications to CUDA remains a challenge to average programmers. In particular, CUDA places on the programmer the burden of packaging GPU code in separate functions, of explicitly managing data transfer between the host and GPU memories, and of manually optimizing the utilization of the GPU memory. Practical experience shows that the programmer needs to make significant code changes, often tedious and error-prone, before getting an optimized program. We have designed hiCUDA, a high-level directive-based language for CUDA programming. It allows programmers to perform these tedious tasks in a simpler manner and directly to the sequential code, thus speeding up the porting process. In this paper, we describe the hiCUDA directives as well as the design and implementation of a prototype compiler that translates a hiCUDA program to a CUDA program. Our compiler is able to support real-world applications that span multiple procedures and use dynamically allocated arrays. Experiments using nine CUDA benchmarks show that the simplicity hiCUDA provides comes at no expense to performance.},
author = {Han, Tianyi David and Abdelrahman, Tarek S.},
doi = {10.1109/TPDS.2010.62},
file = {:home/chiroptera/Dropbox/mendeley/Han, Abdelrahman - 2011 - HiCUDA High-level GPGPU programming.pdf:pdf},
isbn = {1045-9219 VO - 22},
issn = {10459219},
journal = {IEEE Transactions on Parallel and Distributed Systems},
keywords = {CUDA,GPGPU,data-parallel programming,directive-based language,source-to-source compiler},
number = {1},
pages = {78--90},
title = {{HiCUDA: High-level GPGPU programming}},
volume = {23},
year = {2011}
}
@article{Fred2009a,
author = {Fred, Ana},
file = {:home/chiroptera/Dropbox/mendeley/Fred - 2009 - Tutorial Pt.2 Clustering Algorithms.pdf:pdf},
number = {April},
pages = {1--42},
title = {{Tutorial Pt.2: Clustering Algorithms}},
year = {2009}
}
@article{Tavallaee2009,
abstract = {During the last decade, anomaly detection has attracted the attention of many researchers to overcome the weakness of signature-based IDSs in detecting novel attacks, and KDDCUP'99 is the mostly widely used data set for the evaluation of these systems. Having conducted a statistical analysis on this data set, we found two important issues which highly affects the performance of evaluated systems, and results in a very poor evaluation of anomaly detection approaches. To solve these issues, we have proposed a new data set, NSL-KDD, which consists of selected records of the complete KDD data set and does not suffer from any of mentioned shortcomings.},
author = {Tavallaee, Mahbod and Bagheri, Ebrahim and Lu, Wei and Ghorbani, Ali a.},
doi = {10.1109/CISDA.2009.5356528},
file = {:home/chiroptera/Dropbox/mendeley/Tavallaee et al. - 2009 - A detailed analysis of the KDD CUP 99 data set.pdf:pdf},
isbn = {9781424437641},
journal = {IEEE Symposium on Computational Intelligence for Security and Defense Applications, CISDA 2009},
number = {Cisda},
pages = {1--6},
title = {{A detailed analysis of the KDD CUP 99 data set}},
year = {2009}
}
@article{Asanovic2006,
abstract = {The recent switch to parallel microprocessors is a milestone in the history of computing. Industry has laid out a roadmap for multicore designs that preserves the programming paradigm of the past via binary compatibility and cache coherence. Conventional wisdom is now to double the number of cores on a chip with each silicon generation. A multidisciplinary group of Berkeley researchers met nearly two years to discuss this change. Our view is that this evolutionary approach to parallel hardware and software may work from 2 or 8 processor systems, but is likely to face diminishing returns as 16 and 32 processor systems are realized, just as returns fell with greater instruction-level parallelism. We believe that much can be learned by examining the success of parallelism at the extremes of the computing spectrum, namely embedded computing and high performance computing. This led us to frame the parallel landscape with seven questions, and to recommend the following: The overarching goal should be to make it easy to write programs that execute efficiently on highly parallel computing systems The target should be 1000s of cores per chip, as these chips are built from processing elements that are the most efficient in MIPS (Million Instructions per Second) per watt, MIPS per area of silicon, and MIPS per development dollar. Instead of traditional benchmarks, use 13 "Dwarfs" to design and evaluate parallel programming models and architectures. (A dwarf is an algorithmic method that captures a pattern of computation and communication.) "Autotuners" should play a larger role than conventional compilers in translating parallel programs. To maximize programmer productivity, future programming models must be more human-centric than the conventional focus on hardware or applications. To be successful, programming models should be independent of the number of processors. To maximize application efficiency, programming models should support a wide range of data types and successful models of parallelism: task-level parallelism, word-level parallelism, and bit-level parallelism. Architects should not include features that significantly affect performance or energy if programmers cannot accurately measure their impact via performance counters and energy counters. Traditional operating systems will be deconstructed and operating system functionality will be orchestrated using libraries and virtual machines. To explore the design space rapidly, use system emulators based on Field Programmable Gate Arrays (FPGAs) that are highly scalable and low cost. Since real world applications are naturally parallel and hardware is naturally parallel, what we need is a programming model, system software, and a supporting architecture that are naturally parallel. Researchers have the rare opportunity to re-invent these cornerstones of computing, provided they simplify the efficient programming of highly parallel systems.},
author = {Asanovic, Krste and Catanzaro, Bryan Christopher and Patterson, David a and Yelick, Katherine a},
doi = {10.1145/1562764.1562783},
file = {:home/chiroptera/Dropbox/mendeley/Asanovic et al. - 2006 - The Landscape of Parallel Computing Research A View from Berkeley.pdf:pdf},
isbn = {UCB/EECS-2006-183},
issn = {00010782},
journal = {EECS Department University of California Berkeley Tech Rep UCBEECS2006183},
pages = {19},
pmid = {8429457},
title = {{The Landscape of Parallel Computing Research : A View from Berkeley}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.67.8705{\&}amp;rep=rep1{\&}amp;type=pdf},
volume = {18},
year = {2006}
}
@article{Mechanics,
author = {Mechanics, Quantum},
file = {:home/chiroptera/Dropbox/mendeley/Mechanics - Unknown - Strange Bedfellows.pdf:pdf},
title = {{Strange Bedfellows:}}
}
@article{Kim2011,
abstract = {Hierarchical clustering is an important and powerful but computationally extensive operation. Its complexity motivates the exploration of highly parallel approaches such as Adaptive Resonance Theory (ART). Although ART has been implemented on GPU processors, this paper presents the first hierarchical ART GPU implementation we are aware of. Each ART layer is distributed in the GPU's multiprocessors and is trained simultaneously. The experimental results show that for deep trees, the GPU's performance advantage is significant.},
author = {Kim, Sejun and Wunsch, Donald C.},
doi = {10.1109/IJCNN.2011.6033584},
file = {:home/chiroptera/Dropbox/mendeley/Kim, Wunsch - 2011 - A GPU based parallel Hierarchical Fuzzy ART clustering.pdf:pdf},
isbn = {9781457710865},
issn = {2161-4393},
journal = {Proceedings of the International Joint Conference on Neural Networks},
keywords = {GPU,clustering},
mendeley-tags = {GPU,clustering},
number = {5},
pages = {2778--2782},
title = {{A GPU based parallel Hierarchical Fuzzy ART clustering}},
year = {2011}
}
@article{Dong2010,
abstract = {Building the quantum clustering model by quantum characteristic. It is proved by the Simulation experiment, that It can deal with exceptional, high-dimension complicated data and large-scale data set.},
author = {Dong, Yumin and Jia, Fanghua},
doi = {10.1109/ISIP.2010.111},
file = {:home/chiroptera/Dropbox/mendeley/Dong, Jia - 2010 - A new-style generalized quantum clustering model.pdf:pdf},
isbn = {9780769542614},
journal = {Proceedings - 3rd International Symposium on Information Processing, ISIP 2010},
keywords = {Clustering arithmetic,Quantum clustering model,Quantum entropy},
pages = {519--522},
title = {{A new-style generalized quantum clustering model}},
year = {2010}
}
@article{chen2011parallel,
author = {Chen, Wen-Yen and Song, Yangqiu and Bai, Hongjie and Lin, Chih-Jen and Chang, Edward Y},
file = {:home/chiroptera/Dropbox/mendeley/Guerra et al. - 2009 - Supporting user-oriented analysis for multi-view domain-specific visual languages.pdf:pdf},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
number = {3},
pages = {568--586},
publisher = {IEEE},
title = {{Parallel spectral clustering in distributed systems}},
volume = {33},
year = {2011}
}
@article{Wu2009a,
abstract = {In this paper, we report our research on using GPUs to accelerate clustering of very large data sets, which are common in today's real world applications. While many published works have shown that GPUs can be used to accelerate various general purpose applications with respectable performance gains, few attempts have been made to tackle very large problems. Our goal here is to investigate if GPUs can be useful accelerators even with very large data sets that cannot fit into GPU’s onboard memory. Using a popular clustering algorithm, K-Means, as an example, our results have been very positive. On a data set with a billion data points, our GPU-accelerated implementation achieved an order of magnitude performance gain over a highly optimized CPU-only version running on 8 cores, and more than two orders of magnitude gain over a popular benchmark, MineBench, running on a single core.},
author = {Wu, Ren and Zhang, Bin and Hsu, Meichun},
doi = {10.1145/1531666.1531668},
file = {:home/chiroptera/Dropbox/mendeley/Wu, Zhang, Hsu - 2009 - Clustering billions of data points using GPUs.pdf:pdf},
isbn = {9781605585574},
journal = {Proceedings of the combined workshops on UnConventional high performance computing workshop plus memory access workshop},
keywords = {accelerator,clustering,data parallelism,data-mining,gpgpu,gpu,graphics processor,many-core,multi-core,parallel algorithm},
mendeley-tags = {clustering,gpu},
pages = {1--5},
title = {{Clustering billions of data points using GPUs}},
url = {http://portal.acm.org/citation.cfm?id=1531666.1531668},
year = {2009}
}
@article{Bezdek1984,
abstract = {This paper transmits a FORTRAN-IV coding of the fuzzy c-means (FCM) clustering program. The FCM program is applicable to a wide variety of geostatistical data analysis problems. This program generates fuzzy partitions and prototypes for any set of numerical data. These partitions are useful for corroborating known substructures or suggesting substructure in unexplored data. The clustering criterion used to aggregate subsets is a generalized least-squares objective function. Features of this program include a choice of three norms (Euclidean, Diagonal, or Mahalonobis), an adjustable weighting factor that essentially controls sensitivity to noise, acceptance of variable numbers of clusters, and outputs that include several measures of cluster validity.},
author = {Bezdek, James C. and Ehrlich, Robert and Full, William},
doi = {10.1016/0098-3004(84)90020-7},
file = {:home/chiroptera/Dropbox/mendeley/Bezdek, Ehrlich, Full - 1984 - FCM The Fuzzy C-Means Clustering Algorithm.pdf:pdf},
isbn = {0098-3004},
issn = {0098-3004},
journal = {Computers {\&} Geosciences},
keywords = {cluster analysis,cluster validity,fuzzy clustering,fuzzy qmodel,least-squared errors},
number = {2–3},
pages = {191--203},
title = {{FCM: The Fuzzy C-Means Clustering Algorithm}},
url = {http://www.sciencedirect.com/science/article/pii/0098300484900207},
volume = {10},
year = {1984}
}
@article{Basu2002,
abstract = {We studied a number of measures that characterize the difficulty of a classification problem, focusing on the geometrical complexity of the class boundary. We compared a set of real-world problems to random labelings of points and found that real problems contain structures in this measurement space that are significantly different from the random sets. Distributions of problems in this space show that there exist at least two independent factors affecting a problem's difficulty. We suggest using this space to describe a classifier's domain of competence. This can guide static and dynamic selection of classifiers for specific problems as well as subproblems formed by confinement, projection, and transformations of the feature vectors.},
author = {Basu, M.},
doi = {10.1109/34.990132},
file = {:home/chiroptera/Dropbox/mendeley/Basu - 2002 - Complexity measures of supervised classification problems.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {3},
pages = {289--300},
title = {{Complexity measures of supervised classification problems}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-0036522441{\&}partnerID=tZOtx3y1},
volume = {24},
year = {2002}
}
@article{Capannini2012,
author = {Capannini, Gabriele and Silvestri, Fabrizio and Baraglia, Ranieri},
doi = {10.1016/j.ipm.2010.11.010},
file = {:home/chiroptera/Dropbox/mendeley/Capannini, Silvestri, Baraglia - 2012 - Sorting on GPUs for large scale datasets A thorough comparison.pdf:pdf},
issn = {03064573},
journal = {Information Processing {\&} Management},
number = {5},
pages = {903--917},
publisher = {Elsevier Ltd},
title = {{Sorting on GPUs for large scale datasets: A thorough comparison}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0306457310001007},
volume = {48},
year = {2012}
}
@article{Li2009,
abstract = {Based on the concepts and principles of quantum computing, a quantum-inspired evolutionary algorithm for data clustering (QECA) is proposed in this paper. And a novel distance measurement index called manifold distance is introduced. These attribute data are the main source of clustering problem, due to its complex distribution, most clustering algorithms available are only suitable for these types of characteristic data. In this study, a new algorithm which can deal with these data with manifold distribution is more effective. The main motives of using QECA consist in searching for appropriate cluster center so that a similarity metric of clusters are optimized more quickly and effectively. The superiority of QECA over fuzzy c-means (FCM) algorithm and immune evolutionary clustering algorithm (IECA) is extensively demonstrated in our experiments.},
author = {Li, Yangyang and Shi, Hongzhu and Gong, Maoguo and Shang, Ronghua},
doi = {10.1145/1543834.1543963},
file = {:home/chiroptera/Dropbox/mendeley/Li et al. - 2009 - Quantum-inspired evolutionary clustering algorithm based on manifold distance.pdf:pdf},
isbn = {9781605583266},
journal = {Proceedings of the first ACM/SIGEVO Summit on Genetic and Evolutionary Computation - GEC '09},
keywords = {algorithm,data clustering,manifold distance,quantum computing,quantum-inspired evolutionary},
pages = {871},
title = {{Quantum-inspired evolutionary clustering algorithm based on manifold distance}},
url = {http://portal.acm.org/citation.cfm?doid=1543834.1543963},
year = {2009}
}
@article{Varshavsky2005,
author = {Varshavsky, Roy and Linial, Michal and Horn, David},
doi = {10.1007/11576259{\_}18},
file = {:home/chiroptera/Dropbox/mendeley/Varshavsky, Linial, Horn - 2005 - COMPACT A Comparative Package for Clustering Assessment.pdf:pdf},
isbn = {3540297707},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {159--167},
title = {{COMPACT: A Comparative Package for Clustering Assessment}},
volume = {3759 LNCS},
year = {2005}
}
@article{Rajaraman2011,
abstract = {At the highest level of description, this book is about data mining. However, it focuses on data mining of very large amounts of data, that is, data so large it does not fit in main memory. Because of the emphasis on size, many of our examples are about the Web or data derived from the Web. Further, the book takes an algorithmic point of view: data mining is about applying algorithms to data, rather than using data to train a machine-learning engine of some sort.},
author = {Rajaraman, Anand and Ullman, Jeffrey D},
doi = {10.1017/CBO9781139058452},
file = {:home/chiroptera/Dropbox/mendeley/Rajaraman, Ullman - 2011 - Mining of Massive Datasets.pdf:pdf},
isbn = {9781139058452},
issn = {01420615},
journal = {Lecture Notes for Stanford CS345A Web Mining},
pages = {328},
title = {{Mining of Massive Datasets}},
url = {http://ebooks.cambridge.org/ref/id/CBO9781139058452},
volume = {67},
year = {2011}
}
@inproceedings{Farivar2008,
abstract = {Abstract - Graphics Processing Units (GPU) have recently been the subject of attention in re- search as an efficient coprocessor for implementing many classes of highly parallel applications. The GPUs design is engineered for graphics applications, where many independent SIMD workloads are simultaneously dispatched to processing elements. While parallelism has been explored in the context of traditional CPU threads and SIMD processing elements, the principles involved in dividing the steps of a parallel algorithm for execution on GPU architectures remains a significant challenge. In this paper, we introduce a first step towards building an efficient GPU-based parallel implemen- tation of a commonly used clustering algorithm called K-Means on an NVIDIA G80 PCI express graphics board using the CUDA processing extensions. Clustering algorithms are important for search, data mining, spam and intrusion detection applications. Modern desktop machines commonly include desktop search software that can be greatly enhanced by these advances, while low-power ma- chines such as laptops can reduce power consump- tion by utilizing the video chip for these clustering and indexing operations. Our preliminary results show over a 13x performance improvement com- pared to a baseline 3 GHz Intel Pentium(R) based PC running the same algorithm with an average spec G80 graphics card, the NVIDIA 8600GT. The low cost of these video cards (less than {\$}100 market price as of 2008), and the high performance gains suggest that our approach is both practical and economical for common applications.},
author = {Farivar, Reza and Rebolledo, Daniel and Chan, Ellick},
booktitle = {The 2008 International Conference on Parallel and Distributed Processing Techniques and Applications},
file = {:home/chiroptera/Dropbox/mendeley/Farivar, Rebolledo, Chan - 2008 - A parallel implementation of k-means clustering on GPUs.pdf:pdf},
isbn = {1601320841},
pages = {340--345},
title = {{A parallel implementation of k-means clustering on GPUs}},
url = {http://nguyendangbinh.org/Proceedings/IPCV08/Papers/PDP3663.pdf},
year = {2008}
}
@article{Lourenco2007,
abstract = {We address the problem of clustering of contour images from hardware tools based on string descriptions, in a comparative study of cluster combination techniques. Several clustering algorithms are addressed using both the hierarchical agglomerative concept and partitional approaches. In the later class of algorithms, we explore: an adaptation of the K-means algorithm to string patterns using the median string as cluster representative; the error-correcting parsing approach by Fu; and the very recent spectral clustering approach. These algorithms are applied using several dissimilarity measures, namely: minimum code length based measures; dissimilarity based on the concept of reduction in grammatical complexity; and error-correcting parsing. In a first instance, clustering algorithms are applied individually to the image data set, and results are evaluated in terms of the error rate, taking as ground truth known labeling of the data. In a second step, we combine multiple data partitions, that we call a clustering ensemble, using three state-of-the-art clustering combination techniques. Results show that combination methods lead in general to better data partitioning, as compared to ground truth information.},
author = {Louren{\c{c}}o, Andr{\'{e}} and Fred, Ana},
doi = {10.1109/ACVMOT.2005.46},
file = {:home/chiroptera/Dropbox/mendeley/Louren{\c{c}}o, Fred - 2007 - Ensemble methods in the clustering of string patterns.pdf:pdf},
isbn = {0769522718},
journal = {Proceedings - Seventh IEEE Workshop on Applications of Computer Vision, WACV 2005},
pages = {143--148},
title = {{Ensemble methods in the clustering of string patterns}},
year = {2007}
}
@inproceedings{Sousa2015,
author = {{Da Silva Sousa}, C and Mariano, A and Proenca, A},
booktitle = {Parallel, Distributed and Network-Based Processing (PDP), 2015 23rd Euromicro International Conference on},
doi = {10.1109/PDP.2015.72},
file = {:home/chiroptera/Dropbox/mendeley//Da Silva Sousa, Mariano, Proenca - 2015 - A Generic and Highly Efficient Parallel Variant of Boruvka's Algorithm.pdf:pdf},
issn = {1066-6192},
keywords = {Algorithm design and analysis,Arrays,Boruvka's Algorithm,Color,GPU,Graphics processing units,Instruction sets,Kernel,boruvka,gpu,microprocessor chips,minimum spanning tree,minimum spanning tree solver,multicore CPU-chips,multiprocessing systems,parallel,performance,road network graphs,trees (mathematics)},
pages = {610--617},
title = {{A Generic and Highly Efficient Parallel Variant of Boruvka's Algorithm}},
url = {https://github.com/Beatgodes/BoruvkaUMinho},
year = {2015}
}
@article{hopcroft1973n,
author = {Hopcroft, John E and Karp, Richard M},
file = {:home/chiroptera/Dropbox/mendeley/Hopcroft, Karp - 1973 - An n52 algorithm for maximum matchings in bipartite graphs.pdf:pdf},
journal = {SIAM Journal on computing},
number = {4},
pages = {225--231},
publisher = {SIAM},
title = {{An n{\^{}}5/2 algorithm for maximum matchings in bipartite graphs}},
volume = {2},
year = {1973}
}
@article{Harris2007,
abstract = {Parallel prefix sum, also known as parallel Scan, is a useful building block for many parallel algorithms including sorting and building data structures. In this document we introduce Scan and describe step-by-step how it can be implemented efficiently in NVIDIA CUDA. We start with a basic na{\"{\i}}ve algorithm and proceed through more advanced techniques to obtain best performance. We then explain how to scan arrays of arbitrary size that cannot be processed with a single block of threads.},
author = {Harris, Mark and Sengupta, Shubhabrata and Owens, John D.},
file = {:home/chiroptera/Dropbox/mendeley/Harris, Sengupta, Owens - 2007 - Parallel Prefix Sum (Scan) with CUDA Mark.pdf:pdf},
isbn = {9780321515261},
journal = {Gpu gems 3},
number = {April},
pages = {1--24},
title = {{Parallel Prefix Sum (Scan) with CUDA Mark}},
url = {http://dl.acm.org/citation.cfm?id=1407436},
year = {2007}
}
@article{Harvard2005,
author = {Harvard},
file = {:home/chiroptera/Dropbox/mendeley/Harvard - 2005 - The Assignment Problem and the Hungarian Method.pdf:pdf},
journal = {Introduction to Linear Algebra and Multivariable Calculus},
title = {{The Assignment Problem and the Hungarian Method}},
year = {2005}
}
@article{Chavent1998,
author = {Chavent, Marie},
doi = {10.1016/S0167-8655(98)00087-7},
file = {:home/chiroptera/Dropbox/mendeley/Chavent - 1998 - A monothetic clustering method.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {hierarchical clustering methods,inertia criterion,monothetic cluster},
number = {11},
pages = {989--996},
title = {{A monothetic clustering method}},
volume = {19},
year = {1998}
}
@inproceedings{mccallum2000efficient,
author = {McCallum, Andrew and Nigam, Kamal and Ungar, Lyle H},
booktitle = {Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining},
file = {:home/chiroptera/Dropbox/mendeley/McCallum, Nigam, Ungar - 2000 - Efficient clustering of high-dimensional data sets with application to reference matching.pdf:pdf},
organization = {ACM},
pages = {169--178},
title = {{Efficient clustering of high-dimensional data sets with application to reference matching}},
year = {2000}
}
@article{Hursky,
author = {Hursky, Paul and Porter, Michael B},
file = {:home/chiroptera/Dropbox/mendeley/Hursky, Porter - Unknown - Accelerating underwater acoustic propagation modeling using general purpose graphic processing units.pdf:pdf},
keywords = {Split-step Fourier parabolic equation, high-perfor,general purpose graphic processing,high-,performance computing,split-step fourier parabolic equation,unit},
title = {{Accelerating underwater acoustic propagation modeling using general purpose graphic processing units}}
}
@article{Peters2010,
abstract = {Sorting is a well-investigated topic in Computer Science in general and by now many efficient sorting algorithms for CPUs and GPUs have been developed. There is no swapping, paging, etc. available on GPUs to provide more virtual memory than physically available, thus if one wants to sort sequences that exceed GPU memory using the GPU the problem of external sorting arises. In this contribution we present a novel merge-based external sorting algorithm for one or more CUDA-enabled GPUs. We reduce the performance impact of memory transfers to and from the GPU by using an approach similar to regular samplesort and by overlapping memory transfers with GPU computation. We achieve a good utilization of GPUs and load balancing among them by carefully choosing the samples and the amount of GPU memory used for computation. We demonstrate the performance of our algorithm by extended testing. Using two GTX280 the implementation outperforms the fastest CPU sorting algorithms known to the authors.},
author = {Peters, Hagen and Schulz-Hildebrandt, Ole and Luttenberger, Norbert},
doi = {10.1109/IPDPSW.2010.5470833},
file = {:home/chiroptera/Dropbox/mendeley/Peters, Schulz-Hildebrandt, Luttenberger - 2010 - Parallel external sorting for CUDA-enabled GPUs with load balancing and low transfer o.pdf:pdf},
isbn = {9781424465347},
journal = {Proceedings of the 2010 IEEE International Symposium on Parallel and Distributed Processing, Workshops and Phd Forum, IPDPSW 2010},
title = {{Parallel external sorting for CUDA-enabled GPUs with load balancing and low transfer overhead}},
url = {https://comsys.informatik.uni-kiel.de/wp-content/uploads/2012/02/10-Peters-Parallel{\_}external{\_}sorting.pdf},
year = {2010}
}
@article{Horn2003,
abstract = {MOTIVATION: This paper introduces the application of a novel clustering method to microarray expression data. Its first stage involves compression of dimensions that can be achieved by applying SVD to the gene-sample matrix in microarray problems. Thus the data (samples or genes) can be represented by vectors in a truncated space of low dimensionality, 4 and 5 in the examples studied here. We find it preferable to project all vectors onto the unit sphere before applying a clustering algorithm. The clustering algorithm used here is the quantum clustering method that has one free scale parameter. Although the method is not hierarchical, it can be modified to allow hierarchy in terms of this scale parameter. RESULTS: We apply our method to three data sets. The results are very promising. On cancer cell data we obtain a dendrogram that reflects correct groupings of cells. In an AML/ALL data set we obtain very good clustering of samples into four classes of the data. Finally, in clustering of genes in yeast cell cycle data we obtain four groups in a problem that is estimated to contain five families. AVAILABILITY: Software is available as Matlab programs at http://neuron.tau.ac.il/{\~{}}horn/QC.htm.},
author = {Horn, David and Axel, Inon},
doi = {10.1093/bioinformatics/btg053},
file = {:home/chiroptera/Dropbox/mendeley/Horn, Axel - 2003 - Novel clustering algorithm for microarray expression data in a truncated SVD space.ps:ps;:home/chiroptera/Dropbox/mendeley/Horn, Axel - 2003 - Novel clustering algorithm for microarray expression data in a truncated SVD space.pdf:pdf},
issn = {13674803},
journal = {Bioinformatics},
pages = {1110--1115},
pmid = {12801871},
title = {{Novel clustering algorithm for microarray expression data in a truncated SVD space}},
volume = {19},
year = {2003}
}
@article{Karimi2010,
abstract = {CUDA and OpenCL are two different frameworks for GPU programming. OpenCL is an open standard that can be used to program CPUs, GPUs, and other devices from different vendors, while CUDA is specific to NVIDIA GPUs. Although OpenCL promises a portable language for GPU programming, its generality may entail a performance penalty. In this paper, we use complex, near-identical kernels from a Quantum Monte Carlo application to compare the performance of CUDA and OpenCL. We show that when using NVIDIA compiler tools, converting a CUDA kernel to an OpenCL kernel involves minimal modifications. Making such a kernel compile with ATI's build tools involves more modifications. Our performance tests measure and compare data transfer times to and from the GPU, kernel execution times, and end-to-end application execution times for both CUDA and OpenCL.},
archivePrefix = {arXiv},
arxivId = {1005.2581},
author = {Karimi, Kamran and Dickson, Neil G. and Hamze, Firas},
doi = {10.1109/ICPP.2011.45},
eprint = {1005.2581},
file = {:home/chiroptera/Dropbox/mendeley/Karimi, Dickson, Hamze - 2010 - A Performance Comparison of CUDA and OpenCL.pdf:pdf},
isbn = {978-1-4577-1336-1},
journal = {arXiv preprint arXiv:1005.2581},
keywords = {comparison,cuda,opencl},
mendeley-tags = {comparison,cuda,opencl},
title = {{A Performance Comparison of CUDA and OpenCL}},
url = {http://arxiv.org/abs/1005.2581},
year = {2010}
}
@article{Arefin2012a,
abstract = {BACKGROUND: The analysis of biological networks has become a major challenge due to the recent development of high-throughput techniques that are rapidly producing very large data sets. The exploding volumes of biological data are craving for extreme computational power and special computing facilities (i.e. super-computers). An inexpensive solution, such as General Purpose computation based on Graphics Processing Units (GPGPU), can be adapted to tackle this challenge, but the limitation of the device internal memory can pose a new problem of scalability. An efficient data and computational parallelism with partitioning is required to provide a fast and scalable solution to this problem.$\backslash$n$\backslash$nRESULTS: We propose an efficient parallel formulation of the k-Nearest Neighbour (kNN) search problem, which is a popular method for classifying objects in several fields of research, such as pattern recognition, machine learning and bioinformatics. Being very simple and straightforward, the performance of the kNN search degrades dramatically for large data sets, since the task is computationally intensive. The proposed approach is not only fast but also scalable to large-scale instances. Based on our approach, we implemented a software tool GPU-FS-kNN (GPU-based Fast and Scalable k-Nearest Neighbour) for CUDA enabled GPUs. The basic approach is simple and adaptable to other available GPU architectures. We observed speed-ups of 50-60 times compared with CPU implementation on a well-known breast microarray study and its associated data sets.$\backslash$n$\backslash$nCONCLUSION: Our GPU-based Fast and Scalable k-Nearest Neighbour search technique (GPU-FS-kNN) provides a significant performance improvement for nearest neighbour computation in large-scale networks. Source code and the software tool is available under GNU Public License (GPL) at https://sourceforge.net/p/gpufsknn/.},
author = {Arefin, Ahmed Shamsul and Riveros, Carlos and Berretta, Regina and Moscato, Pablo},
doi = {10.1371/journal.pone.0044000},
file = {:home/chiroptera/Dropbox/mendeley/Arefin et al. - 2012 - GPU-FS-kNN A Software Tool for Fast and Scalable kNN Computation Using GPUs.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {8},
pmid = {22937144},
title = {{GPU-FS-kNN: A Software Tool for Fast and Scalable kNN Computation Using GPUs}},
volume = {7},
year = {2012}
}
@article{Blekas2009,
author = {Blekas, Konstantinos and Christodoulidou, K and Lagaris, I E},
file = {:home/chiroptera/Dropbox/mendeley/Blekas, Christodoulidou, Lagaris - 2009 - LNCS 5769 - Newtonian Spectral Clustering.pdf:pdf},
pages = {145--154},
title = {{LNCS 5769 - Newtonian Spectral Clustering}},
year = {2009}
}
@inproceedings{Vineet2009,
author = {Vineet, Vibhav and Harish, Pawan and Patidar, Suryakant and Narayanan, P J},
booktitle = {Proceedings of the Conference on High Performance Graphics 2009},
file = {:home/chiroptera/Dropbox/mendeley//Vineet et al. - 2009 - Fast minimum spanning tree for large graphs on the GPU.pdf:pdf},
organization = {ACM},
pages = {167--171},
title = {{Fast minimum spanning tree for large graphs on the GPU}},
year = {2009}
}
@article{lourencco2013probabilistic,
author = {Louren{\c{c}}o, Andr{\'{e}} and Bul{\`{o}}, Samuel Rota and Rebagliati, Nicola and Fred, Ana L N and Figueiredo, M{\'{a}}rio A T and Pelillo, Marcello},
journal = {Machine Learning},
number = {1-2},
pages = {331--357},
publisher = {Springer},
title = {{Probabilistic consensus clustering using evidence accumulation}},
volume = {98},
year = {2013}
}
@incollection{lourencco2013consensus,
author = {Louren{\c{c}}o, Andr{\'{e}} and Bul{\`{o}}, Samuel Rota and Rebagliati, Nicola and Fred, Ana and Figueiredo, M{\'{a}}rio and Pelillo, Marcello},
booktitle = {Pattern Recognition and Image Analysis},
pages = {69--78},
publisher = {Springer},
title = {{Consensus clustering using partial evidence accumulation}},
year = {2013}
}
@article{Basu2006,
author = {{Tin Kam Ho}},
file = {:home/chiroptera/Dropbox/mendeley/Tin Kam Ho - 2006 - Data complexity in pattern recognition.pdf:pdf},
isbn = {9781846281716},
journal = {Acadmedia.Wku.Edu},
pmid = {14148744},
title = {{Data complexity in pattern recognition}},
url = {http://acadmedia.wku.edu/Zhuhadar/eBooks/1848002858-GraphBase.pdf$\backslash$nhttp://books.google.com/books?hl=en{\&}lr={\&}id=GflBKbzym9oC{\&}oi=fnd{\&}pg=PR11{\&}dq=Data+Complexity+in+Pattern+Recognition{\&}ots=igdE1K1o9a{\&}sig=FYTnt8xDnFsb8AzTX8-FZhPSS6U},
year = {2006}
}
@article{Solan2005,
abstract = {We address the problem, fundamental to linguistics, bioinformatics, and certain other disciplines, of using corpora of raw symbolic sequential data to infer underlying rules that govern their production. Given a corpus of strings (such as text, transcribed speech, chromosome or protein sequence data, sheet music, etc.), our unsupervised algorithm recursively distills from it hierarchically structured patterns. The adios (automatic distillation of structure) algorithm relies on a statistical method for pattern extraction and on structured generalization, two processes that have been implicated in language acquisition. It has been evaluated on artificial context-free grammars with thousands of rules, on natural languages as diverse as English and Chinese, and on protein data correlating sequence with function. This unsupervised algorithm is capable of learning complex syntax, generating grammatical novel sentences, and proving useful in other fields that call for structure discovery from raw data, such as bioinformatics.},
author = {Solan, Zach and Horn, David and Ruppin, Eytan and Edelman, Shimon},
doi = {10.1073/pnas.0409746102},
file = {:home/chiroptera/Dropbox/mendeley/Solan et al. - 2005 - Unsupervised learning of natural languages.pdf:pdf},
isbn = {0027-8424},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {33},
pages = {11629--11634},
pmid = {16087885},
title = {{Unsupervised learning of natural languages.}},
volume = {102},
year = {2005}
}
@article{Wang2002,
abstract = {Clustering is the process of grouping a set of objects into classes of similar objects. Although definitions of similarity vary from one clustering model to another, in most of these models the concept of similarity is based on distances, e.g., Euclidean distance or cosine distance. In other words, similar objects are required to have close values on at least a set of dimensions. In this paper, we explore a more general type of similarity. Under the pCluster model we proposed, two objects are similar if they exhibit a coherent pattern on a subset of dimensions. For instance, in DNA microarray analysis, the expression levels of two genes may rise and fall synchronously in response to a set of environmental stimuli. Although the magnitude of their expression levels may not be close, the patterns they exhibit can be very much alike. Discovery of such clusters of genes is essential in revealing significant connections in gene regulatory networks. E-commerce applications, such as collaborative filtering, can also benefit from the new model, which captures not only the closeness of values of certain leading indicators but also the closeness of (purchasing, browsing, etc.) patterns exhibited by the customers. Our paper introduces an effective algorithm to detect such clusters, and we perform tests on several real and synthetic data sets to show its effectiveness.},
author = {Wang, H and Wang, H and Wang, W and Wang, W and Yang, H and Yang, H and Yu, P S and Yu, P S},
doi = {10.1145/564691.564737},
file = {:home/chiroptera/Dropbox/mendeley/Wang et al. - 2002 - Clustering by pattern similarity in large data sets.pdf:pdf},
isbn = {1581134975},
issn = {07308078},
journal = {2002 ACM SIGMOD international conference on Management of Data},
pages = {394},
title = {{Clustering by pattern similarity in large data sets}},
url = {http://portal.acm.org/citation.cfm?doid=564691.564737},
volume = {2},
year = {2002}
}
@article{Cs2003,
author = {Cs, Sael Lee and Biology, Computational},
file = {:home/chiroptera/Dropbox/mendeley/Cs, Biology - 2003 - Lecture 16 pca and svd.pdf:pdf},
title = {{Lecture 16: pca and svd}},
year = {2003}
}
@article{Bustamam2010,
abstract = {The massively parallel computing using graphical processing unit (GPU), which based on tens of thousands of parallel threats within hundreds of GPU's streaming processors, has gained broad popularity and attracted researchers in a wide range of application areas from finance, computer aided engineering, computational fluid dynamics, game physics, numerics, science, medical imaging, life science, and so on, including molecular biology and bioinformatics. Meanwhile, Markov clustering algorithm (MCL) has become one of the most effective and highly cited methods to detect and analyze the communities/clusters within an interaction network dataset on many real world problems such us social, technological, or biological networks including protein-protein interaction networks. However, as the dataset become bigger and bigger, the computation time of MCL algorithm become slower and slower. Hence, GPU computing is an interesting and challenging alternative to attempt to improve the MCL performance. In this poster paper we introduce our improvement of MCL performance based on ELLPACK-R sparse dataset format using GPU computing with the Compute Unified Device Architecture tool (CUDA) from NVIDIA (called CUDA-MCL). As the results show the significant improvement in CUDA-MCL performance and with the low-cost and widely available GPU devices in the market today, this CUDA-MCL implementation is allowing large-scale parallel computation on off-the-shelf desktop machines. Moreover the GPU computing approaches potentially may contribute to significantly change the way bioinformaticians and biologists compute and interact with their data.},
author = {Bustamam, Alhadi and Burrage, Kevin and Hamilton, Nicholas a.},
doi = {10.1109/ACT.2010.10},
file = {:home/chiroptera/Dropbox/mendeley/Bustamam, Burrage, Hamilton - 2010 - A GPU implementation of fast parallel Markov clustering in bioinformatics using ELLPACK-R sparse da.pdf:pdf},
isbn = {9780769542690},
journal = {Proceedings - 2010 2nd International Conference on Advances in Computing, Control and Telecommunication Technologies, ACT 2010},
pages = {173--175},
title = {{A GPU implementation of fast parallel Markov clustering in bioinformatics using ELLPACK-R sparse data format}},
year = {2010}
}
@article{Bai2009,
abstract = {K-means algorithm is one of the most famous unsupervised clustering algorithms. Many theoretical improvements for the performance of original algorithms have been put forward, while almost all of them are based on single instruction single data (SISD) architecture processors (GPUs), which partly ignored the inherent paralleled characteristic of the algorithms. In this paper, a novel single instruction multiple data (SIMD) architecture processors (GPUs) based k-means algorithm is proposed. In this algorithm, in order to accelerate compute-intensive portions of traditional k-means, both data objects assignment and k-centroids recalculation are offloaded to the GPU in parallel. We have implemented this GPU-based k-means on the newest generation GPU with compute unified device architecture(CUDA). The numerical experiments demonstrated that the speed of GPU-based k-means could reach as high as 40 times of the CPU-based k-means.},
annote = {Doesn't say which tools were used (C, Matlab, Fortran???).},
author = {Bai, Hong Tao and He, Li Li and Ouyang, Dan Tong and Li, Zhan Shan and Li, He},
doi = {10.1109/CSIE.2009.491},
file = {:home/chiroptera/Dropbox/mendeley/Bai et al. - 2009 - K-means on commodity GPUs with CUDA.pdf:pdf},
isbn = {9780769535074},
journal = {2009 WRI World Congress on Computer Science and Information Engineering, CSIE 2009},
pages = {651--655},
title = {{K-means on commodity GPUs with CUDA}},
volume = {3},
year = {2009}
}
@article{harish2009large,
author = {Harish, Pawan and Vineet, Vibhav and Narayanan, P J},
journal = {International Institute of Information Technology Hyderabad, Tech. Rep. IIIT/TR/2009/74},
title = {{Large graph algorithms for massively multithreaded architectures}},
year = {2009}
}
@article{Sibson1973,
abstract = {The SLINK algorithm carries out single-link (nearest-neighbour) cluster analysis on an arbitrary dissimilarity coefficient and provides a representation of the resultant dendrogram which can readily be converted into the usual tree-diagram. The algorithm achieves the theoretical order-of-magnitude bounds for both compactness of storage and speed of operation, and makes the application of the single-link method feasible for a number of OTU's well into the range 10{\^{}}3 to 10{\^{}}4. The algorithm is easily programmable in a variety of languages including FORTRAN.},
author = {Sibson, R.},
doi = {10.1093/comjnl/16.1.30},
file = {:home/chiroptera/Dropbox/mendeley/Sibson - 1973 - SLINK an optimally efficient algorithm for the single-link cluster method.pdf:pdf},
issn = {1460-2067},
journal = {The Computer Journal},
number = {1},
pages = {30--34},
title = {{SLINK: an optimally efficient algorithm for the single-link cluster method}},
url = {http://comjnl.oxfordjournals.org/content/16/1/30.short},
volume = {16},
year = {1973}
}
@article{Millman2011,
abstract = {Python has arguably become the de facto standard for exploratory, interactive, and computation-driven scientific research. This issue discusses Python's advantages for scientific research and presents several of the core Python libraries and tools used in scientific research.},
author = {Millman, K. Jarrod and Aivazis, Michael},
doi = {10.1109/MCSE.2011.36},
file = {:home/chiroptera/Dropbox/mendeley/Millman, Aivazis - 2011 - Python for scientists and engineers.pdf:pdf},
isbn = {1521-9615 VO  - 13},
issn = {15219615},
journal = {Computing in Science and Engineering},
keywords = {Programming languages,Python,Python libraries,Python tools,Scientific computing,interactive research},
number = {2},
pages = {9--12},
title = {{Python for scientists and engineers}},
volume = {13},
year = {2011}
}
@article{Revised2006a,
author = {Revised, Last},
file = {:home/chiroptera/Dropbox/mendeley/Revised - 2006 - Bipartite Matching {\&} the Hungarian Method.pdf:pdf},
title = {{Bipartite Matching {\&} the Hungarian Method}},
url = {http://www.cse.ust.hk/{~}golin/COMP572/Notes/Matching.pdf},
year = {2006}
}
@inproceedings{DiMarco2013,
annote = {K-Means with Dynamic Parallelism is a bit slower.},
author = {DiMarco, Jeffrey and Taufer, Michela},
booktitle = {Proc. SPIE 8752, Modeling and Simulation for Defense Systems and Applications VIII},
doi = {10.1117/12.2018069},
file = {:home/chiroptera/Dropbox/mendeley/DiMarco, Taufer - 2013 - Performance impact of dynamic parallelism on different clustering algorithms(3).pdf:pdf},
isbn = {9780819495433},
issn = {0277786X},
keywords = {0,cuda,cuda 5,divisive hierarchical clustering,k-means},
mendeley-tags = {cuda,k-means},
pages = {87520E----87520E},
title = {{Performance impact of dynamic parallelism on different clustering algorithms}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2018069},
year = {2013}
}
@article{Gupta2014,
author = {Gupta, Siddharth and Palsetia, Diana and Patwary, Mostofa Ali and Agrawal, Ankit and Choudhary, Alok},
doi = {10.1109/IPDPSW.2014.152},
file = {:home/chiroptera/Dropbox/mendeley/Gupta et al. - 2014 - A New Parallel Algorithm for Two-Pass Connected Component Labeling.pdf:pdf},
isbn = {9781479941162},
title = {{A New Parallel Algorithm for Two-Pass Connected Component Labeling}},
year = {2014}
}
@article{Zou2014a,
abstract = {Bioinformatics is challenged by the fact that traditional analysis tools have difficulty in processing large-scale data from high-throughput sequencing. The open source Apache Hadoop project, which adopts the MapReduce framework and a distributed file system, has recently given bioinformatics researchers an opportunity to achieve scalable, efficient and reliable computing performance on Linux clusters and on cloud computing services. In this article, we present MapReduce frame-based applications that can be employed in the next-generation sequencing and other biological domains. In addition, we discuss the challenges faced by this field as well as the future works on parallel computing in bioinformatics.},
author = {Zou, Quan and Li, Xu Bin and Jiang, Wen Rui and Lin, Zi Yu and Li, Gui Lin and Chen, Ke},
doi = {10.1093/bib/bbs088},
file = {:home/chiroptera/Dropbox/mendeley//Zou et al. - 2014 - Survey of MapReduce frame operation in bioinformatics.pdf:pdf},
isbn = {1477-4054 (Electronic)$\backslash$n1467-5463 (Linking)},
issn = {14774054},
journal = {Briefings in Bioinformatics},
keywords = {Bioinformatics,Hadoop,MapReduce,bioinformatics},
mendeley-tags = {MapReduce,bioinformatics},
number = {4},
pages = {637--647},
pmid = {23396756},
title = {{Survey of MapReduce frame operation in bioinformatics}},
volume = {15},
year = {2014}
}
@article{Cui2011,
abstract = {Analyzing and clustering large scale data set is a complex problem. One explored method of solving this problem borrows from nature, imitating the flocking behavior of birds. One limitation of this method of data clustering is its complexity {\$}O(n{\^{}}2){\$}. As the number of data and feature dimensions grows, it becomes increasingly difficult to generate results in a reasonable amount of time. In the last few years, the graphics processing unit (GPU) has received attention for its ability to solve highly-parallel and semi-parallel problems much faster than the traditional sequential processor. In this chapter, we have conducted research to exploit this architecture and apply its strengths to the flocking based data clustering problem. Using the CUDA platform from NVIDIA, we developed a Multiple Species Data Flocking implementation to be run on the NVIDIA GPU. Performance gains ranged from {\$}30{\$} to {\$}60{\$} times improvement of the GPU over the CPU implementation.},
author = {Cui, Xiaohui and Charles, Jesse St. and Potok, Thomas E.},
doi = {10.1109/CyberC.2011.44},
file = {:home/chiroptera/Dropbox/mendeley/Cui, Charles, Potok - 2011 - The GPU Enhanced Parallel Computing for Large Scale Data Clustering.pdf:pdf},
isbn = {978-0-7695-4557-8},
journal = {2011 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery},
keywords = {GPU,clustering,flocking,large scale},
pages = {220--225},
title = {{The GPU Enhanced Parallel Computing for Large Scale Data Clustering}},
year = {2011}
}
@article{Shuai2006,
abstract = {This paper presents a generalized quantum particle model to greatly quicken and improve data clustering. The proposed model uses the random dynamics and quantum entanglement of quantum particles on a particle array. In comparison with classical nonquantum methods, the quantum particle model not only clusters much faster, but also has better clustering quality for multi-shape multi-distribution high-dimensional large-scale data sets with noise. The simulations and comparisons show the effectiveness of the quantum particle model},
author = {Shuai, Dianxun and Lu, Cunpai and Zhang, Bin},
doi = {10.1109/COMPSAC.2006.131},
file = {:home/chiroptera/Dropbox/mendeley/Shuai, Lu, Zhang - 2006 - Entanglement partitioning of quantum particles for data clustering.pdf:pdf},
isbn = {0769526551},
issn = {07303157},
journal = {Proceedings - International Computer Software and Applications Conference},
number = {2},
pages = {285--290},
title = {{Entanglement partitioning of quantum particles for data clustering}},
volume = {2},
year = {2006}
}
@article{Wu2009,
abstract = {In this paper, we report our research on using GPUs as accelerators for Business Intelligence(BI) analytics. We are particularly interested in analytics on very large data sets, which are common in today's real world BI applications. While many published works have shown that GPUs can be used to accelerate various general purpose applications with respectable performance gains, few attempts have been made to tackle very large problems. Our goal here is to investigate if the GPUs can be useful accelerators for BI analytics with very large data sets that cannot fit into GPUs onboard memory. Using a popular clustering algorithm, K-Means, as an example, our results have been very positive. For data sets smaller than GPU's onboard memory, the GPU-accelerated version is 6-12x faster than our highly optimized CPU-only version running on an 8-core workstation, or 200-400x faster than the popular benchmark program, MineBench, running on a single core. This is also 2-4x faster than the best reported work. For large data sets which cannot fit in GPU's memory, we further show that with a design which allows the computation on both CPU and GPU, as well as data transfers between them, to proceed in parallel, the GPU-accelerated version can still offer a dramatic performance boost. For example, for a data set with 100 million 2-d data points and 2,000 clusters, the GPU-accelerated version took about 6 minutes, while the CPU-only version running on an 8-core workstation took about 58 minutes. Compared to other approaches, GPU-accelerated implementations of analytics potentially provide better raw performance, better cost-performance ratios, and better energy performance ratios.},
author = {Wu, Ren and Zhang, Bin and Hsu, Meichun},
file = {:home/chiroptera/Dropbox/mendeley/Wu, Zhang, Hsu - 2009 - GPU-Accelerated Large Scale Analytics.pdf:pdf},
isbn = {HPL-2009-38},
journal = {Development},
keywords = {algorithm,big data,clustering,data mining,data mining clustering parallel algorithm gpu gpgp,gpgpu,gpu,k means,many core,multi core,parallel,s},
mendeley-tags = {big data,gpu},
number = {HPL-2009-38},
pages = {10},
title = {{GPU-Accelerated Large Scale Analytics}},
url = {http://www.hpl.hp.com/techreports/2009/HPL-2009-38.pdf},
year = {2009}
}
@inproceedings{Horn2001a,
abstract = {We propose a novel clusteringmethod that is an extension of ideas inher- ent to scale-space clustering and support-vector clustering. Like the lat- ter, it associates every data point with a vector in Hilbert space, and like the former it puts emphasis on their total sum, that is equal to the scale- space probability function. The novelty of our approach is the study of an operator in Hilbert space, represented by the Schr¨ odinger equation of which the probability function is a solution. This Schr¨ odinger equation contains a potential function that can be derived analytically from the probability function. We associate minima of the potential with cluster centers. Themethod has one variable parameter, the scale of its Gaussian kernel. We demonstrate its applicability on known data sets. By limiting the evaluation of the Schr¨ odinger potential to the locations of data points, we can apply this method to problems in high dimensions. 1},
author = {Horn, David and Gottlieb, Assaf},
booktitle = {NIPS},
file = {:home/chiroptera/Dropbox/mendeley/Horn, Gottlieb - 2001 - The Method of Quantum Clustering.pdf:pdf},
pages = {769----776},
title = {{The Method of Quantum Clustering}},
url = {http://www-2.cs.cmu.edu/Groups/NIPS/NIPS2001/papers/psgz/AA08.ps.gz},
year = {2001}
}
@misc{AnilK.,
author = {Jain, Anil K. and Dubes, Richard C.},
file = {:home/chiroptera/Dropbox/mendeley/Jain, Dubes - Unknown - Algorithms for Clustering Data.pdf:pdf},
isbn = {0-13-022278-X},
title = {{Algorithms for Clustering Data}}
}
@article{Dudoit2003,
abstract = {MOTIVATION: The microarray technology is increasingly being applied in biological and medical research to address a wide range of problems such as the classification of tumors. An important statistical question associated with tumor classification is the identification of new tumor classes using gene expression profiles. Essential aspects of this clustering problem include identifying accurate partitions of the tumor samples into clusters and assessing the confidence of cluster assignments for individual samples. RESULTS: Two new resampling methods, inspired from bagging in prediction, are proposed to improve and assess the accuracy of a given clustering procedure. In these ensemble methods, a partitioning clustering procedure is applied to bootstrap learning sets and the resulting multiple partitions are combined by voting or the creation of a new dissimilarity matrix. As in prediction, the motivation behind bagging is to reduce variability in the partitioning results via averaging. The performances of the new and existing methods were compared using simulated data and gene expression data from two recently published cancer microarray studies. The bagged clustering procedures were in general at least as accurate and often substantially more accurate than a single application of the partitioning clustering procedure. A valuable by-product of bagged clustering are the cluster votes which can be used to assess the confidence of cluster assignments for individual observations. SUPPLEMENTARY INFORMATION: For supplementary information on datasets, analyses, and software, consult http://www.stat.berkeley.edu/{\~{}}sandrine and http://www.bioconductor.org.},
author = {Dudoit, Sandrine and Fridlyand, Jane},
doi = {10.1093/bioinformatics/btg038},
file = {:home/chiroptera/Dropbox/mendeley/Dudoit, Fridlyand - 2003 - Bagging to improve the accuracy of a clustering procedure.pdf:pdf},
isbn = {1367-4803 (Print)$\backslash$r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {9},
pages = {1090--1099},
pmid = {12801869},
title = {{Bagging to improve the accuracy of a clustering procedure}},
volume = {19},
year = {2003}
}
@article{baldi2014searching,
annote = {paper for data set 

http://archive.ics.uci.edu/ml/datasets/HIGGS},
author = {Baldi, Pierre and Sadowski, Peter and Whiteson, Daniel},
journal = {Nature communications},
publisher = {Nature Publishing Group},
title = {{Searching for exotic particles in high-energy physics with deep learning}},
url = {http://archive.ics.uci.edu/ml/datasets/HIGGS},
volume = {5},
year = {2014}
}
@article{Varshavsky2007a,
abstract = {Motivation: Feature selection methods aim to reduce the complexity of data and to uncover the most relevant biological variables. In reality, information in biological datasets is often incomplete as a result of untrustworthy samples and missing values. The reliability of selection methods may therefore be questioned.  Method: Information loss is incorporated into a perturbation scheme, testing which features are stable under it. This method is applied to data analysis by unsupervised feature filtering (UFF). The latter has been shown to be a very successful method in analysis of gene-expression data.  Results: We find that the UFF quality degrades smoothly with information loss. It remains successful even under substantial damage. Our method allows for selection of a best imputation method on a dataset treated by UFF. More importantly, scoring features according to their stability under information loss is shown to be correlated with biological importance in cancer studies. This scoring may lead to novel biological insights.  Contact: royke@cs.huji.ac.il  Supplementary information and code availability: Supplementary data are available at Bioinformatics online. 10.1093/bioinformatics/btm528},
author = {Varshavsky, Roy and Gottlieb, Assaf and Horn, David and Linial, Michal},
doi = {10.1093/bioinformatics/btm528},
file = {:home/chiroptera/Dropbox/mendeley/Varshavsky et al. - 2007 - Unsupervised feature selection under perturbations Meeting the challenges of biological data.pdf:pdf},
issn = {13674803},
journal = {Bioinformatics},
number = {24},
pages = {3343--3349},
title = {{Unsupervised feature selection under perturbations: Meeting the challenges of biological data}},
volume = {23},
year = {2007}
}
@inproceedings{Sirotkovi2012,
abstract = {Image segmentation can be computationally demanding, and therefore require powerful hardware in order to meet performance requirements. Recent rapid increase in the performance of graphic processing unit (GPU) hardware, coupled with simplified programming methods, have made GPU an efficient coprocessor for executing variety of highly parallel applications. This paper presents an implementation of k-means image segmentation on the GPU platform with Compute Unified Device Architecture (CUDA). Parallel k-means segmentation is realized in hybrid manner i.e. proposed approach distributes computation load between Central Processing Unit (CPU) and GPU. The emphasis is placed on adaptation of the core algorithm to efficiently process datasets characteristic for image segmentation while exploiting benefits of underlying GPU hardware architecture. Numerical experiments have demonstrated considerably faster segmentation execution with proposed approach comparing to classical CPU-based approach.},
author = {Sirotkovi, J and Dujmi, H and Papi, V},
booktitle = {MIPRO, 2012 Proceedings of the 35th International Convention},
file = {:home/chiroptera/Dropbox/mendeley/Sirotkovi, Dujmi, Papi - 2012 - K-means image segmentation on massively parallel GPU architecture.pdf:pdf},
isbn = {9789532330724},
keywords = {CPU,CUDA,Clustering algorithms,Computer architecture,GPU hardware performance,Graphics processing unit,Image segmentation,Instruction sets,Kernel,Labeling,central processing unit,compute unified device architecture,coprocessor,core algorithm,dataset characteristic,graphic processing unit hardware,graphics processing units,image segmentation,k-means image segmentation,parallel GPU hardware architecture,parallel architectures,parallel k-means segmentation,pattern clustering,performance evaluation,simplified programming methods},
mendeley-tags = {CPU,CUDA,Clustering algorithms,Computer architecture,GPU hardware performance,Graphics processing unit,Image segmentation,Instruction sets,Kernel,Labeling,central processing unit,compute unified device architecture,coprocessor,core algorithm,dataset characteristic,graphic processing unit hardware,graphics processing units,image segmentation,k-means image segmentation,parallel GPU hardware architecture,parallel architectures,parallel k-means segmentation,pattern clustering,performance evaluation,simplified programming methods},
pages = {489--494},
title = {{K-means image segmentation on massively parallel GPU architecture}},
year = {2012}
}
@article{Weinstein2009a,
abstract = {Last year, in 2008, I gave a talk titled {\{}$\backslash$it Quantum Calisthenics{\}}. This year I am going to tell you about how the work I described then has spun off into a most unlikely direction. What I am going to talk about is how one maps the problem of finding clusters in a given data set into a problem in quantum mechanics. I will then use the tricks I described to let quantum evolution lets the clusters come together on their own.},
archivePrefix = {arXiv},
arxivId = {0911.0462},
author = {Weinstein, Marvin},
eprint = {0911.0462},
file = {:home/chiroptera/Dropbox/mendeley/Weinstein - 2009 - Strange Bedfellows Quantum Mechanics and Data Mining.pdf:pdf},
pages = {11},
title = {{Strange Bedfellows: Quantum Mechanics and Data Mining}},
url = {http://arxiv.org/abs/0911.0462},
year = {2009}
}
@misc{hornmatlab,
file = {:home/chiroptera/Dropbox/mendeley/Unknown - Unknown - Quantum Clustering Matlab Implementation.html:html},
title = {{Quantum Clustering Matlab Implementation}},
url = {http://horn.tau.ac.il/QC.htm}
}
@article{strack2014impact,
annote = {paper for DIABETES data set 

http://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008},
author = {Strack, Beata and DeShazo, Jonathan P and Gennings, Chris and Olmo, Juan L and Ventura, Sebastian and Cios, Krzysztof J and Clore, John N},
journal = {BioMed research international},
publisher = {Hindawi Publishing Corporation},
title = {{Impact of HbA1c measurement on hospital readmission rates: analysis of 70,000 clinical database patient records}},
url = {http://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008},
volume = {2014},
year = {2014}
}
@inproceedings{bigdatastream,
author = {O'Callaghan, L and Mishra, N and Meyerson, A and Guha, S and Motwani, R},
booktitle = {Data Engineering, 2002. Proceedings. 18th International Conference on},
doi = {10.1109/ICDE.2002.994785},
file = {:home/chiroptera/Dropbox/mendeley/O'Callaghan et al. - 2002 - Streaming-data algorithms for high-quality clustering.pdf:pdf},
issn = {1063-6382},
keywords = {Algorithm design and analysis,Clustering algorithms,Computer science,Data analysis,Data engineering,Lab-on-a-chip,Laboratories,Partitioning algorithms,Telecommunications,Telephony,Web documents,click streams,data analysis,high-quality clustering,large data stream clusters,pattern clustering,single-pass algorithms,streaming data analysis,streaming-data algorithms,telephone records},
pages = {685--694},
title = {{Streaming-data algorithms for high-quality clustering}},
year = {2002}
}
@article{Kumar2004,
author = {Kumar, Nimit and Behera, Laxmidhar},
doi = {10.1023/B:NEPL.0000039429.89321.07},
issn = {1370-4621},
journal = {Neural Processing Letters},
keywords = {clustering,quantum},
mendeley-tags = {clustering,quantum},
month = {aug},
number = {1},
pages = {11--22},
title = {{Visual–Motor Coordination Using a Quantum Clustering Based Neural Control Scheme}},
url = {http://link.springer.com/10.1023/B:NEPL.0000039429.89321.07},
volume = {20},
year = {2004}
}
@article{Oliphant2007,
abstract = {Python is an excellent "steering" language for scientific codes written in other languages. However, with additional basic tools, Python transforms into a high-level language suited for scientific and engineering code that's often fast enough to be immediately useful but also flexible enough to be sped up with additional extensions.},
author = {Oliphant, Travis E.},
doi = {10.1109/MCSE.2007.58},
file = {:home/chiroptera/Dropbox/mendeley/Oliphant - 2007 - Python for scientific computing.pdf:pdf},
isbn = {9781584889298},
issn = {15219615},
journal = {Computing in Science and Engineering},
number = {3},
pages = {10--20},
title = {{Python for scientific computing}},
volume = {9},
year = {2007}
}
@article{Weinstein2013,
abstract = {How does one search for a needle in a multi-dimensional haystack without knowing what a needle is and without knowing if there is one in the haystack? This kind of problem requires a paradigm shift - away from hypothesis driven searches of the data - towards a methodology that lets the data speak for itself. Dynamic Quantum Clustering (DQC) is such a methodology. DQC is a powerful visual method that works with big, high-dimensional data. It exploits variations of the density of the data (in feature space) and unearths subsets of the data that exhibit correlations among all the measured variables. The outcome of a DQC analysis is a movie that shows how and why sets of data-points are eventually classified as members of simple clusters or as members of - what we call - extended structures. This allows DQC to be successfully used in a non-conventional exploratory mode where one searches data for unexpected information without the need to model the data. We show how this works for big, complex, real-world datasets that come from five distinct fields: i.e., x-ray nano-chemistry, condensed matter, biology, seismology and finance. These studies show how DQC excels at uncovering unexpected, small - but meaningful - subsets of the data that contain important information. We also establish an important new result: namely, that big, complex datasets often contain interesting structures that will be missed by many conventional clustering techniques. Experience shows that these structures appear frequently enough that it is crucial to know they can exist, and that when they do, they encode important hidden information. In short, we not only demonstrate that DQC can be flexibly applied to datasets that present significantly different challenges, we also show how a simple analysis can be used to look for the needle in the haystack, determine what it is, and find what this means.},
archivePrefix = {arXiv},
arxivId = {1310.2700},
author = {Weinstein, M and Meirer, F and Hume, A},
eprint = {1310.2700},
file = {:home/chiroptera/Dropbox/mendeley/Weinstein, Meirer, Hume - 2013 - Analyzing Big Data with Dynamic Quantum Clustering.pdf:pdf},
journal = {arXiv preprint arXiv: {\ldots}},
pages = {1--37},
title = {{Analyzing Big Data with Dynamic Quantum Clustering}},
url = {http://arxiv.org/abs/1310.2700 http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:No+Title{\#}0},
year = {2013}
}
@inproceedings{Talbi2004,
abstract = {This paper presents a new algorithm for solving the travelling salesman problem (TSP). The TSP is one of the most known combinatorial optimisation problems. It is about finding the shortest Hamiltonian cycle relating N cities. The algorithm is inspired from both genetic algorithms and quantum computing fields. It extends the standard genetic algorithms by combining them to some concepts and principles provided from quantum computing field such as quantum bit, states superposition and interference. The obtained results from the application of the proposed algorithm on some instances of TSP are significantly better than those provided by standard genetic algorithms.},
author = {Talbi, H and Draa, A and Batouche, M},
booktitle = {Industrial Technology, 2004. IEEE ICIT '04. 2004 IEEE International Conference on},
doi = {10.1109/ICIT.2004.1490730},
file = {:home/chiroptera/Dropbox/mendeley/Talbi, Draa, Batouche - 2004 - A new quantum-inspired genetic algorithm for solving the travelling salesman problem.pdf:pdf},
keywords = {Biological cells,Cities and towns,Ferroelectric films,Genetic algorithms,Genetic mutations,Hamiltonian cycle,Laboratories,Parallel processing,Quantum computing,Random access memory,Traveling salesman problems,combinatorial optimisation problems,genetic algorithms,quantum computing,quantum computing field,quantum-inspired genetic algorithm,travelling salesman problem,travelling salesman problems},
pages = {1192--1197},
title = {{A new quantum-inspired genetic algorithm for solving the travelling salesman problem}},
volume = {3},
year = {2004}
}
@article{Zechner2009a,
abstract = {In this paper an optimized k-means implementation on the graphics processing unit (GPU) is presented. NVIDIApsilas compute unified device architecture (CUDA), available from the G80 GPU family onwards, is used as the programming environment. Emphasis is placed on optimizations directly targeted at this architecture to best exploit the computational capabilities available. Additionally drawbacks and limitations of previous related work, e.g. maximum instance, dimension and centroid count are addressed. The algorithm is realized in a hybrid manner, parallelizing distance calculations on the GPU while sequentially updating cluster centroids on the CPU based on the results from the GPU calculations. An empirical performance study on synthetic data is given, demonstrating a maximum 14times speed increase to a fully SIMD optimized CPU implementation.},
author = {Zechner, Mario and Granitzer, Michael},
doi = {10.1109/INTENSIVE.2009.19},
file = {:home/chiroptera/Dropbox/mendeley/Zechner, Granitzer - 2009 - Accelerating k-means on the graphics processor via CUDA(2).pdf:pdf},
isbn = {9780769535852},
journal = {Proceedings of the 1st International Conference on Intensive Applications and Services, INTENSIVE 2009},
pages = {7--15},
title = {{Accelerating k-means on the graphics processor via CUDA}},
year = {2009}
}
@article{Faro2012,
author = {Faro, Alberto and Giordano, Daniela and Palazzo, Simone},
file = {:home/chiroptera/Dropbox/mendeley/Faro, Giordano, Palazzo - 2012 - Integrating Unsupervised and Supervised Clustering Methods on a GPU platform for Fast Image Segmentatio.pdf:pdf},
isbn = {9781467325844},
keywords = {gpu,image segmentation,parallel clustering},
title = {{Integrating Unsupervised and Supervised Clustering Methods on a GPU platform for Fast Image Segmentation}},
year = {2012}
}
@article{Soman2010,
abstract = {Graphics processing units provide a large computational power at a very low price which position them as an ubiquitous accelerator. General purpose programming on the graphics processing units (GPGPU) is best suited for regular data parallel algorithms. They are not directly amenable for algorithms which have irregular data access patterns such as list ranking, and finding the connected components of a graph, and the like. In this work, we present a GPU-optimized implementation for finding the connected components of a given graph. Our implementation tries to minimize the impact of irregularity, both at the data level and functional level. Our implementation achieves a speed up of 9 to 12 times over the best sequential CPU implementation. For instance, our implementation finds connected components of a graph of 10 million nodes and 60 million edges in about 500 milliseconds on a GPU, given a random edge list. We also draw interesting observations on why PRAM algorithms, such as the Shiloach-Vishkin algorithm may not be a good fit for the GPU and how they should be modified.},
author = {Soman, Jyothish and Kishore, Kothapalli and Narayanan, P. J.},
doi = {10.1109/IPDPSW.2010.5470817},
file = {:home/chiroptera/Dropbox/mendeley/Soman, Kishore, Narayanan - 2010 - A fast GPU algorithm for graph connectivity.pdf:pdf},
isbn = {9781424465347},
journal = {Proceedings of the 2010 IEEE International Symposium on Parallel and Distributed Processing, Workshops and Phd Forum, IPDPSW 2010},
keywords = {Connected components,GPGPU,GPU,Irregular algorithms},
title = {{A fast GPU algorithm for graph connectivity}},
year = {2010}
}
@article{Blekas2007,
abstract = {Given a data set, a dynamical procedure is applied to the data points in order to shrink and separate, possibly overlapping clusters. Namely, Newton's equations of motion are employed to concentrate the data points around their cluster centers, using an attractive potential, constructed specially for this purpose. During this process, important information is gathered concerning the spread of each cluster. In succession this information is used to create an objective function that maps each cluster to a local maximum. Global optimization is then used to retrieve the positions of the maxima that correspond to the locations of the cluster centers. Further refinement is achieved by applying the EM-algorithm to a Gaussian mixture model whose construction and initialization is based on the acquired information. To assess the effectiveness of our method, we have conducted experiments on a plethora of benchmark data sets. In addition we have compared its performance against four clustering techniques that are well established in the literature. [All rights reserved Elsevier]},
author = {Blekas, K. and Lagaris, I.E.},
doi = {10.1016/j.patcog.2006.07.012},
file = {:home/chiroptera/Dropbox/mendeley/Blekas, Lagaris - 2007 - Newtonian clustering An approach based on molecular dynamics and global optimization.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {clustering,global optimization,molecular dynamics,order statistics},
pages = {1734--1744},
title = {{Newtonian clustering: An approach based on molecular dynamics and global optimization}},
volume = {40},
year = {2007}
}
@article{Anderson2013a,
abstract = {A new generation of data processing systems, including web search, Google's Knowledge Graph, IBM's Watson, and sev- eral di erent recommendation systems, combine rich databases with software driven by machine learning. The spectacular successes of these trained systems have been among the most notable in all of computing and have generated excitement in health care, nance, energy, and general business. But building them can be challenging, even for computer scien- tists with PhD-level training. If these systems are to have a truly broad impact, building them must become easier. We explore one crucial pain point in the construction of trained systems: feature engineering. Given the sheer size of modern datasets, feature developers must (1) write code with few e ective clues about how their code will interact with the data and (2) repeatedly endure long system waits even though their code typically changes little from run to run. We propose brainwash, a vision for a feature engineer- ing data system that could dramatically ease the Explore- Extract-Evaluate interaction loop that characterizes many trained system projects.},
author = {Anderson, Michael and Antenucci, D and Bittorf, Victor},
file = {:home/chiroptera/Dropbox/mendeley/Anderson, Antenucci, Bittorf - 2013 - Brainwash A Data System for Feature Engineering.pdf:pdf},
journal = {Eecs.Umich.Edu},
title = {{Brainwash: A Data System for Feature Engineering}},
url = {http://web.eecs.umich.edu/{~}mrander/pubs/mythical{\_}man.pdf},
year = {2013}
}
@article{Ho2002,
abstract = {We studied a number of measures that charaterize the difficulty of a classification problem, focusing on the geometrical complexity of the class boudary. We compared a set of real world problems to random labelings of points and found that real problems contain structures in this measurement space that are significantly different from the random sets. Distributions of problems in this space show that there exist at least two independent factors affecting a problem's difficulty. We suggest using this space to describe a classifier's domain of competence. This can guide static and dynamic selection of classifiers fro specific problems as well as subproblems formed by confinement, projection, and transformations of the feature vectors.},
author = {Ho, Tin K and Basu, Mitra},
doi = {10.1109/34.990132},
file = {:home/chiroptera/Dropbox/mendeley/Ho, Basu - 2002 - Complexity measures of supervised classification problems.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Learning},
number = {3},
pages = {289--300},
title = {{Complexity measures of supervised classification problems}},
volume = {24},
year = {2002}
}
@article{NVIDIACorporation2006,
author = {{NVIDIA Corporation}},
file = {:home/chiroptera/Dropbox/mendeley/NVIDIA Corporation - 2006 - CUDA Programming Model Overview.pdf:pdf},
title = {{CUDA Programming Model Overview}},
year = {2006}
}
@article{Hou2013,
author = {Hou, Rui and Jiang, Tao and Zhang, Liuhang and Qi, Pengfei and Dong, Jianbo and Wang, Haibin and Gu, Xiongli and Zhang, Shujie},
doi = {10.1109/HPCA.2013.6522317},
file = {:home/chiroptera/Dropbox/mendeley/Hou et al. - 2013 - Cost effective data center servers.pdf:pdf},
isbn = {978-1-4673-5587-2},
journal = {2013 IEEE 19th International Symposium on High Performance Computer Architecture (HPCA)},
pages = {179--187},
title = {{Cost effective data center servers}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6522317},
year = {2013}
}
@article{Zhai2014,
abstract = {Big data processing is receiving significant amount of interest as an important technology to reveal the information behind the data, such as trends, characteristics, etc. MapReduce is one of the most popular distributed parallel data processing framework. However, some high-end applications, especially some scientific analyses have both data-intensive and computation intensive features. Therefore, we have designed and implemented a high performance big data process framework called Lit, which leverages the power of Hadoop and GPUs. In this paper, we presented the basic design and architecture of Lit. More importantly, we spent a lot of effort on optimizing the communications between CPU and GPU. Lit integrated GPU with Hadoop to improve the computational power of each node in the cluster. To simplify the parallel programming, Lit provided an annotation based approach to automatically generate CUDA codes from Hadoop codes. Lit hid the complexity of programming on CPU/GPU cluster by providing extended compiler and optimizer. To utilize the simplified programming, scalability and fault tolerance benefits of Hadoop and combine them with the high performance computation power of GPU, Lit extended the Hadoop by applying a GPUClassloader to detect the GPU, generate and compile CUDA codes, and invoke the shared library. For all CPU-GPU co-processing systems, the communication with the GPU is the well-known performance bottleneck. We introduced data flow optimization approach to reduce unnecessary memory copies. Our experimental results show that Lit can achieve an average speedup of 1x to 3x on three typical applications over Hadoop, and the data flow optimization approach for the Lit can achieve about 16{\%} performance gain. © 2013 IEEE.},
annote = {MapReduce for using GPU in clusters. Has review of other GPU MapReduce implementations.},
author = {Zhai, Yanlong and Guo, Ying and Chen, Qiurui and Yang, Kai and Mbarushimana, Emmanuel},
doi = {10.1109/HPCC.and.EUC.2013.147},
file = {:home/chiroptera/Dropbox/mendeley/Zhai et al. - 2014 - Design and optimization of a big data computing framework based on CPUGPU cluster.pdf:pdf},
isbn = {9780769550886},
journal = {Proceedings - 2013 IEEE International Conference on High Performance Computing and Communications, HPCC 2013 and 2013 IEEE International Conference on Embedded and Ubiquitous Computing, EUC 2013},
keywords = {GPU,cluster,mapreduce},
mendeley-tags = {GPU,cluster,mapreduce},
pages = {1039--1046},
title = {{Design and optimization of a big data computing framework based on CPU/GPU cluster}},
year = {2014}
}
@article{Guo2013,
author = {Guo, Yiru and Liu, Weiguo and Voss, Gerrit and Mueller-Wittig, Wolfgang},
doi = {10.1109/HPCC.and.EUC.2013.88},
file = {:home/chiroptera/Dropbox/mendeley/Guo et al. - 2013 - GCMR A GPU Cluster-Based MapReduce Framework for Large-Scale Data Processing.pdf:pdf},
isbn = {978-0-7695-5088-6},
journal = {2013 IEEE 10th International Conference on High Performance Computing and Communications {\&} 2013 IEEE International Conference on Embedded and Ubiquitous Computing},
keywords = {-mapreduce,1,15,3,8,9,MapReduce,and bioinformatics,big data,cluster,cuda,database operations,due to the continuing,gpu,gpu cluster,image processing,mpi,rapid growth of data,size in},
mendeley-tags = {MapReduce,big data,cluster,gpu},
pages = {580--586},
title = {{GCMR: A GPU Cluster-Based MapReduce Framework for Large-Scale Data Processing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6831970},
year = {2013}
}
@article{Papenhausen2013,
abstract = {Clustering is an important preparation step in big data processing. It may even be used to detect redundant data points as well as outliers. Elimination of redundant data and duplicates can serve as a viable means for data reduction and it can also aid in sampling. Visual feedback is very valuable here to give users confidence in this process. Furthermore, big data preprocessing is seldom interactive, which stands at conflict with users who seek answers immediately. The best one can do is incremental preprocessing in which partial and hopefully quite accurate results become available relatively quickly and are then refined over time. We propose a correlation clustering framework which uses MDS for layout and GPU-acceleration to accomplish these goals. Our domain application is the correlation clustering of atmospheric mass spectrum data with 8 million data points of 450 dimensions each. © 2013 IEEE.},
author = {Papenhausen, Eric and Wang, Bing and Ha, Sungsoo and Zelenyuk, Alla and Imre, Dan and Mueller, Klaus},
doi = {10.1109/BigData.2013.6691716},
file = {:home/chiroptera/Dropbox/mendeley/Papenhausen et al. - 2013 - GPU-accelerated incremental correlation clustering of large data with visual feedback.pdf:pdf},
isbn = {9781479912926},
journal = {Proceedings - 2013 IEEE International Conference on Big Data, Big Data 2013},
keywords = {GPU,big data,clustering,correlation,visual analytics,visualization},
pages = {63--70},
title = {{GPU-accelerated incremental correlation clustering of large data with visual feedback}},
year = {2013}
}
@article{Dutton2010,
abstract = {Today's design of avionics is being pulled in two opposite directions - increased use of COTS and more requirements for certification. In particular, graphics processing has largely relied upon COTS devices even though the need for certification of GPUs has been amplified. This paper provides a background on graphics processing, summarizes the avionics requirements, surveys the options for graphics processing in avionics, and presents an architecture for an FPGA-based graphics processor.},
author = {Dutton, Marcus and Keezer, David},
doi = {10.1109/DASC.2010.5655325},
file = {:home/chiroptera/Dropbox/mendeley/Dutton, Keezer - 2010 - The challenges of graphics processing in the avionics industry.pdf:pdf},
isbn = {9781424466160},
issn = {2155-7195},
journal = {AIAA/IEEE Digital Avionics Systems Conference - Proceedings},
pages = {1--9},
title = {{The challenges of graphics processing in the avionics industry}},
year = {2010}
}
@article{Fang2011,
abstract = {This paper presents a comprehensive performance comparison between CUDA and OpenCL. We have selected 16 benchmarks ranging from synthetic applications to real-world ones. We make an extensive analysis of the performance gaps taking into account programming models, ptimization strategies, architectural details, and underlying compilers. Our results show that, for most applications, CUDA performs at most 30$\backslash${\&}{\#}x025; better than OpenCL. We also show that this difference is due to unfair comparisons: in fact, OpenCL can achieve similar performance to CUDA under a fair comparison. Therefore, we define a fair comparison of the two types of applications, providing guidelines for more potential analyses. We also investigate OpenCL's portability by running the benchmarks on other prevailing platforms with minor modifications. Overall, we conclude that OpenCL's portability does not fundamentally affect its performance, and OpenCL can be a good alternative to CUDA.},
annote = {Very good introduction.},
archivePrefix = {arXiv},
arxivId = {1005.2581},
author = {Fang, Jianbin and Varbanescu, Ana Lucia and Sips, Henk},
doi = {10.1109/ICPP.2011.45},
eprint = {1005.2581},
file = {:home/chiroptera/Dropbox/mendeley/Fang, Varbanescu, Sips - 2011 - A comprehensive performance comparison of CUDA and OpenCL.pdf:pdf},
isbn = {9780769545103},
issn = {01903918},
journal = {Proceedings of the International Conference on Parallel Processing},
keywords = {CUDA,OpenCL,Performance Comparison,comparison,cuda,opencl},
mendeley-tags = {comparison,cuda,opencl},
pages = {216--225},
title = {{A comprehensive performance comparison of CUDA and OpenCL}},
year = {2011}
}
@article{Scott1999a,
abstract = {Most research in text classification has used the “bag of words” representation of text. This paper examines some alternative ways to represent text based on syntactic and semantic relationships between words (phrases, synonyms and hypernyms). We describe the new representations and try to justify our suspicions that they could have improved the performance of a rule-based learner. The representations are evaluated using the RIPPER rule-based learner on the Reuters-21578 and DigiTrad test corpora, but on their own the new representations are not found to produce a significant performance improvement. Finally, we try combining classifiers based on different representations using a majority voting technique. This step does produce some performance improvement on both test collections. In general, our work supports the emerging consensus in the information retrieval community that more sophisticated Natural Language Processing techniques need to be developed before better text representations can be produced. We conclude that for now, research into new learning algorithms and methods for combining existing learners holds the most promise.},
author = {Scott, Sam and Matwin, Stan},
doi = {10.1016/j.jbi.2012.04.010},
file = {:home/chiroptera/Dropbox/mendeley/Scott, Matwin - 1999 - Feature engineering for text classification.pdf:pdf},
isbn = {1577351894},
journal = {Machine Learning-International Workshop},
keywords = {classification,feature,feature engineering,from extractor,hypernyms,learning,learning algorithms,machine learning,performance,phrases,representations,ripper,semantic relationships,synonyms,test collections,text classification,text representations,wordnet},
pages = {1--13},
pmid = {22580178},
title = {{Feature engineering for text classification}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/cbdv.200490137/abstract$\backslash$nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.7033{\&}amp;rep=rep1{\&}amp;type=pdf},
volume = {6},
year = {1999}
}
@article{Slota2014,
author = {Slota, George M. and Rajamanickam, Sivasankaran and Madduri, Kamesh},
doi = {10.1109/IPDPS.2014.64},
file = {:home/chiroptera/Dropbox/mendeley/Slota, Rajamanickam, Madduri - 2014 - BFS and Coloring-based Parallel Algorithms for Strongly Connected Components and Related Problems.pdf:pdf},
isbn = {1530-2075 VO -},
keywords = {-strongly connected components,7,bfs,coloring,its use,most parallel,multicore algorithms,often met with limited,performance analysis,scc algorithms have avoided,success,therefore},
pages = {1--22},
title = {{BFS and Coloring-based Parallel Algorithms for Strongly Connected Components and Related Problems}},
year = {2014}
}
@article{Xiao2008,
abstract = {The conventional k-means clustering algorithm must know the number of clusters in advance and the clustering result is sensitive to the selection of the initial cluster centroids. The sensitivity may make the algorithm converge to the local optima. This paper proposes an improved k-means clustering algorithm based on quantum-inspired genetic algorithm (KMQGA). In KMQGA, Q-bit based representation is employed for exploration and exploitation in discrete 0-1 hyperspace by using rotation operation of quantum gate as well as three genetic algorithm operations (selection, crossover and mutation) of Q-bit. Without knowing the exact number of clusters beforehand, the KMQGA can get the optimal number of clusters as well as providing the optimal cluster centroids after several iterations of the four operations (selection, crossover, mutation, and rotation). The simulated datasets and the real datasets are used to validate KMQGA and to compare KMQGA with an improved k-means clustering algorithm based on the famous variable string length genetic algorithm (KMVGA) respectively. The experimental results show that KMQGA is promising and the effectiveness and the search quality of KMQGA is better than those of KMVGA.},
author = {Xiao, Jing and Yan, YuPing and Lin, Ying and Yuan, Ling and Zhang, Jun},
file = {:home/chiroptera/Dropbox/mendeley/Xiao et al. - 2008 - A Quantum-inspired Genetic Algorithm for data clustering.pdf:pdf},
journal = {2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)},
title = {{A Quantum-inspired Genetic Algorithm for data clustering}},
year = {2008}
}
@article{Han2000,
abstract = {This paper proposes a novel evolutionary computing method called a genetic quantum algorithm (GQA). GQA is based on the concept and principles of quantum computing such as qubits and superposition of states. Instead of binary, numeric, or symbolic representation, by adopting qubit chromosome as a representation GQA can represent a linear superposition of solutions due to its probabilistic representation. As genetic operators, quantum gates are employed for the search of the best solution. Rapid convergence and good global search capability characterize the performance of GQA. The effectiveness and the applicability of GQA are demonstrated by experimental results on the knapsack problem, which is a well-known combinatorial optimization problem. The results show that GQA is superior to other genetic algorithms using penalty functions, repair methods and decoders},
author = {Han, Kuk-Hyun and Kim, Jong-Hwan},
doi = {10.1109/CEC.2000.870809},
file = {:home/chiroptera/Dropbox/mendeley/Han, Kim - 2000 - Genetic quantum algorithm and its application to combinatorial optimization problem.pdf:pdf},
isbn = {0-7803-6375-2},
journal = {Proceedings of the 2000 Congress on Evolutionary Computation. CEC00 (Cat. No.00TH8512)},
title = {{Genetic quantum algorithm and its application to combinatorial optimization problem}},
volume = {2},
year = {2000}
}
@inproceedings{Fred2002,
abstract = {We explore the idea of evidence accumulation for combining the results of multiple clusterings. Initially, n d-dimensional data is decomposed into a large number of compact clusters; the K-means algorithm performs this decomposition, with several clusterings obtained by N random initializations of the K-means. Taking the co-occurrences of pairs of patterns in the same cluster as votes for their association, the data partitions are mapped into a co-association matrix of patterns. This n{\&}amp;times;n matrix represents a new similarity measure between patterns. The final clusters are obtained by applying a MST-based clustering algorithm on this matrix. Results on both synthetic and real data show the ability of the method to identify arbitrary shaped clusters in multidimensional data.},
author = {Fred, Ana N L and Jain, Anil K},
booktitle = {Pattern Recognition, 2002. Proceedings. 16th International Conference on},
doi = {10.1109/ICPR.2002.1047450},
file = {:home/chiroptera/Dropbox/mendeley/Fred, Jain - 2002 - Data clustering using evidence accumulation.pdf:pdf},
isbn = {0-7695-1695-X},
issn = {1051-4651},
pages = {276--280},
title = {{Data clustering using evidence accumulation}},
volume = {4},
year = {2002}
}
@article{Zou2014,
abstract = {Bioinformatics is challenged by the fact that traditional analysis tools have difficulty in processing large-scale data from high-throughput sequencing. The open source Apache Hadoop project, which adopts the MapReduce framework and a distributed file system, has recently given bioinformatics researchers an opportunity to achieve scalable, efficient and reliable computing performance on Linux clusters and on cloud computing services. In this article, we present MapReduce frame-based applications that can be employed in the next-generation sequencing and other biological domains. In addition, we discuss the challenges faced by this field as well as the future works on parallel computing in bioinformatics.},
author = {Zou, Quan and Li, Xu Bin and Jiang, Wen Rui and Lin, Zi Yu and Li, Gui Lin and Chen, Ke},
doi = {10.1093/bib/bbs088},
file = {:home/chiroptera/Dropbox/mendeley//Zou et al. - 2014 - Survey of MapReduce frame operation in bioinformatics.pdf:pdf},
isbn = {1477-4054 (Electronic)$\backslash$n1467-5463 (Linking)},
issn = {14774054},
journal = {Briefings in Bioinformatics},
keywords = {Bioinformatics,Hadoop,MapReduce,bioinformatics},
mendeley-tags = {MapReduce,bioinformatics},
number = {4},
pages = {637--647},
pmid = {23396756},
title = {{Survey of MapReduce frame operation in bioinformatics}},
volume = {15},
year = {2014}
}
@article{Fred2006,
abstract = {Each clustering algorithm induces a similarity between given data points, according to the underlying clustering criteria. Given the large number of available clustering techniques, one is faced with the following questions: (a) Which measure of similarity should be used in a given clustering problem? (b) Should the same similarity measure be used throughout the d-dimensional feature space? In other words, are the underlying clusters in given data of similar shape? Our goal is to learn the pairwise similarity between points in order to facilitate a proper partitioning of the data without the a priori knowledge of k, the number of clusters, and of the shape of these clusters. We explore a clustering ensemble approach combined with cluster stability criteria to selectively learn the similarity from a collection of different clustering algorithms with various parameter configurations},
author = {Fred, Ana L N and Jain, Anil K.},
doi = {10.1109/ICPR.2006.754},
file = {:home/chiroptera/Dropbox/mendeley/Fred, Jain - 2006 - Learning pairwise similarity for data clustering.pdf:pdf},
isbn = {0769525210},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
pages = {925--928},
title = {{Learning pairwise similarity for data clustering}},
volume = {1},
year = {2006}
}
@inproceedings{Sun2004,
abstract = {In this paper, inspired by the analysis of convergence of PSO, we study the individual particle of a PSO system moving in a quantum multidimensional space and establish a quantum delta potential well model for PSO. After that, a trial method of parameter control and QDPSO is proposed. The experiment result shows much advantage of QDPSO to the traditional PSO.},
author = {Sun, Jun and Feng, Bin and Xu, Wenbo},
booktitle = {Evolutionary Computation, 2004. CEC2004. Congress on},
doi = {10.1109/CEC.2004.1330875},
keywords = {Birds,Educational institutions,Equations,Evolutionary computation,Information analysis,Information technology,Organisms,PSO system,Particle swarm optimization,Potential well,QDPSO,Sun,evolutionary computation,optimisation,parameter control,particle swarm optimization,quantum behavior,quantum delta potential well model,quantum multidimensional space,quantum theory},
pages = {325--331 Vol.1},
title = {{Particle swarm optimization with particles having quantum behavior}},
volume = {1},
year = {2004}
}
@article{Connor2010,
abstract = {We present a parallel algorithm for k-nearest neighbor graph construction that uses Morton ordering. Experiments show that our approach has the following advantages over existing methods: 1) faster construction of k-nearest neighbor graphs in practice on multicore machines, 2) less space usage, 3) better cache efficiency, 4) ability to handle large data sets, and 5) ease of parallelization and implementation. If the point set has a bounded expansion constant, our algorithm requires one-comparison-based parallel sort of points, according to Morton order plus near-linear additional steps to output the k-nearest neighbor graph.},
author = {Connor, Michael and Kumar, Piyush},
doi = {10.1109/TVCG.2010.9},
file = {:home/chiroptera/Dropbox/mendeley/Connor, Kumar - 2010 - Fast construction of k-nearest neighbor graphs for point clouds.pdf:pdf},
isbn = {2009010019},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {??-nearest neighbor graphics,Morton ordering,Nearest neighbor searching,Parallel algorithms,Point-based graphics},
number = {4},
pages = {599--608},
pmid = {20467058},
title = {{Fast construction of k-nearest neighbor graphs for point clouds}},
volume = {16},
year = {2010}
}
@article{Grossman2013,
abstract = {As the scale of high performance computing systems grows, three main challenges arise: the programmability, reliability, and energy efficiency of those systems. Accomplishing all three without sacrificing performance requires a rethinking of legacy distributed programming models and homogeneous clusters. In this work, we integrate Hadoop MapReduce with OpenCL to enable the use of heterogeneous processors in a distributed system. We do this by exploiting the implicit data- parallelism of mappers and reducers in a MapReduce system. Combining Hadoop and OpenCL provides 1) an easy-to-learn and flexible application programming interface in a high level and popular programming language, 2) the reliability guarantees and distributed filesystem of Hadoop, and 3) the low power consumption and performance acceleration of heterogeneous processors. This paper presents HadoopCL: an extension to Hadoop which supports execution of user-written Java kernels on heterogeneous devices, optimizes communication through asynchronous transfers and dedicated I/O threads, automatically generates OpenCL kernels from Java bytecode using the open source tool APARAPI, and achieves nearly 3x overall speedup and better than 55x speedup of the computational sections for example MapReduce applications, relative to Hadoop.},
author = {Grossman, Max and Breternitz, Mauricio and Sarkar, Vivek},
doi = {10.1109/IPDPSW.2013.246},
file = {:home/chiroptera/Dropbox/mendeley/Grossman, Breternitz, Sarkar - 2013 - HadoopCL MapReduce on distributed heterogeneous platforms through seamless integration of hadoop a.pdf:pdf},
isbn = {978-0-7695-4979-8},
journal = {Proceedings - IEEE 27th International Parallel and Distributed Processing Symposium Workshops and PhD Forum, IPDPSW 2013},
keywords = {GPGPU,Hadoop,OpenCL,heterogeneous,multicore},
pages = {1918--1927},
title = {{HadoopCL: MapReduce on distributed heterogeneous platforms through seamless integration of hadoop and OpenCL}},
year = {2013}
}
@article{Jain2010,
abstract = {Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms into a system of ranked taxa: domain, kingdom, phylum, class, etc. Cluster analysis is the formal study of methods and algorithms for grouping, or clustering, objects according to measured or perceived intrinsic characteristics or similarity. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes data clustering (unsupervised learning) from classification or discriminant analysis (supervised learning). The aim of clustering is to find structure in data and is therefore exploratory in nature. Clustering has a long and rich history in a variety of scientific fields. One of the most popular and simple clustering algorithms, K-means, was first published in 1955. In spite of the fact that K-means was proposed over 50 years ago and thousands of clustering algorithms have been published since then, K-means is still widely used. This speaks to the difficulty in designing a general purpose clustering algorithm and the ill-posed problem of clustering. We provide a brief overview of clustering, summarize well known clustering methods, discuss the major challenges and key issues in designing clustering algorithms, and point out some of the emerging and useful research directions, including semi-supervised clustering, ensemble clustering, simultaneous feature selection during data clustering, and large scale data clustering. ?? 2009 Elsevier B.V. All rights reserved.},
author = {Jain, Anil K},
doi = {10.1016/j.patrec.2009.09.011},
file = {:home/chiroptera/Dropbox/mendeley/Jain - 2010 - Data clustering 50 years beyond K-means.pdf:pdf},
isbn = {9781424417360},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Data clustering,Historical developments,King-Sun Fu prize,Perspectives on clustering,User's dilemma},
number = {8},
pages = {651--666},
publisher = {Elsevier B.V.},
title = {{Data clustering: 50 years beyond K-means}},
url = {http://dx.doi.org/10.1016/j.patrec.2009.09.011},
volume = {31},
year = {2010}
}
@article{Fred2003,
author = {Fred, Ana L N},
file = {:home/chiroptera/Dropbox/mendeley/Fred - 2003 - A New Cluster Isolation Criterion Based on Dissimilarity Increments ´.pdf:pdf},
number = {8},
pages = {1--15},
title = {{A New Cluster Isolation Criterion Based on Dissimilarity Increments ´}},
volume = {25},
year = {2003}
}
@article{Kindratenko2009a,
abstract = {Large-scale GPU clusters are gaining popularity in the scientific computing community. However, their deployment and production use are associated with a number of new challenges. In this paper, we present our efforts to address some of the challenges with building and running GPU clusters in HPC environments. We touch upon such issues as balanced cluster architecture, resource sharing in a cluster environment, programming models, and applications for GPU clusters.},
author = {Kindratenko, Volodymyr V. and Enos, Jeremy J. and Shi, Guochun and Showerman, Michael T. and Arnold, Galen W. and Stone, John E. and Phillips, James C. and Hwu, Wen Mei},
doi = {10.1109/CLUSTR.2009.5289128},
file = {:home/chiroptera/Dropbox/mendeley/Kindratenko et al. - 2009 - GPU clusters for high-performance computing.pdf:pdf},
isbn = {9781424450121},
issn = {15525244},
journal = {Proceedings - IEEE International Conference on Cluster Computing, ICCC},
title = {{GPU clusters for high-performance computing}},
year = {2009}
}
@book{reif1993synthesis,
editor = {Reif, John H},
publisher = {Morgan Kaufmann Publishers Inc.},
title = {{Synthesis of parallel algorithms}},
year = {1993}
}
@incollection{song2008parallel,
author = {Song, Yangqiu and Chen, Wen-Yen and Bai, Hongjie and Lin, Chih-Jen and Chang, Edward Y},
booktitle = {Machine Learning and Knowledge Discovery in Databases},
file = {:home/chiroptera/Dropbox/mendeley/Song et al. - 2008 - Parallel spectral clustering.pdf:pdf},
pages = {374--389},
publisher = {Springer},
title = {{Parallel spectral clustering}},
year = {2008}
}
@article{Steinhaeuser2010,
abstract = {The analysis of climate data has relied heavily on hypothesis-driven statistical methods, while projections of future climate are based primarily on physics-based computational models. However, in recent years a wealth of new datasets has become available. Therefore, we take a more data-centric approach and propose a unified framework for studying climate, with an aim toward characterizing observed phenomena as well as discovering new knowledge in climate science. Specifically, we posit that complex networks are well suited for both descriptive analysis and predictive modeling tasks. We show that the structural properties of 'climate networks' have useful interpretation within the domain. Further, we extract clusters from these networks and demonstrate their predictive power as climate indices. Our experimental results establish that the network clusters are statistically significantly better predictors than clusters derived using a more traditional clustering approach. Using complex networks as data representation thus enables the unique opportunity for descriptive and predictive modeling to inform each other. 2010 Wiley Periodicals, Inc.},
author = {Steinhaeuser, Karsten and Chawla, Nitesh V and Ganguly, Auroop R},
doi = {10.1002/sam},
file = {:home/chiroptera/Dropbox/mendeley/Steinhaeuser, Chawla, Ganguly - 2010 - Complex Networks as a Unified Framework for Descriptive Analysis and Predictive Modeling in Clima.pdf:pdf},
issn = {19321872},
journal = {Science And Technology},
keywords = {climate data,community detection,complex networks,multivariate predictive modeling,network analysis},
number = {5},
pages = {497--511},
title = {{Complex Networks as a Unified Framework for Descriptive Analysis and Predictive Modeling in Climate Science}},
volume = {4},
year = {2010}
}
@article{Bader2006,
abstract = {Minimum spanning tree (MST) is one of the most studied combinatorial problems with practical applications in VLSI layout, wireless communication, and distributed networks, recent problems in biology and medicine such as cancer detection, medical imaging, and proteomics, and national security and bioterrorism such as detecting the spread of toxins through populations in the case of biological/chemical warfare. Most of the previous attempts for improving the speed of MST using parallel computing are too complicated to implement or perform well only on special graphs with regular structure. In this paper we design and implement four parallel MST algorithms (three variations of Bor??vka plus our new approach) for arbitrary sparse graphs that for the first time give speedup when compared with the best sequential algorithm. In fact, our algorithms also solve the minimum spanning forest problem. We provide an experimental study of our algorithms on symmetric multiprocessors such as IBMs pSeries and Sun's Enterprise servers. Our new implementation achieves good speedups over a wide range of input graphs with regular and irregular structures, including the graphs used by previous parallel MST studies. For example, on an arbitrary random graph with 1 M vertices and 20 M edges, our new approach achieves a speedup of 5 using 8 processors. The source code for these algorithms is freely available from our web site. ?? 2006 Elsevier Inc. All rights reserved.},
author = {Bader, David a. and Cong, Guojing},
doi = {10.1016/j.jpdc.2006.06.001},
file = {:home/chiroptera/Dropbox/mendeley/Bader, Cong - 2006 - Fast shared-memory algorithms for computing the minimum spanning forest of sparse graphs.pdf:pdf},
isbn = {0-7695-2132-0},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
keywords = {Connectivity,High-performance algorithm engineering,Parallel graph algorithms},
number = {11},
pages = {1366--1378},
title = {{Fast shared-memory algorithms for computing the minimum spanning forest of sparse graphs}},
volume = {66},
year = {2006}
}
@article{Ji2013,
abstract = {Type Ia supernovae (SNe Ia) play a crucial role as standardizable cosmological candles, though the nature of their progenitors is a subject of active investigation. Recent observational and theoretical work has pointed to merging white dwarf binaries, referred to as the double-degenerate channel, as the possible progenitor systems for some SNe Ia. Additionally, recent theoretical work suggests that mergers which fail to detonate may produce magnetized, rapidly-rotating white dwarfs. In this paper, we present the first multidimensional simulations of the post-merger evolution of white dwarf binaries to include the effect of the magnetic field. In these systems, the two white dwarfs complete a final merger on a dynamical timescale, and are tidally disrupted, producing a rapidly-rotating white dwarf merger surrounded by a hot corona and a thick, differentially-rotating disk. The disk is strongly susceptible to the magnetorotational instability (MRI), and we demonstrate that this leads to the rapid growth of an initially dynamically weak magnetic field in the disk, the spin-down of the white dwarf merger, and to the subsequent central ignition of the white dwarf merger. Additionally, these magnetized models exhibit new features not present in prior hydrodynamic studies of white dwarf mergers, including the development of MRI turbulence in the hot disk, magnetized outflows carrying a significant fraction of the disk mass, and the magnetization of the white dwarf merger to field strengths {\$}\backslashsim 2 \backslashtimes 10{\^{}}8{\$} G. We discuss the impact of our findings on the origin and observed properties of SNe Ia and magnetized white dwarfs.},
archivePrefix = {arXiv},
arxivId = {1302.5700},
author = {Bogert, F. Alexander and Smith, Nicholas and Holdener, John and Jong, Eric M. De and Hart, Andrew F. and Shamir, Lior and Allen, Alice and {Luca Cinquini}, Shakeh E. Khudikyan and Thompson, David R. and Mattmann, Chris A. and Wagstaff, Kiri and Lazio, Joseph and Jones, Dayton L. and Teuben, Peter},
doi = {10.1088/0004-637X/773/2/136},
eprint = {1302.5700},
file = {:home/chiroptera/Dropbox/mendeley/Bogert et al. - 2013 - Computing in Astronomy Applications and Examples.pdf:pdf},
issn = {0004-637X},
journal = {The Astrophysical Journal},
pages = {14},
title = {{Computing in Astronomy: Applications and Examples}},
url = {http://arxiv.org/abs/1302.5700},
volume = {submitted},
year = {2013}
}
@article{kruskal1956shortest,
author = {Kruskal, Joseph B},
file = {:home/chiroptera/Dropbox/mendeley/Kruskal - 1956 - On the shortest spanning subtree of a graph and the traveling salesman problem.pdf:pdf},
journal = {Proceedings of the American Mathematical society},
number = {1},
pages = {48--50},
publisher = {JSTOR},
title = {{On the shortest spanning subtree of a graph and the traveling salesman problem}},
volume = {7},
year = {1956}
}
@article{Garcia2008,
author = {Garcia, Vincent and Debreuve, E and Barlaud, M},
doi = {10.1109/CVPRW.2008.4563100},
file = {:home/chiroptera/Dropbox/mendeley/Garcia, Debreuve, Barlaud - 2008 - Fast k nearest neighbor search using GPU.pdf:pdf},
isbn = {978-1-4244-2339-2},
journal = {Computer Vision and Pattern Recognition (CVPR)},
number = {2},
title = {{Fast k nearest neighbor search using GPU}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4563100},
year = {2008}
}
@article{Arefin2011,
author = {Arefin, Ahmed Shamsul and Inostroza-Ponta, Mario and Mathieson, Luke and Berretta, Regina and Moscato, Pablo},
doi = {10.1007/978-3-642-24669-2{\_}36},
file = {:home/chiroptera/Dropbox/mendeley/Arefin et al. - 2011 - Clustering nodes in large-scale biological networks using external memory algorithms.pdf:pdf},
isbn = {9783642246685},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Data clustering,external memory algorithms,gene expression data analysis,graph algorithms},
number = {PART 2},
pages = {375--386},
title = {{Clustering nodes in large-scale biological networks using external memory algorithms}},
volume = {7017 LNCS},
year = {2011}
}
@article{Wu2011,
abstract = {The {\$}k{\$}-means algorithm is widely used for unsupervised clustering. This paper describes an efficient CUDA-based {\$}k{\$}-means algorithm. Different from existing GPU-based k-means algorithms, our algorithm achieves better efficiency by utilizing the triangle inequality. Our algorithm explores the trade-off between load balance and memory access coalescing through data layout management. Because the effectiveness of the triangle inequity depends on the input data, we further propose a hybrid algorithm that adaptively determines whether to apply the triangle inequality. The efficiency of our algorithm is validated through extensive experiments, which demonstrate improved performance over existing CPU-based and CUDA-based k-means algorithms, in terms of both speed and scalability.},
author = {Wu, Jiadong and Hong, Bo},
doi = {10.1109/IPDPS.2011.331},
file = {:home/chiroptera/Dropbox/mendeley/Wu, Hong - 2011 - An efficient k-means algorithm on CUDA.pdf:pdf},
isbn = {9780769543857},
issn = {1530-2075},
journal = {IEEE International Symposium on Parallel and Distributed Processing Workshops and Phd Forum},
keywords = {CUDA,GPU,K-means},
pages = {1740--1749},
title = {{An efficient k-means algorithm on CUDA}},
year = {2011}
}
@article{Topchy,
abstract = {Clustering ensembles combine multiple partitions of the given data into a single clustering solution of better quality. Inspired by the success of supervised boosting algorithms, we devise an adaptive scheme for integration of multiple non-independent clusterings. Individual partitions in the ensemble are sequentially generated by clustering specially selected subsamples of the given data set. The sampling probability for each data point dynamically depends on the consistency of its previous assignments in the ensemble. New subsamples are drawn to increasingly focus on the problematic regions of the input feature space. A measure of a data point's clustering consistency is defined to guide this adaptation. An empirical study compares the performance of adaptive and regular clustering ensembles using different consensus functions on a number of data sets. Experimental results demonstrate improved accuracy for some clustering structures.},
author = {Topchy, Alexander and Minaei-Bidgoli, Behrouz and Jain, Anil K and Punch, William F},
doi = {10.1109/ICPR.2004.1334105},
file = {:home/chiroptera/Dropbox/mendeley/Topchy et al. - Unknown - Adaptive Clustering Ensembles.pdf:pdf},
isbn = {0769521282},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
number = {i},
pages = {272--275},
title = {{Adaptive clustering ensembles}},
volume = {1},
year = {2004}
}
@article{Strehl2000,
author = {Strehl, Alexander and Ghosh, Joydeep},
doi = {10.1162/153244303321897735},
file = {:home/chiroptera/Dropbox/mendeley/Strehl, Ghosh - 2000 - Cluster Ensembles – A Knowledge Reuse Framework for Combining Multiple Partitions.pdf:pdf},
issn = {0003-6951},
journal = {CrossRef Listing of Deleted DOIs},
keywords = {cluster analysis,clustering,consensus functions,ensemble,knowledge reuse,multi-learner systems,mutual information,partitioning,unsupervised learning},
pages = {583--617},
title = {{Cluster Ensembles – A Knowledge Reuse Framework for Combining Multiple Partitions}},
url = {http://dl.acm.org/citation.cfm?id=944919.944935},
volume = {1},
year = {2000}
}
@article{Woolley,
author = {Woolley, Cliff},
file = {:home/chiroptera/Dropbox/mendeley/Woolley - Unknown - CUDA Overview GPGPU Revolutionizes Computing.pdf:pdf},
title = {{CUDA Overview GPGPU Revolutionizes Computing}}
}
@article{Jarvis1973,
abstract = {A nonparametric clustering technique incorporating the concept of similarity based on the sharing of near neighbors is pre- sented. In addition to being an essentially paraliel approach, the com- putational elegance of the method is such that the scheme is applicable to a wide class of practical problems involving large sample size and high dimensionality. No attempt is made to show how a priori problem knowledge can be introduced into the procedure.},
annote = {Used by Fred2005.},
author = {Jarvis, R a and Patrick, Edward a},
doi = {10.1109/T-C.1973.223640},
file = {:home/chiroptera/Dropbox/mendeley/Jarvis, Patrick - 1973 - Clustering Using a Similarity Measure Based on Shared Near Neighbors.pdf:pdf},
isbn = {00189340 (ISSN)},
issn = {0018-9340},
journal = {Ieee Transactions on Computers},
keywords = {Clustering,nonparametric,pattern recognition,shared near neighbors,similarity measure.},
number = {11},
pages = {1025--1034},
title = {{Clustering Using a Similarity Measure Based on Shared Near Neighbors}},
volume = {C-22},
year = {1973}
}
@article{Pennycook2013,
abstract = {This paper reports on the development of an MPI/OpenCL implementation of LU, an application-level benchmark from the NAS Parallel Benchmark Suite. An account of the design decisions addressed during the development of this code is presented, demonstrating the importance of memory arrangement and work-item/work-group distribution strategies when applications are deployed on different device types. The resulting platform-agnostic, single source application is benchmarked on a number of different architectures, and is shown to be 1.3-1.5× slower than native FORTRAN 77 or CUDA implementations on a single node and 1.3-3.1× slower on multiple nodes. We also explore the potential performance gains of OpenCL's device fissioning capability, demonstrating up to a 3× speed-up over our original OpenCL implementation. © 2013 Elsevier Ltd. All rights reserved.},
author = {Pennycook, S. J. and Hammond, S. D. and Wright, S. a. and Herdman, J. a. and Miller, I. and Jarvis, S. a.},
doi = {10.1016/j.jpdc.2012.07.005},
file = {:home/chiroptera/Dropbox/mendeley/Pennycook et al. - 2013 - An investigation of the performance portability of OpenCL.pdf:pdf},
isbn = {Pennycook, S.J., Hammond, S.D., Wright, S.A., Herdman, J.A., Miller, I. and Jarvis, S.A. (2012) An Investigation of the Performance Portability of OpenCL. Journal of Parallel and Distributed Computing. ISSN 0743-7315 (In Press)},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
keywords = {GPU computing,High performance computing,Many-core computing,OpenCL,Optimisation,opencl},
mendeley-tags = {opencl},
number = {11},
pages = {1439--1450},
publisher = {Elsevier Inc.},
title = {{An investigation of the performance portability of OpenCL}},
url = {http://dx.doi.org/10.1016/j.jpdc.2012.07.005},
volume = {73},
year = {2013}
}
@article{Liang2009,
abstract = {Recent development in Graphics Processing Units (GPUs) has enabled inexpensive high performance computing for general-purpose applications. Due to GPU's tremendous computing capability, it has emerged as the co-processor of the CPU to achieve a high overall throughput. CUDA programming model provides the programmers adequate C language like APIs to better exploit the parallel power of the GPU. K-nearest neighbor is a widely used classification technique and has significant applications in various domains. The computational-intensive nature of KNN requires a high performance implementation. In this paper, we present a CUDA-based parallel implementation of KNN, CUKNN, using CUDA multi-thread model. Various CUDA optimization techniques are applied to maximize the utilization of the GPU. CUKNN outperforms significantly and achieve up to 15.2X speedup. It also shows good scalability when varying the dimension of the training dataset and the number of records in training dataset.},
author = {Liang, Shenshen and Wang, Cheng and Liu, Ying and Jian, Liheng},
doi = {10.1109/YCICT.2009.5382329},
file = {:home/chiroptera/Dropbox/mendeley/Liang et al. - 2009 - CUKNN A parallel implementation of K-nearest neighbor on CUDA-enabled GPU.pdf:pdf},
isbn = {9781424450756},
journal = {Proceedings - 2009 IEEE Youth Conference on Information, Computing and Telecommunication, YC-ICT2009},
keywords = {CUDA,Classification,KNN,Parallel computing},
pages = {415--418},
title = {{CUKNN: A parallel implementation of K-nearest neighbor on CUDA-enabled GPU}},
year = {2009}
}
@article{Misi2012,
author = {Mi{\v{s}}i, Marko J and {\^{C}}, M and Toma{\v{s}}evi, Milo V},
file = {:home/chiroptera/Dropbox/mendeley/Mi{\v{s}}i, {\^{C}}, Toma{\v{s}}evi - 2012 - Evolution and Trends in GPU Computing.pdf:pdf},
isbn = {9789532330724},
journal = {MIPRO, 2012 Proceedings of the 35th International Convention},
keywords = {API,CPU,CUDA,GPU computing,NVIDIA compute unified device architecture,application program interfaces,application programming interface,central processing units,commercial applications,general purpose computation,graphics coprocessors,graphics hardware,graphics processing units,latency-oriented processors,parallel architectures,task-parallel processors,throughput oriented processors},
pages = {289--294},
title = {{Evolution and Trends in GPU Computing}},
year = {2012}
}
@article{Karantasis2010,
abstract = {Many-core graphics processors are playing today an important role in the advancements of modern highly concurrent processors. Their ability to accelerate computation is being explored under several scientific fields. In the current paper we present the acceleration of a widely used data clustering algorithm, K-means, in the context of high performance GPU clusters. As opposed to most related implementation efforts that use MPI to port their target applications on a GPU cluster, our implementation follows the Software Distributed Shared Memory (SDSM) paradigm in order to distribute information and computation across the accelerator cluster. In order to investigate the efficiency of a programming model that offers shared memory abstraction on GPU clusters we present two implementations, one that is based on a SDSM implementation of OpenMP and another that utilizes the Pleiad cluster middleware on top of the Java platform. The first results show that such an implementation is feasible in order to accelerate a broad category of large scale, data intensive applications, among which K-means is a characteristic case.},
author = {Karantasis, Konstantinos I. and Polychronopoulos, Eleftherios D. and Dimitrakopoulos, George N.},
doi = {10.1109/CLUSTERWKSP.2010.5613079},
file = {:home/chiroptera/Dropbox/mendeley/Karantasis, Polychronopoulos, Dimitrakopoulos - 2010 - Accelerating data clustering on GPU-based clusters under shared memory abstractio.pdf:pdf},
isbn = {9781424483969},
journal = {2010 IEEE International Conference on Cluster Computing Workshops and Posters, Cluster Workshops 2010},
keywords = {GPU clusters, SDSM, Pleiad, CUDA, K-means},
title = {{Accelerating data clustering on GPU-based clusters under shared memory abstraction}},
year = {2010}
}
@article{hung2011new,
author = {Hung, Chih-Cheng and Kulkarni, Sameer and Kuo, Bor-Chen},
journal = {Selected Topics in Signal Processing, IEEE Journal of},
number = {3},
pages = {543--553},
publisher = {IEEE},
title = {{A new weighted fuzzy c-means clustering algorithm for remotely sensed image classification}},
volume = {5},
year = {2011}
}
@book{templates,
address = {Philadelphia, PA},
author = {Barrett, R and Berry, M and Chan, T F and Demmel, J and Donato, J and Dongarra, J and Eijkhout, V and Pozo, R and Romine, C and der Vorst, H Van},
publisher = {SIAM},
title = {{Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods, 2nd Edition}},
url = {http://netlib.org/linalg/html{\_}templates/report.html},
year = {1994}
}
@inproceedings{Vestias2014,
abstract = {Floating-point computing with more than one TFLOP of peak performance is already a reality in recent Field-Programmable Gate Arrays (FPGA). General-Purpose Graphics Processing Units (GPGPU) and recent many-core CPUs have also taken advantage of the recent technological innovations in integrated circuit (IC) design and had also dramatically improved their peak performances. In this paper, we compare the trends of these computing architectures for high-performance computing and survey these platforms in the execution of algorithms belonging to different scientific application domains. Trends in peak performance, power consumption and sustained performances, for particular applications, show that FPGAs are increasing the gap to GPUs and many-core CPUs moving them away from high-performance computing with intensive floating-point calculations. FPGAs become competitive for custom floating-point or fixed-point representations, for smaller input sizes of certain algorithms, for combinational logic problems and parallel map-reduce problems.},
author = {Vestias, Mario and Neto, Horacio},
booktitle = {2014 24th International Conference on Field Programmable Logic and Applications (FPL)},
doi = {10.1109/FPL.2014.6927483},
file = {:home/chiroptera/Dropbox/mendeley/Vestias, Neto - 2014 - Trends of CPU, GPU and FPGA for high-performance computing.pdf:pdf},
isbn = {978-3-00-044645-0},
keywords = {FPGA,Field programmable gate arrays,GPU,Graphics processing units,IC design,Market research,Monte Carlo methods,Performance evaluation,Table lookup,central processing unit,combinational logic problems,field programmable gate array,field programmable gate arrays,fixed-point representation,floating-point computing,floating-point representation,graphics processing unit,graphics processing units,high-performance computing,integrated circuit design,many-core CPU,multiprocessing systems,parallel MapReduce problems,parallel programming},
pages = {1--6},
title = {{Trends of CPU, GPU and FPGA for high-performance computing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6927483},
year = {2014}
}
@inproceedings{Shalom2009,
abstract = {We explore the use of todaypsilas high-end graphics processing units on desktops to perform hierarchical agglomerative clustering with the compute unified device architecture - CUDA of NVIDIA. Although the advancement in graphics cards has made the gaming industry to flourish,there is a lot more to be gained the field of scientific computing, high performance computing and their applications. Previous works have illustrated considerable speed gains on computing pair wise Euclidean distances between vectors, which is the fundamental operation in hierarchical clustering. We have used CUDA to implement the complete hierarchical agglomerative clustering algorithm and show almost double the speed gain using much cheaper desk top graphics card. In this paper we briefly explain the highly parallel and internally distributed programming structure of CUDA. We explore CUDA capabilities and propose methods to efficiently handle data within the graphics hardware for data intense, data independent, iterative or repetitive general purpose algorithms such as the hierarchical clustering. We achieved results with speed gains of about 30 to 65 times over the CPU implementation using micro array gene expressions.},
author = {Shalom, S and Dash, Manoranjan and Tue, Minh and Wilson, Nithin},
booktitle = {2009 International Conference on Signal Processing Systems, ICSPS 2009},
doi = {10.1109/ICSPS.2009.167},
file = {:home/chiroptera/Dropbox/mendeley/Shalom et al. - 2009 - Hierarchical Agglomerative Clustering Using Graphics Processor with Compute Unified Device Architecture.pdf:pdf},
isbn = {9780769536545},
keywords = {Acceleration of computations,CUDA hierarchical clustering,GPGPU,High performance computing,Parallel computing},
pages = {556--561},
title = {{Hierarchical Agglomerative Clustering Using Graphics Processor with Compute Unified Device Architecture}},
year = {2009}
}
@incollection{vitter2002external,
author = {Vitter, Jeffrey Scott},
booktitle = {Handbook of massive data sets},
file = {:home/chiroptera/Dropbox/mendeley/Vitter - 2002 - External memory algorithms.pdf:pdf},
pages = {359--416},
publisher = {Springer},
title = {{External memory algorithms}},
year = {2002}
}
@article{Zechner2009,
abstract = {In this paper an optimized k-means implementation on the graphics processing unit (GPU) is presented. NVIDIApsilas compute unified device architecture (CUDA), available from the G80 GPU family onwards, is used as the programming environment. Emphasis is placed on optimizations directly targeted at this architecture to best exploit the computational capabilities available. Additionally drawbacks and limitations of previous related work, e.g. maximum instance, dimension and centroid count are addressed. The algorithm is realized in a hybrid manner, parallelizing distance calculations on the GPU while sequentially updating cluster centroids on the CPU based on the results from the GPU calculations. An empirical performance study on synthetic data is given, demonstrating a maximum 14times speed increase to a fully SIMD optimized CPU implementation.},
author = {Zechner, Mario and Granitzer, Michael},
doi = {10.1109/INTENSIVE.2009.19},
file = {:home/chiroptera/Dropbox/mendeley/Zechner, Granitzer - 2009 - Accelerating k-means on the graphics processor via CUDA.pdf:pdf},
isbn = {9780769535852},
journal = {Proceedings of the 1st International Conference on Intensive Applications and Services, INTENSIVE 2009},
pages = {7--15},
title = {{Accelerating k-means on the graphics processor via CUDA}},
year = {2009}
}
@phdthesis{SousaThesis,
author = {Sousa, Cristiano Rafael da Silva},
file = {:home/chiroptera/Dropbox/mendeley/Sousa - 2014 - Efficient sequential and parallel versions of MST-solvers for multi-core CPU-chips and GPUs.pdf:pdf},
school = {Universidade do Minho},
title = {{Efficient sequential and parallel versions of MST-solvers for multi-core CPU-chips and GPUs}},
year = {2014}
}
@article{Zhang2008a,
abstract = {Affinity propagation (AP) is a clustering algorithm which has much better performance than traditional clustering approach such as k-means algorithm. In this paper, we present an algorithm called voting partition affinity propagation (voting-PAP) which is a method for clustering using evidence accumulation based on AP. Resulting clusters by voting-PAP are not constrained to be hyper-spherically shaped. Voting-PAP consists of three parts: partition affinity propagation (PAP), relaxed multi-root minimum spanning tree (MST) and majority voting. PAP is a method which can produce different exemplar set based on AP. Relaxed multi-root MST is a data point assign algorithm which has better performance than nearest assign rule. Majority voting is a scheme used to find a consistent clustering result of different partitions based on the idea of evidence accumulation. We also discuss how to find an appropriate threshold corresponding to an approximate ideal consistent partition in this paper.},
author = {Zhang, Xuqing Zhang Xuqing and Wu, Fei Wu Fei and Zhuang, Yueting Zhuang Yueting},
doi = {10.1109/ICPR.2008.4761213},
file = {:home/chiroptera/Dropbox/mendeley/Zhang, Wu, Zhuang - 2008 - Clustering by evidence accumulation on affinity propagation.pdf:pdf},
isbn = {978-1-4244-2174-9},
issn = {1051-4651},
journal = {2008 19th International Conference on Pattern Recognition},
title = {{Clustering by evidence accumulation on affinity propagation}},
year = {2008}
}
